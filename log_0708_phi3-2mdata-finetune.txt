nohup: ignoring input
[2024-07-09 10:06:30,869] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-09 10:06:32,696] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-09 10:06:32,697] [INFO] [runner.py:568:main] cmd = /root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None bunny/train/train.py --deepspeed ./script/deepspeed/zero3.json --model_name_or_path /root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct --model_type phi-3 --version phi3 --data_path ./data/finetune/bunny_llava_allava_2m.json --image_folder ./data/finetune/images --vision_tower /root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256 --pretrain_mm_mlp_adapter ./checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin --mm_projector_type mlp2x_gelu --image_aspect_ratio pad --group_by_modality_length False --bf16 True --output_dir ./checkpoints-phi-3/bunny-phi-3-2mdata --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 2000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
[2024-07-09 10:06:33,953] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-09 10:06:35,764] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-09 10:06:35,764] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-09 10:06:35,764] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-09 10:06:35,764] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-09 10:06:35,764] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-09 10:06:35,764] [INFO] [launch.py:256:main] process 565945 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=0', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:35,765] [INFO] [launch.py:256:main] process 565946 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=1', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:35,765] [INFO] [launch.py:256:main] process 565947 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=2', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:35,765] [INFO] [launch.py:256:main] process 565948 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=3', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:35,766] [INFO] [launch.py:256:main] process 565949 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=4', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:35,766] [INFO] [launch.py:256:main] process 565950 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=5', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:35,766] [INFO] [launch.py:256:main] process 565951 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=6', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:35,766] [INFO] [launch.py:256:main] process 565952 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=7', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-base-patch16-256', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain/mm_projector.bin', '--mm_projector_type', 'mlp2x_gelu', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-2mdata', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '2000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-09 10:06:41,067] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-09 10:06:41,309] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 10:06:41,399] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-07-09 10:06:41,492] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-09 10:06:41,612] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 10:06:41,615] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 10:06:41,618] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 10:06:41,633] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 10:06:41,650] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 10:06:41,656] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-09 10:06:41,729] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-09 10:06:41,849] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 10:06:41,849] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 10:06:41,849] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-09 10:06:41,861] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 10:06:41,889] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 10:06:41,904] [INFO] [comm.py:637:init_distributed] cdb=None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
[2024-07-09 10:06:44,456] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 195, num_elems = 3.82B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.29it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.25it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.37it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]
[2024-07-09 10:06:47,473] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 403, num_elems = 3.91B
Formatting inputs...Skip in lazy mode
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Parameter Offload: Total persistent parameters: 337152 in 197 params
  0%|          | 0/15436 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
 52%|█████▏    | 8001/15436 [00:11<00:10, 694.62it/s]                                                     {'loss': 0.8838, 'grad_norm': 0.7023573530800392, 'learning_rate': 9.89298813300317e-06, 'epoch': 0.52}
 52%|█████▏    | 8001/15436 [00:11<00:10, 694.62it/s]                                                     {'loss': 0.9056, 'grad_norm': 0.7361892167914372, 'learning_rate': 9.890889943570973e-06, 'epoch': 0.52}
 52%|█████▏    | 8002/15436 [00:21<00:10, 694.62it/s] 52%|█████▏    | 8002/15436 [00:26<00:10, 694.62it/s] 52%|█████▏    | 8003/15436 [00:26<00:30, 241.13it/s]                                                     {'loss': 0.8657, 'grad_norm': 0.7887857247225818, 'learning_rate': 9.88879175894279e-06, 'epoch': 0.52}
 52%|█████▏    | 8003/15436 [00:26<00:30, 241.13it/s] 52%|█████▏    | 8004/15436 [00:33<00:44, 167.73it/s]                                                     {'loss': 0.9055, 'grad_norm': 0.7350994345858533, 'learning_rate': 9.886693579211015e-06, 'epoch': 0.52}
 52%|█████▏    | 8004/15436 [00:33<00:44, 167.73it/s] 52%|█████▏    | 8005/15436 [00:41<01:04, 115.59it/s]                                                     {'loss': 0.9388, 'grad_norm': 0.7716399982295666, 'learning_rate': 9.884595404468022e-06, 'epoch': 0.52}
 52%|█████▏    | 8005/15436 [00:41<01:04, 115.59it/s] 52%|█████▏    | 8006/15436 [00:49<01:35, 77.70it/s]                                                     {'loss': 0.9065, 'grad_norm': 0.725183132629458, 'learning_rate': 9.88249723480619e-06, 'epoch': 0.52}
 52%|█████▏    | 8006/15436 [00:49<01:35, 77.70it/s] 52%|█████▏    | 8007/15436 [00:54<02:02, 60.48it/s]                                                    {'loss': 0.9347, 'grad_norm': 0.7760374492940029, 'learning_rate': 9.880399070317907e-06, 'epoch': 0.52}
 52%|█████▏    | 8007/15436 [00:54<02:02, 60.48it/s] 52%|█████▏    | 8008/15436 [01:00<02:48, 44.00it/s]                                                    {'loss': 0.9705, 'grad_norm': 0.796789888789373, 'learning_rate': 9.878300911095545e-06, 'epoch': 0.52}
 52%|█████▏    | 8008/15436 [01:00<02:48, 44.00it/s] 52%|█████▏    | 8009/15436 [01:07<04:06, 30.13it/s]                                                    {'loss': 0.9423, 'grad_norm': 0.7983696718503178, 'learning_rate': 9.876202757231491e-06, 'epoch': 0.52}
 52%|█████▏    | 8009/15436 [01:07<04:06, 30.13it/s] 52%|█████▏    | 8010/15436 [01:14<06:06, 20.28it/s]                                                    {'loss': 0.8721, 'grad_norm': 0.7872868703055497, 'learning_rate': 9.874104608818121e-06, 'epoch': 0.52}
 52%|█████▏    | 8010/15436 [01:14<06:06, 20.28it/s] 52%|█████▏    | 8011/15436 [01:21<08:33, 14.46it/s]                                                    {'loss': 0.8167, 'grad_norm': 0.7222405596381519, 'learning_rate': 9.872006465947814e-06, 'epoch': 0.52}
 52%|█████▏    | 8011/15436 [01:21<08:33, 14.46it/s] 52%|█████▏    | 8012/15436 [01:27<12:06, 10.22it/s]                                                    {'loss': 0.8961, 'grad_norm': 0.6890362410739509, 'learning_rate': 9.869908328712953e-06, 'epoch': 0.52}
 52%|█████▏    | 8012/15436 [01:27<12:06, 10.22it/s] 52%|█████▏    | 8013/15436 [01:33<16:09,  7.66it/s]                                                    {'loss': 0.9008, 'grad_norm': 0.7292313419731388, 'learning_rate': 9.867810197205916e-06, 'epoch': 0.52}
 52%|█████▏    | 8013/15436 [01:33<16:09,  7.66it/s] 52%|█████▏    | 8014/15436 [01:37<21:14,  5.82it/s]                                                    {'loss': 0.9811, 'grad_norm': 0.7629521761210887, 'learning_rate': 9.865712071519077e-06, 'epoch': 0.52}
 52%|█████▏    | 8014/15436 [01:37<21:14,  5.82it/s] 52%|█████▏    | 8015/15436 [01:48<36:42,  3.37it/s]                                                    {'loss': 0.8569, 'grad_norm': 0.7425398291859784, 'learning_rate': 9.86361395174482e-06, 'epoch': 0.52}
 52%|█████▏    | 8015/15436 [01:48<36:42,  3.37it/s] 52%|█████▏    | 8016/15436 [01:53<47:57,  2.58it/s]                                                    {'loss': 0.9799, 'grad_norm': 0.6943800564577548, 'learning_rate': 9.861515837975529e-06, 'epoch': 0.52}
 52%|█████▏    | 8016/15436 [01:53<47:57,  2.58it/s] 52%|█████▏    | 8017/15436 [02:00<1:06:50,  1.85it/s]                                                      {'loss': 0.9877, 'grad_norm': 0.6925134948887427, 'learning_rate': 9.859417730303566e-06, 'epoch': 0.52}
 52%|█████▏    | 8017/15436 [02:00<1:06:50,  1.85it/s] 52%|█████▏    | 8018/15436 [02:06<1:28:57,  1.39it/s]                                                      {'loss': 0.8698, 'grad_norm': 0.8185212101166229, 'learning_rate': 9.857319628821324e-06, 'epoch': 0.52}
 52%|█████▏    | 8018/15436 [02:06<1:28:57,  1.39it/s] 52%|█████▏    | 8019/15436 [02:11<1:56:16,  1.06it/s]                                                      {'loss': 0.8205, 'grad_norm': 0.6951787488509699, 'learning_rate': 9.855221533621173e-06, 'epoch': 0.52}
 52%|█████▏    | 8019/15436 [02:11<1:56:16,  1.06it/s] 52%|█████▏    | 8020/15436 [02:19<2:47:47,  1.36s/it]                                                      {'loss': 0.8279, 'grad_norm': 0.7198974409898817, 'learning_rate': 9.85312344479549e-06, 'epoch': 0.52}
 52%|█████▏    | 8020/15436 [02:19<2:47:47,  1.36s/it] 52%|█████▏    | 8021/15436 [02:26<3:42:08,  1.80s/it]                                                      {'loss': 0.8893, 'grad_norm': 0.7241273033777992, 'learning_rate': 9.851025362436654e-06, 'epoch': 0.52}
 52%|█████▏    | 8021/15436 [02:26<3:42:08,  1.80s/it] 52%|█████▏    | 8022/15436 [02:35<5:06:40,  2.48s/it]                                                      {'loss': 0.9027, 'grad_norm': 0.7412351049260166, 'learning_rate': 9.848927286637044e-06, 'epoch': 0.52}
 52%|█████▏    | 8022/15436 [02:35<5:06:40,  2.48s/it] 52%|█████▏    | 8023/15436 [02:41<6:17:00,  3.05s/it]                                                      {'loss': 0.9008, 'grad_norm': 0.7595857150149924, 'learning_rate': 9.846829217489033e-06, 'epoch': 0.52}
 52%|█████▏    | 8023/15436 [02:41<6:17:00,  3.05s/it] 52%|█████▏    | 8024/15436 [02:48<7:27:08,  3.62s/it]                                                      {'loss': 0.9272, 'grad_norm': 0.6713954518665727, 'learning_rate': 9.844731155084998e-06, 'epoch': 0.52}
 52%|█████▏    | 8024/15436 [02:48<7:27:08,  3.62s/it] 52%|█████▏    | 8025/15436 [02:53<7:55:54,  3.85s/it]                                                      {'loss': 1.0139, 'grad_norm': 0.7092316506019665, 'learning_rate': 9.842633099517315e-06, 'epoch': 0.52}
 52%|█████▏    | 8025/15436 [02:53<7:55:54,  3.85s/it] 52%|█████▏    | 8026/15436 [03:00<9:12:54,  4.48s/it]                                                      {'loss': 0.8975, 'grad_norm': 0.7417930969983411, 'learning_rate': 9.84053505087836e-06, 'epoch': 0.52}
 52%|█████▏    | 8026/15436 [03:00<9:12:54,  4.48s/it] 52%|█████▏    | 8027/15436 [03:09<11:10:12,  5.43s/it]                                                       {'loss': 0.862, 'grad_norm': 0.7312349662721239, 'learning_rate': 9.83843700926051e-06, 'epoch': 0.52}
 52%|█████▏    | 8027/15436 [03:09<11:10:12,  5.43s/it] 52%|█████▏    | 8028/15436 [03:16<11:57:28,  5.81s/it]                                                       {'loss': 0.8378, 'grad_norm': 0.6575308397785853, 'learning_rate': 9.836338974756134e-06, 'epoch': 0.52}
 52%|█████▏    | 8028/15436 [03:16<11:57:28,  5.81s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2438 > 2048). Running this sequence through the model will result in indexing errors
 52%|█████▏    | 8029/15436 [03:21<11:36:26,  5.64s/it]                                                       {'loss': 0.9513, 'grad_norm': 0.739552464707636, 'learning_rate': 9.834240947457613e-06, 'epoch': 0.52}
 52%|█████▏    | 8029/15436 [03:21<11:36:26,  5.64s/it] 52%|█████▏    | 8030/15436 [03:32<14:49:46,  7.21s/it]                                                       {'loss': 0.8265, 'grad_norm': 0.7902284766480292, 'learning_rate': 9.832142927457323e-06, 'epoch': 0.52}
 52%|█████▏    | 8030/15436 [03:32<14:49:46,  7.21s/it] 52%|█████▏    | 8031/15436 [03:37<13:27:52,  6.55s/it]                                                       {'loss': 0.9604, 'grad_norm': 0.6827508454647081, 'learning_rate': 9.830044914847626e-06, 'epoch': 0.52}
 52%|█████▏    | 8031/15436 [03:37<13:27:52,  6.55s/it] 52%|█████▏    | 8032/15436 [03:43<13:21:41,  6.50s/it]                                                       {'loss': 0.8871, 'grad_norm': 0.7011832959850656, 'learning_rate': 9.827946909720907e-06, 'epoch': 0.52}
 52%|█████▏    | 8032/15436 [03:43<13:21:41,  6.50s/it] 52%|█████▏    | 8033/15436 [03:52<14:48:52,  7.20s/it]                                                       {'loss': 0.88, 'grad_norm': 0.6764445493304597, 'learning_rate': 9.82584891216954e-06, 'epoch': 0.52}
 52%|█████▏    | 8033/15436 [03:52<14:48:52,  7.20s/it] 52%|█████▏    | 8034/15436 [04:02<16:08:26,  7.85s/it]                                                       {'loss': 0.8673, 'grad_norm': 0.7650452368230998, 'learning_rate': 9.823750922285888e-06, 'epoch': 0.52}
 52%|█████▏    | 8034/15436 [04:02<16:08:26,  7.85s/it] 52%|█████▏    | 8035/15436 [04:11<16:46:27,  8.16s/it]                                                       {'loss': 0.9868, 'grad_norm': 0.7277437178097094, 'learning_rate': 9.821652940162334e-06, 'epoch': 0.52}
 52%|█████▏    | 8035/15436 [04:11<16:46:27,  8.16s/it] 52%|█████▏    | 8036/15436 [04:16<15:17:04,  7.44s/it]                                                       {'loss': 0.9391, 'grad_norm': 0.7838641627777392, 'learning_rate': 9.819554965891244e-06, 'epoch': 0.52}
 52%|█████▏    | 8036/15436 [04:16<15:17:04,  7.44s/it] 52%|█████▏    | 8037/15436 [04:24<15:09:46,  7.38s/it]                                                       {'loss': 0.8603, 'grad_norm': 0.6739745406926517, 'learning_rate': 9.81745699956499e-06, 'epoch': 0.52}
 52%|█████▏    | 8037/15436 [04:24<15:09:46,  7.38s/it] 52%|█████▏    | 8038/15436 [04:31<15:09:54,  7.38s/it]                                                       {'loss': 0.9219, 'grad_norm': 0.7864772151887567, 'learning_rate': 9.815359041275948e-06, 'epoch': 0.52}
 52%|█████▏    | 8038/15436 [04:31<15:09:54,  7.38s/it] 52%|█████▏    | 8039/15436 [04:42<17:09:23,  8.35s/it]                                                       {'loss': 0.8933, 'grad_norm': 0.6780038231255665, 'learning_rate': 9.813261091116487e-06, 'epoch': 0.52}
 52%|█████▏    | 8039/15436 [04:42<17:09:23,  8.35s/it] 52%|█████▏    | 8040/15436 [04:48<16:07:30,  7.85s/it]                                                       {'loss': 0.9063, 'grad_norm': 0.7691589820156981, 'learning_rate': 9.811163149178976e-06, 'epoch': 0.52}
 52%|█████▏    | 8040/15436 [04:48<16:07:30,  7.85s/it] 52%|█████▏    | 8041/15436 [04:54<14:56:15,  7.27s/it]                                                       {'loss': 0.8743, 'grad_norm': 0.8452958963479756, 'learning_rate': 9.809065215555789e-06, 'epoch': 0.52}
 52%|█████▏    | 8041/15436 [04:54<14:56:15,  7.27s/it] 52%|█████▏    | 8042/15436 [05:03<16:05:13,  7.83s/it]                                                       {'loss': 0.905, 'grad_norm': 0.789461939974024, 'learning_rate': 9.806967290339292e-06, 'epoch': 0.52}
 52%|█████▏    | 8042/15436 [05:03<16:05:13,  7.83s/it] 52%|█████▏    | 8043/15436 [05:08<13:58:41,  6.81s/it]                                                       {'loss': 0.8415, 'grad_norm': 0.7436962175264937, 'learning_rate': 9.804869373621859e-06, 'epoch': 0.52}
 52%|█████▏    | 8043/15436 [05:08<13:58:41,  6.81s/it] 52%|█████▏    | 8044/15436 [05:13<12:43:33,  6.20s/it]                                                       {'loss': 1.0208, 'grad_norm': 0.7691683493978729, 'learning_rate': 9.802771465495858e-06, 'epoch': 0.52}
 52%|█████▏    | 8044/15436 [05:13<12:43:33,  6.20s/it] 52%|█████▏    | 8045/15436 [05:17<11:35:08,  5.64s/it]                                                       {'loss': 0.9802, 'grad_norm': 0.8292848749485711, 'learning_rate': 9.800673566053657e-06, 'epoch': 0.52}
 52%|█████▏    | 8045/15436 [05:17<11:35:08,  5.64s/it] 52%|█████▏    | 8046/15436 [05:24<12:25:34,  6.05s/it]                                                       {'loss': 0.8324, 'grad_norm': 0.8977455209814369, 'learning_rate': 9.798575675387627e-06, 'epoch': 0.52}
 52%|█████▏    | 8046/15436 [05:24<12:25:34,  6.05s/it] 52%|█████▏    | 8047/15436 [05:36<15:59:59,  7.80s/it]                                                       {'loss': 0.8394, 'grad_norm': 0.6847708959626042, 'learning_rate': 9.796477793590138e-06, 'epoch': 0.52}
 52%|█████▏    | 8047/15436 [05:36<15:59:59,  7.80s/it] 52%|█████▏    | 8048/15436 [05:40<13:58:57,  6.81s/it]                                                       {'loss': 0.9201, 'grad_norm': 0.769952054372092, 'learning_rate': 9.794379920753545e-06, 'epoch': 0.52}
 52%|█████▏    | 8048/15436 [05:40<13:58:57,  6.81s/it] 52%|█████▏    | 8049/15436 [05:46<13:34:27,  6.62s/it]                                                       {'loss': 0.953, 'grad_norm': 0.7790071603398521, 'learning_rate': 9.792282056970235e-06, 'epoch': 0.52}
 52%|█████▏    | 8049/15436 [05:46<13:34:27,  6.62s/it] 52%|█████▏    | 8050/15436 [05:52<13:00:09,  6.34s/it]                                                       {'loss': 0.9847, 'grad_norm': 0.7522702456162473, 'learning_rate': 9.79018420233256e-06, 'epoch': 0.52}
 52%|█████▏    | 8050/15436 [05:52<13:00:09,  6.34s/it] 52%|█████▏    | 8051/15436 [06:01<14:41:58,  7.17s/it]                                                       {'loss': 0.8256, 'grad_norm': 0.680083413279363, 'learning_rate': 9.788086356932893e-06, 'epoch': 0.52}
 52%|█████▏    | 8051/15436 [06:01<14:41:58,  7.17s/it] 52%|█████▏    | 8052/15436 [06:09<15:04:00,  7.35s/it]                                                       {'loss': 0.8218, 'grad_norm': 0.7275131485048546, 'learning_rate': 9.7859885208636e-06, 'epoch': 0.52}
 52%|█████▏    | 8052/15436 [06:09<15:04:00,  7.35s/it] 52%|█████▏    | 8053/15436 [06:15<14:26:15,  7.04s/it]                                                       {'loss': 0.9419, 'grad_norm': 0.6946583606979255, 'learning_rate': 9.783890694217047e-06, 'epoch': 0.52}
 52%|█████▏    | 8053/15436 [06:15<14:26:15,  7.04s/it] 52%|█████▏    | 8054/15436 [06:24<15:36:48,  7.61s/it]                                                       {'loss': 0.9109, 'grad_norm': 0.6956439095347454, 'learning_rate': 9.781792877085598e-06, 'epoch': 0.52}
 52%|█████▏    | 8054/15436 [06:24<15:36:48,  7.61s/it] 52%|█████▏    | 8055/15436 [06:32<15:23:07,  7.50s/it]                                                       {'loss': 0.961, 'grad_norm': 0.8273266605697678, 'learning_rate': 9.779695069561619e-06, 'epoch': 0.52}
 52%|█████▏    | 8055/15436 [06:32<15:23:07,  7.50s/it] 52%|█████▏    | 8056/15436 [06:43<17:46:41,  8.67s/it]                                                       {'loss': 0.945, 'grad_norm': 0.6518801862629008, 'learning_rate': 9.777597271737474e-06, 'epoch': 0.52}
 52%|█████▏    | 8056/15436 [06:43<17:46:41,  8.67s/it] 52%|█████▏    | 8057/15436 [06:51<17:31:42,  8.55s/it]                                                       {'loss': 0.8822, 'grad_norm': 0.6524601378558176, 'learning_rate': 9.775499483705527e-06, 'epoch': 0.52}
 52%|█████▏    | 8057/15436 [06:51<17:31:42,  8.55s/it] 52%|█████▏    | 8058/15436 [06:57<15:47:45,  7.71s/it]                                                       {'loss': 0.8473, 'grad_norm': 0.7482347951463952, 'learning_rate': 9.773401705558145e-06, 'epoch': 0.52}
 52%|█████▏    | 8058/15436 [06:57<15:47:45,  7.71s/it] 52%|█████▏    | 8059/15436 [07:05<16:04:58,  7.85s/it]                                                       {'loss': 0.9152, 'grad_norm': 0.740311595952352, 'learning_rate': 9.771303937387685e-06, 'epoch': 0.52}
 52%|█████▏    | 8059/15436 [07:05<16:04:58,  7.85s/it] 52%|█████▏    | 8060/15436 [07:14<16:45:28,  8.18s/it]                                                       {'loss': 0.8961, 'grad_norm': 0.7456964382170684, 'learning_rate': 9.769206179286517e-06, 'epoch': 0.52}
 52%|█████▏    | 8060/15436 [07:14<16:45:28,  8.18s/it] 52%|█████▏    | 8061/15436 [07:22<16:52:46,  8.24s/it]                                                       {'loss': 0.7943, 'grad_norm': 0.7040655499933602, 'learning_rate': 9.767108431347002e-06, 'epoch': 0.52}
 52%|█████▏    | 8061/15436 [07:22<16:52:46,  8.24s/it] 52%|█████▏    | 8062/15436 [07:27<14:46:11,  7.21s/it]                                                       {'loss': 0.8834, 'grad_norm': 0.7655393067308651, 'learning_rate': 9.765010693661492e-06, 'epoch': 0.52}
 52%|█████▏    | 8062/15436 [07:27<14:46:11,  7.21s/it] 52%|█████▏    | 8063/15436 [07:35<15:17:19,  7.47s/it]                                                       {'loss': 0.9771, 'grad_norm': 0.7476753320832955, 'learning_rate': 9.762912966322366e-06, 'epoch': 0.52}
 52%|█████▏    | 8063/15436 [07:35<15:17:19,  7.47s/it] 52%|█████▏    | 8064/15436 [07:42<14:51:49,  7.26s/it]                                                       {'loss': 0.843, 'grad_norm': 0.6914604179727427, 'learning_rate': 9.760815249421973e-06, 'epoch': 0.52}
 52%|█████▏    | 8064/15436 [07:42<14:51:49,  7.26s/it] 52%|█████▏    | 8065/15436 [07:48<14:11:39,  6.93s/it]                                                       {'loss': 0.8419, 'grad_norm': 0.7560221117011856, 'learning_rate': 9.758717543052672e-06, 'epoch': 0.52}
 52%|█████▏    | 8065/15436 [07:48<14:11:39,  6.93s/it] 52%|█████▏    | 8066/15436 [07:55<14:06:01,  6.89s/it]                                                       {'loss': 0.8266, 'grad_norm': 0.738523111321196, 'learning_rate': 9.756619847306835e-06, 'epoch': 0.52}
 52%|█████▏    | 8066/15436 [07:55<14:06:01,  6.89s/it] 52%|█████▏    | 8067/15436 [08:03<14:28:25,  7.07s/it]                                                       {'loss': 0.9184, 'grad_norm': 0.6975888719312668, 'learning_rate': 9.754522162276811e-06, 'epoch': 0.52}
 52%|█████▏    | 8067/15436 [08:03<14:28:25,  7.07s/it] 52%|█████▏    | 8068/15436 [08:13<16:20:51,  7.99s/it]                                                       {'loss': 0.8281, 'grad_norm': 0.7242402977728902, 'learning_rate': 9.75242448805496e-06, 'epoch': 0.52}
 52%|█████▏    | 8068/15436 [08:13<16:20:51,  7.99s/it] 52%|█████▏    | 8069/15436 [08:18<14:50:46,  7.25s/it]                                                       {'loss': 0.8971, 'grad_norm': 0.7092976156504611, 'learning_rate': 9.750326824733648e-06, 'epoch': 0.52}
 52%|█████▏    | 8069/15436 [08:18<14:50:46,  7.25s/it] 52%|█████▏    | 8070/15436 [08:25<14:40:14,  7.17s/it]                                                       {'loss': 0.8953, 'grad_norm': 0.7139619784582537, 'learning_rate': 9.748229172405229e-06, 'epoch': 0.52}
 52%|█████▏    | 8070/15436 [08:25<14:40:14,  7.17s/it] 52%|█████▏    | 8071/15436 [08:34<15:30:59,  7.58s/it]                                                       {'loss': 0.9111, 'grad_norm': 0.7382311949865998, 'learning_rate': 9.746131531162055e-06, 'epoch': 0.52}
 52%|█████▏    | 8071/15436 [08:34<15:30:59,  7.58s/it] 52%|█████▏    | 8072/15436 [08:42<15:53:56,  7.77s/it]                                                       {'loss': 0.9968, 'grad_norm': 0.6597547608550691, 'learning_rate': 9.744033901096494e-06, 'epoch': 0.52}
 52%|█████▏    | 8072/15436 [08:42<15:53:56,  7.77s/it] 52%|█████▏    | 8073/15436 [08:52<17:33:48,  8.59s/it]                                                       {'loss': 0.9768, 'grad_norm': 0.7142433101479534, 'learning_rate': 9.741936282300895e-06, 'epoch': 0.52}
 52%|█████▏    | 8073/15436 [08:52<17:33:48,  8.59s/it] 52%|█████▏    | 8074/15436 [08:58<15:27:04,  7.56s/it]                                                       {'loss': 0.929, 'grad_norm': 0.6953886424761796, 'learning_rate': 9.739838674867617e-06, 'epoch': 0.52}
 52%|█████▏    | 8074/15436 [08:58<15:27:04,  7.56s/it] 52%|█████▏    | 8075/15436 [09:04<14:36:54,  7.15s/it]                                                       {'loss': 0.9489, 'grad_norm': 0.7068670434323702, 'learning_rate': 9.737741078889019e-06, 'epoch': 0.52}
 52%|█████▏    | 8075/15436 [09:04<14:36:54,  7.15s/it] 52%|█████▏    | 8076/15436 [09:10<14:06:35,  6.90s/it]                                                       {'loss': 0.8639, 'grad_norm': 0.753271871069115, 'learning_rate': 9.735643494457448e-06, 'epoch': 0.52}
 52%|█████▏    | 8076/15436 [09:10<14:06:35,  6.90s/it] 52%|█████▏    | 8077/15436 [09:18<14:38:00,  7.16s/it]                                                       {'loss': 0.8805, 'grad_norm': 0.8058173960028746, 'learning_rate': 9.733545921665268e-06, 'epoch': 0.52}
 52%|█████▏    | 8077/15436 [09:18<14:38:00,  7.16s/it] 52%|█████▏    | 8078/15436 [09:26<14:56:17,  7.31s/it]                                                       {'loss': 0.9209, 'grad_norm': 0.7108929672187595, 'learning_rate': 9.731448360604832e-06, 'epoch': 0.52}
 52%|█████▏    | 8078/15436 [09:26<14:56:17,  7.31s/it] 52%|█████▏    | 8079/15436 [09:33<15:09:05,  7.41s/it]                                                       {'loss': 0.9099, 'grad_norm': 0.7358528570366877, 'learning_rate': 9.729350811368483e-06, 'epoch': 0.52}
 52%|█████▏    | 8079/15436 [09:33<15:09:05,  7.41s/it] 52%|█████▏    | 8080/15436 [09:40<14:34:45,  7.14s/it]                                                       {'loss': 0.8826, 'grad_norm': 0.7682516953084617, 'learning_rate': 9.72725327404859e-06, 'epoch': 0.52}
 52%|█████▏    | 8080/15436 [09:40<14:34:45,  7.14s/it] 52%|█████▏    | 8081/15436 [09:49<15:38:49,  7.66s/it]                                                       {'loss': 0.8982, 'grad_norm': 0.7309357187460824, 'learning_rate': 9.725155748737496e-06, 'epoch': 0.52}
 52%|█████▏    | 8081/15436 [09:49<15:38:49,  7.66s/it] 52%|█████▏    | 8082/15436 [09:55<14:35:30,  7.14s/it]                                                       {'loss': 0.9296, 'grad_norm': 0.7380282894211673, 'learning_rate': 9.723058235527554e-06, 'epoch': 0.52}
 52%|█████▏    | 8082/15436 [09:55<14:35:30,  7.14s/it] 52%|█████▏    | 8083/15436 [09:59<13:10:11,  6.45s/it]                                                       {'loss': 0.8716, 'grad_norm': 0.7293198211526349, 'learning_rate': 9.720960734511118e-06, 'epoch': 0.52}
 52%|█████▏    | 8083/15436 [09:59<13:10:11,  6.45s/it] 52%|█████▏    | 8084/15436 [10:05<12:37:00,  6.18s/it]                                                       {'loss': 0.9747, 'grad_norm': 0.7000077535011641, 'learning_rate': 9.718863245780538e-06, 'epoch': 0.52}
 52%|█████▏    | 8084/15436 [10:05<12:37:00,  6.18s/it] 52%|█████▏    | 8085/15436 [10:11<12:51:43,  6.30s/it]                                                       {'loss': 0.9858, 'grad_norm': 0.8386628016213326, 'learning_rate': 9.716765769428162e-06, 'epoch': 0.52}
 52%|█████▏    | 8085/15436 [10:11<12:51:43,  6.30s/it] 52%|█████▏    | 8086/15436 [10:18<13:08:03,  6.43s/it]                                                       {'loss': 0.9374, 'grad_norm': 0.6969730405185824, 'learning_rate': 9.714668305546346e-06, 'epoch': 0.52}
 52%|█████▏    | 8086/15436 [10:18<13:08:03,  6.43s/it] 52%|█████▏    | 8087/15436 [10:25<13:12:03,  6.47s/it]                                                       {'loss': 0.9574, 'grad_norm': 0.7684372013266143, 'learning_rate': 9.712570854227437e-06, 'epoch': 0.52}
 52%|█████▏    | 8087/15436 [10:25<13:12:03,  6.47s/it] 52%|█████▏    | 8088/15436 [10:33<14:16:02,  6.99s/it]                                                       {'loss': 0.9121, 'grad_norm': 0.8628148862402863, 'learning_rate': 9.71047341556378e-06, 'epoch': 0.52}
 52%|█████▏    | 8088/15436 [10:33<14:16:02,  6.99s/it] 52%|█████▏    | 8089/15436 [10:38<13:11:51,  6.47s/it]                                                       {'loss': 0.9186, 'grad_norm': 0.7160493978259921, 'learning_rate': 9.70837598964773e-06, 'epoch': 0.52}
 52%|█████▏    | 8089/15436 [10:38<13:11:51,  6.47s/it] 52%|█████▏    | 8090/15436 [10:48<15:10:20,  7.44s/it]                                                       {'loss': 0.8715, 'grad_norm': 0.7192002337384585, 'learning_rate': 9.706278576571633e-06, 'epoch': 0.52}
 52%|█████▏    | 8090/15436 [10:48<15:10:20,  7.44s/it] 52%|█████▏    | 8091/15436 [10:54<14:37:44,  7.17s/it]                                                       {'loss': 0.8446, 'grad_norm': 0.74560277689741, 'learning_rate': 9.704181176427828e-06, 'epoch': 0.52}
 52%|█████▏    | 8091/15436 [10:54<14:37:44,  7.17s/it] 52%|█████▏    | 8092/15436 [11:00<13:48:30,  6.77s/it]                                                       {'loss': 0.9145, 'grad_norm': 0.72260686928296, 'learning_rate': 9.702083789308677e-06, 'epoch': 0.52}
 52%|█████▏    | 8092/15436 [11:00<13:48:30,  6.77s/it] 52%|█████▏    | 8093/15436 [11:07<13:49:09,  6.78s/it]                                                       {'loss': 0.9094, 'grad_norm': 0.766776709325528, 'learning_rate': 9.699986415306509e-06, 'epoch': 0.52}
 52%|█████▏    | 8093/15436 [11:07<13:49:09,  6.78s/it] 52%|█████▏    | 8094/15436 [11:16<15:23:22,  7.55s/it]                                                       {'loss': 0.945, 'grad_norm': 0.8913873638079307, 'learning_rate': 9.697889054513684e-06, 'epoch': 0.52}
 52%|█████▏    | 8094/15436 [11:16<15:23:22,  7.55s/it] 52%|█████▏    | 8095/15436 [11:21<13:42:46,  6.72s/it]                                                       {'loss': 0.8443, 'grad_norm': 0.6672351773823808, 'learning_rate': 9.69579170702254e-06, 'epoch': 0.52}
 52%|█████▏    | 8095/15436 [11:21<13:42:46,  6.72s/it] 52%|█████▏    | 8096/15436 [11:27<13:05:51,  6.42s/it]                                                       {'loss': 0.8934, 'grad_norm': 0.7011085216742103, 'learning_rate': 9.693694372925422e-06, 'epoch': 0.52}
 52%|█████▏    | 8096/15436 [11:27<13:05:51,  6.42s/it] 52%|█████▏    | 8097/15436 [11:31<11:47:53,  5.79s/it]                                                       {'loss': 0.9303, 'grad_norm': 0.7550385453308565, 'learning_rate': 9.691597052314675e-06, 'epoch': 0.52}
 52%|█████▏    | 8097/15436 [11:31<11:47:53,  5.79s/it] 52%|█████▏    | 8098/15436 [11:36<11:02:35,  5.42s/it]                                                       {'loss': 0.9567, 'grad_norm': 0.7659534405560458, 'learning_rate': 9.689499745282642e-06, 'epoch': 0.52}
 52%|█████▏    | 8098/15436 [11:36<11:02:35,  5.42s/it] 52%|█████▏    | 8099/15436 [11:42<11:34:27,  5.68s/it]                                                       {'loss': 0.9295, 'grad_norm': 0.7887632246649746, 'learning_rate': 9.687402451921663e-06, 'epoch': 0.52}
 52%|█████▏    | 8099/15436 [11:42<11:34:27,  5.68s/it] 52%|█████▏    | 8100/15436 [11:49<12:08:45,  5.96s/it]                                                       {'loss': 0.9152, 'grad_norm': 0.7101139473187942, 'learning_rate': 9.685305172324083e-06, 'epoch': 0.52}
 52%|█████▏    | 8100/15436 [11:49<12:08:45,  5.96s/it] 52%|█████▏    | 8101/15436 [11:54<11:56:23,  5.86s/it]                                                       {'loss': 0.9212, 'grad_norm': 0.7772196906775152, 'learning_rate': 9.683207906582243e-06, 'epoch': 0.52}
 52%|█████▏    | 8101/15436 [11:54<11:56:23,  5.86s/it] 52%|█████▏    | 8102/15436 [12:01<12:09:26,  5.97s/it]                                                       {'loss': 0.8856, 'grad_norm': 0.6766609586103286, 'learning_rate': 9.681110654788483e-06, 'epoch': 0.52}
 52%|█████▏    | 8102/15436 [12:01<12:09:26,  5.97s/it] 52%|█████▏    | 8103/15436 [12:07<12:37:52,  6.20s/it]                                                       {'loss': 0.9708, 'grad_norm': 0.7290169387704195, 'learning_rate': 9.679013417035144e-06, 'epoch': 0.52}
 52%|█████▏    | 8103/15436 [12:07<12:37:52,  6.20s/it] 53%|█████▎    | 8104/15436 [12:12<11:47:36,  5.79s/it]                                                       {'loss': 0.9059, 'grad_norm': 0.7017478151200967, 'learning_rate': 9.676916193414569e-06, 'epoch': 0.53}
 53%|█████▎    | 8104/15436 [12:12<11:47:36,  5.79s/it] 53%|█████▎    | 8105/15436 [12:19<12:26:28,  6.11s/it]                                                       {'loss': 0.9065, 'grad_norm': 0.7245966446237319, 'learning_rate': 9.674818984019085e-06, 'epoch': 0.53}
 53%|█████▎    | 8105/15436 [12:19<12:26:28,  6.11s/it] 53%|█████▎    | 8106/15436 [12:24<12:01:38,  5.91s/it]                                                       {'loss': 0.8319, 'grad_norm': 0.7908081086237293, 'learning_rate': 9.672721788941043e-06, 'epoch': 0.53}
 53%|█████▎    | 8106/15436 [12:24<12:01:38,  5.91s/it] 53%|█████▎    | 8107/15436 [12:31<12:35:15,  6.18s/it]                                                       {'loss': 0.9211, 'grad_norm': 0.8170716348566176, 'learning_rate': 9.670624608272778e-06, 'epoch': 0.53}
 53%|█████▎    | 8107/15436 [12:31<12:35:15,  6.18s/it] 53%|█████▎    | 8108/15436 [12:40<14:26:02,  7.09s/it]                                                       {'loss': 0.8943, 'grad_norm': 0.8275556406567371, 'learning_rate': 9.668527442106619e-06, 'epoch': 0.53}
 53%|█████▎    | 8108/15436 [12:40<14:26:02,  7.09s/it] 53%|█████▎    | 8109/15436 [12:45<12:53:03,  6.33s/it]                                                       {'loss': 0.9357, 'grad_norm': 0.659096389311579, 'learning_rate': 9.666430290534915e-06, 'epoch': 0.53}
 53%|█████▎    | 8109/15436 [12:45<12:53:03,  6.33s/it] 53%|█████▎    | 8110/15436 [12:53<13:43:54,  6.75s/it]                                                       {'loss': 1.0208, 'grad_norm': 0.7758211528417981, 'learning_rate': 9.664333153649988e-06, 'epoch': 0.53}
 53%|█████▎    | 8110/15436 [12:53<13:43:54,  6.75s/it] 53%|█████▎    | 8111/15436 [13:00<14:01:57,  6.90s/it]                                                       {'loss': 0.8602, 'grad_norm': 0.6410076342390281, 'learning_rate': 9.662236031544187e-06, 'epoch': 0.53}
 53%|█████▎    | 8111/15436 [13:00<14:01:57,  6.90s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2944 > 2048). Running this sequence through the model will result in indexing errors
 53%|█████▎    | 8112/15436 [13:08<14:47:54,  7.27s/it]                                                       {'loss': 0.9176, 'grad_norm': 0.7675416155519929, 'learning_rate': 9.660138924309838e-06, 'epoch': 0.53}
 53%|█████▎    | 8112/15436 [13:08<14:47:54,  7.27s/it] 53%|█████▎    | 8113/15436 [13:14<14:06:58,  6.94s/it]                                                       {'loss': 0.8799, 'grad_norm': 0.7861289635437486, 'learning_rate': 9.658041832039273e-06, 'epoch': 0.53}
 53%|█████▎    | 8113/15436 [13:14<14:06:58,  6.94s/it] 53%|█████▎    | 8114/15436 [13:20<13:07:37,  6.45s/it]                                                       {'loss': 0.8619, 'grad_norm': 0.7576052628537215, 'learning_rate': 9.655944754824833e-06, 'epoch': 0.53}
 53%|█████▎    | 8114/15436 [13:20<13:07:37,  6.45s/it] 53%|█████▎    | 8115/15436 [13:26<13:09:06,  6.47s/it]                                                       {'loss': 0.9095, 'grad_norm': 0.7609196450383328, 'learning_rate': 9.653847692758845e-06, 'epoch': 0.53}
 53%|█████▎    | 8115/15436 [13:26<13:09:06,  6.47s/it] 53%|█████▎    | 8116/15436 [13:31<11:57:49,  5.88s/it]                                                       {'loss': 0.929, 'grad_norm': 0.7613064470666384, 'learning_rate': 9.65175064593364e-06, 'epoch': 0.53}
 53%|█████▎    | 8116/15436 [13:31<11:57:49,  5.88s/it] 53%|█████▎    | 8117/15436 [13:39<13:24:15,  6.59s/it]                                                       {'loss': 0.9069, 'grad_norm': 0.8092164452630449, 'learning_rate': 9.649653614441554e-06, 'epoch': 0.53}
 53%|█████▎    | 8117/15436 [13:39<13:24:15,  6.59s/it] 53%|█████▎    | 8118/15436 [13:44<12:23:56,  6.10s/it]                                                       {'loss': 0.9316, 'grad_norm': 0.7045171861212353, 'learning_rate': 9.647556598374915e-06, 'epoch': 0.53}
 53%|█████▎    | 8118/15436 [13:44<12:23:56,  6.10s/it] 53%|█████▎    | 8119/15436 [13:51<13:08:26,  6.47s/it]                                                       {'loss': 0.9125, 'grad_norm': 0.7475777942237619, 'learning_rate': 9.645459597826048e-06, 'epoch': 0.53}
 53%|█████▎    | 8119/15436 [13:51<13:08:26,  6.47s/it] 53%|█████▎    | 8120/15436 [13:58<13:28:40,  6.63s/it]                                                       {'loss': 0.8489, 'grad_norm': 0.7221872754786053, 'learning_rate': 9.64336261288729e-06, 'epoch': 0.53}
 53%|█████▎    | 8120/15436 [13:58<13:28:40,  6.63s/it] 53%|█████▎    | 8121/15436 [14:04<13:04:42,  6.44s/it]                                                       {'loss': 0.8787, 'grad_norm': 0.7813173125539009, 'learning_rate': 9.641265643650968e-06, 'epoch': 0.53}
 53%|█████▎    | 8121/15436 [14:04<13:04:42,  6.44s/it] 53%|█████▎    | 8122/15436 [14:10<12:34:21,  6.19s/it]                                                       {'loss': 0.8829, 'grad_norm': 0.7361585776432025, 'learning_rate': 9.639168690209401e-06, 'epoch': 0.53}
 53%|█████▎    | 8122/15436 [14:10<12:34:21,  6.19s/it] 53%|█████▎    | 8123/15436 [14:16<12:26:51,  6.13s/it]                                                       {'loss': 0.938, 'grad_norm': 0.7290001951808938, 'learning_rate': 9.63707175265493e-06, 'epoch': 0.53}
 53%|█████▎    | 8123/15436 [14:16<12:26:51,  6.13s/it] 53%|█████▎    | 8124/15436 [14:22<12:20:08,  6.07s/it]                                                       {'loss': 0.9576, 'grad_norm': 0.753709209899791, 'learning_rate': 9.634974831079868e-06, 'epoch': 0.53}
 53%|█████▎    | 8124/15436 [14:22<12:20:08,  6.07s/it] 53%|█████▎    | 8125/15436 [14:30<13:41:00,  6.74s/it]                                                       {'loss': 0.8859, 'grad_norm': 0.7925962845267034, 'learning_rate': 9.632877925576553e-06, 'epoch': 0.53}
 53%|█████▎    | 8125/15436 [14:30<13:41:00,  6.74s/it] 53%|█████▎    | 8126/15436 [14:38<14:39:35,  7.22s/it]                                                       {'loss': 0.9703, 'grad_norm': 0.8369980092701201, 'learning_rate': 9.630781036237302e-06, 'epoch': 0.53}
 53%|█████▎    | 8126/15436 [14:38<14:39:35,  7.22s/it] 53%|█████▎    | 8127/15436 [14:46<15:12:02,  7.49s/it]                                                       {'loss': 0.8801, 'grad_norm': 0.6909675806838008, 'learning_rate': 9.628684163154437e-06, 'epoch': 0.53}
 53%|█████▎    | 8127/15436 [14:46<15:12:02,  7.49s/it] 53%|█████▎    | 8128/15436 [14:51<13:16:17,  6.54s/it]                                                       {'loss': 0.8882, 'grad_norm': 0.7599216182857274, 'learning_rate': 9.626587306420287e-06, 'epoch': 0.53}
 53%|█████▎    | 8128/15436 [14:51<13:16:17,  6.54s/it] 53%|█████▎    | 8129/15436 [15:00<15:10:36,  7.48s/it]                                                       {'loss': 0.8652, 'grad_norm': 0.8120481422821101, 'learning_rate': 9.624490466127176e-06, 'epoch': 0.53}
 53%|█████▎    | 8129/15436 [15:00<15:10:36,  7.48s/it] 53%|█████▎    | 8130/15436 [15:09<15:47:23,  7.78s/it]                                                       {'loss': 0.8347, 'grad_norm': 0.7918562775187723, 'learning_rate': 9.62239364236742e-06, 'epoch': 0.53}
 53%|█████▎    | 8130/15436 [15:09<15:47:23,  7.78s/it] 53%|█████▎    | 8131/15436 [15:18<16:34:01,  8.16s/it]                                                       {'loss': 0.969, 'grad_norm': 0.6974752242282007, 'learning_rate': 9.620296835233344e-06, 'epoch': 0.53}
 53%|█████▎    | 8131/15436 [15:18<16:34:01,  8.16s/it] 53%|█████▎    | 8132/15436 [15:24<15:12:14,  7.49s/it]                                                       {'loss': 0.8875, 'grad_norm': 0.8065113568760088, 'learning_rate': 9.61820004481727e-06, 'epoch': 0.53}
 53%|█████▎    | 8132/15436 [15:24<15:12:14,  7.49s/it] 53%|█████▎    | 8133/15436 [15:29<13:52:34,  6.84s/it]                                                       {'loss': 0.9275, 'grad_norm': 0.7286881403038132, 'learning_rate': 9.616103271211512e-06, 'epoch': 0.53}
 53%|█████▎    | 8133/15436 [15:29<13:52:34,  6.84s/it] 53%|█████▎    | 8134/15436 [15:38<15:11:08,  7.49s/it]                                                       {'loss': 0.885, 'grad_norm': 0.7416228881432104, 'learning_rate': 9.614006514508398e-06, 'epoch': 0.53}
 53%|█████▎    | 8134/15436 [15:38<15:11:08,  7.49s/it] 53%|█████▎    | 8135/15436 [15:44<14:16:15,  7.04s/it]                                                       {'loss': 0.8767, 'grad_norm': 0.7414891827303407, 'learning_rate': 9.61190977480024e-06, 'epoch': 0.53}
 53%|█████▎    | 8135/15436 [15:44<14:16:15,  7.04s/it] 53%|█████▎    | 8136/15436 [15:50<13:35:57,  6.71s/it]                                                       {'loss': 0.9313, 'grad_norm': 0.722752996430837, 'learning_rate': 9.609813052179354e-06, 'epoch': 0.53}
 53%|█████▎    | 8136/15436 [15:50<13:35:57,  6.71s/it] 53%|█████▎    | 8137/15436 [15:55<12:15:31,  6.05s/it]                                                       {'loss': 0.9242, 'grad_norm': 0.780360955345951, 'learning_rate': 9.607716346738065e-06, 'epoch': 0.53}
 53%|█████▎    | 8137/15436 [15:55<12:15:31,  6.05s/it] 53%|█████▎    | 8138/15436 [16:00<12:04:13,  5.95s/it]                                                       {'loss': 0.9088, 'grad_norm': 0.7293911809401254, 'learning_rate': 9.605619658568681e-06, 'epoch': 0.53}
 53%|█████▎    | 8138/15436 [16:00<12:04:13,  5.95s/it] 53%|█████▎    | 8139/15436 [16:06<11:55:38,  5.88s/it]                                                       {'loss': 0.8768, 'grad_norm': 0.8166459927472077, 'learning_rate': 9.60352298776352e-06, 'epoch': 0.53}
 53%|█████▎    | 8139/15436 [16:06<11:55:38,  5.88s/it] 53%|█████▎    | 8140/15436 [16:15<13:28:33,  6.65s/it]                                                       {'loss': 0.8734, 'grad_norm': 0.7845600778925519, 'learning_rate': 9.601426334414898e-06, 'epoch': 0.53}
 53%|█████▎    | 8140/15436 [16:15<13:28:33,  6.65s/it] 53%|█████▎    | 8141/15436 [16:20<12:52:49,  6.36s/it]                                                       {'loss': 0.8978, 'grad_norm': 0.7444500293715874, 'learning_rate': 9.599329698615123e-06, 'epoch': 0.53}
 53%|█████▎    | 8141/15436 [16:20<12:52:49,  6.36s/it] 53%|█████▎    | 8142/15436 [16:25<11:48:21,  5.83s/it]                                                       {'loss': 0.8837, 'grad_norm': 0.773399583823074, 'learning_rate': 9.597233080456521e-06, 'epoch': 0.53}
 53%|█████▎    | 8142/15436 [16:25<11:48:21,  5.83s/it] 53%|█████▎    | 8143/15436 [16:29<10:54:41,  5.39s/it]                                                       {'loss': 0.9653, 'grad_norm': 0.941907835973675, 'learning_rate': 9.595136480031394e-06, 'epoch': 0.53}
 53%|█████▎    | 8143/15436 [16:29<10:54:41,  5.39s/it] 53%|█████▎    | 8144/15436 [16:37<12:09:19,  6.00s/it]                                                       {'loss': 0.8593, 'grad_norm': 0.7166022156360506, 'learning_rate': 9.593039897432053e-06, 'epoch': 0.53}
 53%|█████▎    | 8144/15436 [16:37<12:09:19,  6.00s/it] 53%|█████▎    | 8145/15436 [16:44<13:16:58,  6.56s/it]                                                       {'loss': 0.9524, 'grad_norm': 0.7116510573764799, 'learning_rate': 9.590943332750813e-06, 'epoch': 0.53}
 53%|█████▎    | 8145/15436 [16:44<13:16:58,  6.56s/it] 53%|█████▎    | 8146/15436 [16:49<12:05:45,  5.97s/it]                                                       {'loss': 0.9436, 'grad_norm': 0.7467743909678192, 'learning_rate': 9.588846786079982e-06, 'epoch': 0.53}
 53%|█████▎    | 8146/15436 [16:49<12:05:45,  5.97s/it] 53%|█████▎    | 8147/15436 [16:55<12:04:40,  5.97s/it]                                                       {'loss': 0.9306, 'grad_norm': 0.8225528138542131, 'learning_rate': 9.586750257511868e-06, 'epoch': 0.53}
 53%|█████▎    | 8147/15436 [16:55<12:04:40,  5.97s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2096 > 2048). Running this sequence through the model will result in indexing errors
 53%|█████▎    | 8148/15436 [17:02<12:53:38,  6.37s/it]                                                       {'loss': 0.9239, 'grad_norm': 0.7368960139722659, 'learning_rate': 9.584653747138784e-06, 'epoch': 0.53}
 53%|█████▎    | 8148/15436 [17:02<12:53:38,  6.37s/it] 53%|█████▎    | 8149/15436 [17:11<14:18:55,  7.07s/it]                                                       {'loss': 0.9692, 'grad_norm': 0.7201955346719299, 'learning_rate': 9.582557255053033e-06, 'epoch': 0.53}
 53%|█████▎    | 8149/15436 [17:11<14:18:55,  7.07s/it] 53%|█████▎    | 8150/15436 [17:18<13:57:00,  6.89s/it]                                                       {'loss': 0.8951, 'grad_norm': 0.7447925600014036, 'learning_rate': 9.58046078134692e-06, 'epoch': 0.53}
 53%|█████▎    | 8150/15436 [17:18<13:57:00,  6.89s/it] 53%|█████▎    | 8151/15436 [17:24<13:38:14,  6.74s/it]                                                       {'loss': 0.8201, 'grad_norm': 0.7092535353298116, 'learning_rate': 9.578364326112756e-06, 'epoch': 0.53}
 53%|█████▎    | 8151/15436 [17:24<13:38:14,  6.74s/it] 53%|█████▎    | 8152/15436 [17:34<15:46:16,  7.79s/it]                                                       {'loss': 0.908, 'grad_norm': 0.7453882969829743, 'learning_rate': 9.576267889442848e-06, 'epoch': 0.53}
 53%|█████▎    | 8152/15436 [17:34<15:46:16,  7.79s/it] 53%|█████▎    | 8153/15436 [17:41<15:17:08,  7.56s/it]                                                       {'loss': 0.9402, 'grad_norm': 0.7365353941796144, 'learning_rate': 9.57417147142949e-06, 'epoch': 0.53}
 53%|█████▎    | 8153/15436 [17:41<15:17:08,  7.56s/it] 53%|█████▎    | 8154/15436 [17:46<13:24:52,  6.63s/it]                                                       {'loss': 0.9561, 'grad_norm': 0.8022442662133917, 'learning_rate': 9.572075072164997e-06, 'epoch': 0.53}
 53%|█████▎    | 8154/15436 [17:46<13:24:52,  6.63s/it] 53%|█████▎    | 8155/15436 [17:50<12:04:46,  5.97s/it]                                                       {'loss': 0.896, 'grad_norm': 0.8039315395718359, 'learning_rate': 9.569978691741665e-06, 'epoch': 0.53}
 53%|█████▎    | 8155/15436 [17:50<12:04:46,  5.97s/it] 53%|█████▎    | 8156/15436 [17:56<11:53:28,  5.88s/it]                                                       {'loss': 0.8825, 'grad_norm': 0.8017987532259478, 'learning_rate': 9.567882330251794e-06, 'epoch': 0.53}
 53%|█████▎    | 8156/15436 [17:56<11:53:28,  5.88s/it] 53%|█████▎    | 8157/15436 [18:02<12:24:54,  6.14s/it]                                                       {'loss': 0.8832, 'grad_norm': 0.670434885414323, 'learning_rate': 9.565785987787692e-06, 'epoch': 0.53}
 53%|█████▎    | 8157/15436 [18:02<12:24:54,  6.14s/it] 53%|█████▎    | 8158/15436 [18:10<13:09:08,  6.51s/it]                                                       {'loss': 0.8157, 'grad_norm': 0.7122714213416804, 'learning_rate': 9.56368966444165e-06, 'epoch': 0.53}
 53%|█████▎    | 8158/15436 [18:10<13:09:08,  6.51s/it] 53%|█████▎    | 8159/15436 [18:15<12:21:10,  6.11s/it]                                                       {'loss': 0.9324, 'grad_norm': 0.693045070203623, 'learning_rate': 9.561593360305977e-06, 'epoch': 0.53}
 53%|█████▎    | 8159/15436 [18:15<12:21:10,  6.11s/it] 53%|█████▎    | 8160/15436 [18:23<13:31:11,  6.69s/it]                                                       {'loss': 0.8526, 'grad_norm': 0.6763208081322901, 'learning_rate': 9.559497075472967e-06, 'epoch': 0.53}
 53%|█████▎    | 8160/15436 [18:23<13:31:11,  6.69s/it] 53%|█████▎    | 8161/15436 [18:34<16:19:47,  8.08s/it]                                                       {'loss': 0.9041, 'grad_norm': 0.6431710044893301, 'learning_rate': 9.557400810034915e-06, 'epoch': 0.53}
 53%|█████▎    | 8161/15436 [18:34<16:19:47,  8.08s/it] 53%|█████▎    | 8162/15436 [18:43<16:33:27,  8.19s/it]                                                       {'loss': 0.9954, 'grad_norm': 0.6900270334499536, 'learning_rate': 9.555304564084125e-06, 'epoch': 0.53}
 53%|█████▎    | 8162/15436 [18:43<16:33:27,  8.19s/it] 53%|█████▎    | 8163/15436 [18:50<16:12:09,  8.02s/it]                                                       {'loss': 0.9484, 'grad_norm': 0.8091655366879181, 'learning_rate': 9.553208337712885e-06, 'epoch': 0.53}
 53%|█████▎    | 8163/15436 [18:50<16:12:09,  8.02s/it] 53%|█████▎    | 8164/15436 [19:00<16:55:18,  8.38s/it]                                                       {'loss': 0.8892, 'grad_norm': 0.7502810453790104, 'learning_rate': 9.551112131013493e-06, 'epoch': 0.53}
 53%|█████▎    | 8164/15436 [19:00<16:55:18,  8.38s/it] 53%|█████▎    | 8165/15436 [19:06<15:32:28,  7.69s/it]                                                       {'loss': 0.9613, 'grad_norm': 0.6719764601878125, 'learning_rate': 9.549015944078244e-06, 'epoch': 0.53}
 53%|█████▎    | 8165/15436 [19:06<15:32:28,  7.69s/it] 53%|█████▎    | 8166/15436 [19:14<15:33:39,  7.71s/it]                                                       {'loss': 0.9919, 'grad_norm': 0.7198364565423908, 'learning_rate': 9.546919776999435e-06, 'epoch': 0.53}
 53%|█████▎    | 8166/15436 [19:14<15:33:39,  7.71s/it] 53%|█████▎    | 8167/15436 [19:19<14:06:00,  6.98s/it]                                                       {'loss': 1.0208, 'grad_norm': 0.7455624830409252, 'learning_rate': 9.544823629869346e-06, 'epoch': 0.53}
 53%|█████▎    | 8167/15436 [19:19<14:06:00,  6.98s/it] 53%|█████▎    | 8168/15436 [19:26<13:59:50,  6.93s/it]                                                       {'loss': 0.9513, 'grad_norm': 0.6559489856109213, 'learning_rate': 9.542727502780285e-06, 'epoch': 0.53}
 53%|█████▎    | 8168/15436 [19:26<13:59:50,  6.93s/it] 53%|█████▎    | 8169/15436 [19:31<12:58:51,  6.43s/it]                                                       {'loss': 0.8982, 'grad_norm': 0.7807436264346349, 'learning_rate': 9.540631395824532e-06, 'epoch': 0.53}
 53%|█████▎    | 8169/15436 [19:31<12:58:51,  6.43s/it] 53%|█████▎    | 8170/15436 [19:39<13:50:41,  6.86s/it]                                                       {'loss': 0.809, 'grad_norm': 0.7031594061641477, 'learning_rate': 9.538535309094375e-06, 'epoch': 0.53}
 53%|█████▎    | 8170/15436 [19:39<13:50:41,  6.86s/it] 53%|█████▎    | 8171/15436 [19:45<13:24:56,  6.65s/it]                                                       {'loss': 0.8386, 'grad_norm': 0.8023429323037108, 'learning_rate': 9.53643924268211e-06, 'epoch': 0.53}
 53%|█████▎    | 8171/15436 [19:45<13:24:56,  6.65s/it] 53%|█████▎    | 8172/15436 [19:52<13:42:49,  6.80s/it]                                                       {'loss': 0.8329, 'grad_norm': 0.760875884737081, 'learning_rate': 9.53434319668002e-06, 'epoch': 0.53}
 53%|█████▎    | 8172/15436 [19:52<13:42:49,  6.80s/it] 53%|█████▎    | 8173/15436 [19:59<14:01:19,  6.95s/it]                                                       {'loss': 0.9777, 'grad_norm': 0.7371729520028681, 'learning_rate': 9.532247171180395e-06, 'epoch': 0.53}
 53%|█████▎    | 8173/15436 [19:59<14:01:19,  6.95s/it] 53%|█████▎    | 8174/15436 [20:05<13:12:27,  6.55s/it]                                                       {'loss': 0.8429, 'grad_norm': 0.7133730392408894, 'learning_rate': 9.530151166275524e-06, 'epoch': 0.53}
 53%|█████▎    | 8174/15436 [20:05<13:12:27,  6.55s/it] 53%|█████▎    | 8175/15436 [20:11<13:09:55,  6.53s/it]                                                       {'loss': 0.9654, 'grad_norm': 0.7908898395801952, 'learning_rate': 9.528055182057684e-06, 'epoch': 0.53}
 53%|█████▎    | 8175/15436 [20:11<13:09:55,  6.53s/it] 53%|█████▎    | 8176/15436 [20:19<13:44:16,  6.81s/it]                                                       {'loss': 0.997, 'grad_norm': 0.8166677353965591, 'learning_rate': 9.525959218619164e-06, 'epoch': 0.53}
 53%|█████▎    | 8176/15436 [20:19<13:44:16,  6.81s/it] 53%|█████▎    | 8177/15436 [20:27<14:25:37,  7.15s/it]                                                       {'loss': 0.9806, 'grad_norm': 0.8891284618863603, 'learning_rate': 9.52386327605225e-06, 'epoch': 0.53}
 53%|█████▎    | 8177/15436 [20:27<14:25:37,  7.15s/it] 53%|█████▎    | 8178/15436 [20:34<14:23:46,  7.14s/it]                                                       {'loss': 0.8094, 'grad_norm': 0.6907890422049224, 'learning_rate': 9.521767354449218e-06, 'epoch': 0.53}
 53%|█████▎    | 8178/15436 [20:34<14:23:46,  7.14s/it] 53%|█████▎    | 8179/15436 [20:41<14:25:38,  7.16s/it]                                                       {'loss': 0.8769, 'grad_norm': 0.8197922344443667, 'learning_rate': 9.519671453902358e-06, 'epoch': 0.53}
 53%|█████▎    | 8179/15436 [20:41<14:25:38,  7.16s/it] 53%|█████▎    | 8180/15436 [20:46<12:53:58,  6.40s/it]                                                       {'loss': 0.9582, 'grad_norm': 0.837388141514323, 'learning_rate': 9.517575574503945e-06, 'epoch': 0.53}
 53%|█████▎    | 8180/15436 [20:46<12:53:58,  6.40s/it] 53%|█████▎    | 8181/15436 [20:52<12:42:02,  6.30s/it]                                                       {'loss': 0.8463, 'grad_norm': 0.7378182207016207, 'learning_rate': 9.515479716346255e-06, 'epoch': 0.53}
 53%|█████▎    | 8181/15436 [20:52<12:42:02,  6.30s/it] 53%|█████▎    | 8182/15436 [20:59<13:21:37,  6.63s/it]                                                       {'loss': 0.8952, 'grad_norm': 0.7592287832253746, 'learning_rate': 9.513383879521577e-06, 'epoch': 0.53}
 53%|█████▎    | 8182/15436 [20:59<13:21:37,  6.63s/it] 53%|█████▎    | 8183/15436 [21:07<13:50:14,  6.87s/it]                                                       {'loss': 0.8797, 'grad_norm': 0.6952775900544121, 'learning_rate': 9.511288064122185e-06, 'epoch': 0.53}
 53%|█████▎    | 8183/15436 [21:07<13:50:14,  6.87s/it] 53%|█████▎    | 8184/15436 [21:16<15:08:46,  7.52s/it]                                                       {'loss': 0.8582, 'grad_norm': 0.6839234815742083, 'learning_rate': 9.509192270240348e-06, 'epoch': 0.53}
 53%|█████▎    | 8184/15436 [21:16<15:08:46,  7.52s/it] 53%|█████▎    | 8185/15436 [21:22<14:21:42,  7.13s/it]                                                       {'loss': 0.8513, 'grad_norm': 0.6937021797923323, 'learning_rate': 9.507096497968354e-06, 'epoch': 0.53}
 53%|█████▎    | 8185/15436 [21:22<14:21:42,  7.13s/it] 53%|█████▎    | 8186/15436 [21:28<13:49:17,  6.86s/it]                                                       {'loss': 0.9392, 'grad_norm': 0.7324048065229395, 'learning_rate': 9.505000747398472e-06, 'epoch': 0.53}
 53%|█████▎    | 8186/15436 [21:28<13:49:17,  6.86s/it] 53%|█████▎    | 8187/15436 [21:33<12:33:07,  6.23s/it]                                                       {'loss': 1.0168, 'grad_norm': 0.753036839915015, 'learning_rate': 9.502905018622974e-06, 'epoch': 0.53}
 53%|█████▎    | 8187/15436 [21:33<12:33:07,  6.23s/it] 53%|█████▎    | 8188/15436 [21:38<11:35:42,  5.76s/it]                                                       {'loss': 0.8449, 'grad_norm': 0.7676391506982706, 'learning_rate': 9.500809311734137e-06, 'epoch': 0.53}
 53%|█████▎    | 8188/15436 [21:38<11:35:42,  5.76s/it] 53%|█████▎    | 8189/15436 [21:43<11:08:17,  5.53s/it]                                                       {'loss': 0.9554, 'grad_norm': 0.6816963229626649, 'learning_rate': 9.498713626824234e-06, 'epoch': 0.53}
 53%|█████▎    | 8189/15436 [21:43<11:08:17,  5.53s/it] 53%|█████▎    | 8190/15436 [21:54<14:42:26,  7.31s/it]                                                       {'loss': 0.916, 'grad_norm': 0.728113358759619, 'learning_rate': 9.496617963985529e-06, 'epoch': 0.53}
 53%|█████▎    | 8190/15436 [21:54<14:42:26,  7.31s/it] 53%|█████▎    | 8191/15436 [22:01<14:22:19,  7.14s/it]                                                       {'loss': 0.9423, 'grad_norm': 0.7184646878785004, 'learning_rate': 9.494522323310301e-06, 'epoch': 0.53}
 53%|█████▎    | 8191/15436 [22:01<14:22:19,  7.14s/it] 53%|█████▎    | 8192/15436 [22:06<13:20:13,  6.63s/it]                                                       {'loss': 0.845, 'grad_norm': 0.7719021238154967, 'learning_rate': 9.492426704890812e-06, 'epoch': 0.53}
 53%|█████▎    | 8192/15436 [22:06<13:20:13,  6.63s/it] 53%|█████▎    | 8193/15436 [22:12<12:47:37,  6.36s/it]                                                       {'loss': 0.8752, 'grad_norm': 0.792057039233719, 'learning_rate': 9.490331108819339e-06, 'epoch': 0.53}
 53%|█████▎    | 8193/15436 [22:12<12:47:37,  6.36s/it] 53%|█████▎    | 8194/15436 [22:18<12:51:09,  6.39s/it]                                                       {'loss': 0.9464, 'grad_norm': 0.673557388055891, 'learning_rate': 9.488235535188141e-06, 'epoch': 0.53}
 53%|█████▎    | 8194/15436 [22:18<12:51:09,  6.39s/it] 53%|█████▎    | 8195/15436 [22:27<14:24:07,  7.16s/it]                                                       {'loss': 0.7789, 'grad_norm': 0.6927911422751504, 'learning_rate': 9.486139984089486e-06, 'epoch': 0.53}
 53%|█████▎    | 8195/15436 [22:27<14:24:07,  7.16s/it] 53%|█████▎    | 8196/15436 [22:33<13:13:18,  6.57s/it]                                                       {'loss': 0.9302, 'grad_norm': 0.7851964248757755, 'learning_rate': 9.484044455615642e-06, 'epoch': 0.53}
 53%|█████▎    | 8196/15436 [22:33<13:13:18,  6.57s/it] 53%|█████▎    | 8197/15436 [22:37<11:52:25,  5.90s/it]                                                       {'loss': 0.8694, 'grad_norm': 0.7169245291103543, 'learning_rate': 9.481948949858876e-06, 'epoch': 0.53}
 53%|█████▎    | 8197/15436 [22:37<11:52:25,  5.90s/it] 53%|█████▎    | 8198/15436 [22:44<12:23:38,  6.16s/it]                                                       {'loss': 0.9155, 'grad_norm': 0.7135543975926627, 'learning_rate': 9.479853466911439e-06, 'epoch': 0.53}
 53%|█████▎    | 8198/15436 [22:44<12:23:38,  6.16s/it] 53%|█████▎    | 8199/15436 [22:51<13:14:20,  6.59s/it]                                                       {'loss': 0.8543, 'grad_norm': 0.692620851939293, 'learning_rate': 9.47775800686561e-06, 'epoch': 0.53}
 53%|█████▎    | 8199/15436 [22:51<13:14:20,  6.59s/it] 53%|█████▎    | 8200/15436 [22:57<12:55:38,  6.43s/it]                                                       {'loss': 0.9259, 'grad_norm': 0.7759151408593606, 'learning_rate': 9.475662569813637e-06, 'epoch': 0.53}
 53%|█████▎    | 8200/15436 [22:57<12:55:38,  6.43s/it] 53%|█████▎    | 8201/15436 [23:03<12:38:58,  6.29s/it]                                                       {'loss': 0.9232, 'grad_norm': 0.7074038271669914, 'learning_rate': 9.473567155847784e-06, 'epoch': 0.53}
 53%|█████▎    | 8201/15436 [23:03<12:38:58,  6.29s/it] 53%|█████▎    | 8202/15436 [23:10<12:34:14,  6.26s/it]                                                       {'loss': 0.9557, 'grad_norm': 0.698137332241659, 'learning_rate': 9.471471765060314e-06, 'epoch': 0.53}
 53%|█████▎    | 8202/15436 [23:10<12:34:14,  6.26s/it] 53%|█████▎    | 8203/15436 [23:18<14:08:10,  7.04s/it]                                                       {'loss': 0.8668, 'grad_norm': 0.7092443874327501, 'learning_rate': 9.469376397543481e-06, 'epoch': 0.53}
 53%|█████▎    | 8203/15436 [23:18<14:08:10,  7.04s/it] 53%|█████▎    | 8204/15436 [23:26<14:26:00,  7.18s/it]                                                       {'loss': 0.8966, 'grad_norm': 0.7693158841420469, 'learning_rate': 9.467281053389543e-06, 'epoch': 0.53}
 53%|█████▎    | 8204/15436 [23:26<14:26:00,  7.18s/it] 53%|█████▎    | 8205/15436 [23:31<13:21:25,  6.65s/it]                                                       {'loss': 0.9476, 'grad_norm': 0.7127079169968809, 'learning_rate': 9.46518573269076e-06, 'epoch': 0.53}
 53%|█████▎    | 8205/15436 [23:31<13:21:25,  6.65s/it] 53%|█████▎    | 8206/15436 [23:38<13:05:08,  6.52s/it]                                                       {'loss': 0.9519, 'grad_norm': 0.7363444958282124, 'learning_rate': 9.463090435539381e-06, 'epoch': 0.53}
 53%|█████▎    | 8206/15436 [23:38<13:05:08,  6.52s/it] 53%|█████▎    | 8207/15436 [23:46<14:24:37,  7.18s/it]                                                       {'loss': 0.9287, 'grad_norm': 0.745396958117715, 'learning_rate': 9.460995162027663e-06, 'epoch': 0.53}
 53%|█████▎    | 8207/15436 [23:46<14:24:37,  7.18s/it] 53%|█████▎    | 8208/15436 [23:56<15:49:59,  7.89s/it]                                                       {'loss': 0.8674, 'grad_norm': 0.6995722024099257, 'learning_rate': 9.458899912247861e-06, 'epoch': 0.53}
 53%|█████▎    | 8208/15436 [23:56<15:49:59,  7.89s/it] 53%|█████▎    | 8209/15436 [24:00<13:55:07,  6.93s/it]                                                       {'loss': 0.9233, 'grad_norm': 0.7117081176879841, 'learning_rate': 9.456804686292223e-06, 'epoch': 0.53}
 53%|█████▎    | 8209/15436 [24:00<13:55:07,  6.93s/it] 53%|█████▎    | 8210/15436 [24:09<15:06:15,  7.52s/it]                                                       {'loss': 0.8138, 'grad_norm': 0.6685566282070916, 'learning_rate': 9.454709484253006e-06, 'epoch': 0.53}
 53%|█████▎    | 8210/15436 [24:09<15:06:15,  7.52s/it] 53%|█████▎    | 8211/15436 [24:14<13:18:03,  6.63s/it]                                                       {'loss': 0.9737, 'grad_norm': 0.7357564970530984, 'learning_rate': 9.452614306222456e-06, 'epoch': 0.53}
 53%|█████▎    | 8211/15436 [24:14<13:18:03,  6.63s/it] 53%|█████▎    | 8212/15436 [24:19<12:26:32,  6.20s/it]                                                       {'loss': 0.896, 'grad_norm': 0.8605860858513839, 'learning_rate': 9.450519152292816e-06, 'epoch': 0.53}
 53%|█████▎    | 8212/15436 [24:19<12:26:32,  6.20s/it] 53%|█████▎    | 8213/15436 [24:25<12:01:11,  5.99s/it]                                                       {'loss': 0.9429, 'grad_norm': 0.78951252791021, 'learning_rate': 9.448424022556347e-06, 'epoch': 0.53}
 53%|█████▎    | 8213/15436 [24:25<12:01:11,  5.99s/it] 53%|█████▎    | 8214/15436 [24:30<11:35:56,  5.78s/it]                                                       {'loss': 0.9658, 'grad_norm': 0.7519296156695382, 'learning_rate': 9.446328917105286e-06, 'epoch': 0.53}
 53%|█████▎    | 8214/15436 [24:30<11:35:56,  5.78s/it] 53%|█████▎    | 8215/15436 [24:34<10:41:09,  5.33s/it]                                                       {'loss': 1.0022, 'grad_norm': 0.8475221922103869, 'learning_rate': 9.44423383603188e-06, 'epoch': 0.53}
 53%|█████▎    | 8215/15436 [24:34<10:41:09,  5.33s/it] 53%|█████▎    | 8216/15436 [24:47<15:10:16,  7.56s/it]                                                       {'loss': 0.8281, 'grad_norm': 0.8096844869009864, 'learning_rate': 9.442138779428376e-06, 'epoch': 0.53}
 53%|█████▎    | 8216/15436 [24:47<15:10:16,  7.56s/it] 53%|█████▎    | 8217/15436 [24:52<13:26:23,  6.70s/it]                                                       {'loss': 0.8915, 'grad_norm': 0.7859844779577901, 'learning_rate': 9.440043747387016e-06, 'epoch': 0.53}
 53%|█████▎    | 8217/15436 [24:52<13:26:23,  6.70s/it] 53%|█████▎    | 8218/15436 [24:57<12:41:40,  6.33s/it]                                                       {'loss': 0.9285, 'grad_norm': 0.7277165703560429, 'learning_rate': 9.437948740000042e-06, 'epoch': 0.53}
 53%|█████▎    | 8218/15436 [24:57<12:41:40,  6.33s/it] 53%|█████▎    | 8219/15436 [25:04<13:08:49,  6.56s/it]                                                       {'loss': 0.9267, 'grad_norm': 0.822585978014823, 'learning_rate': 9.435853757359697e-06, 'epoch': 0.53}
 53%|█████▎    | 8219/15436 [25:04<13:08:49,  6.56s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2511 > 2048). Running this sequence through the model will result in indexing errors
 53%|█████▎    | 8220/15436 [25:10<12:38:25,  6.31s/it]                                                       {'loss': 0.8929, 'grad_norm': 0.7647247522630228, 'learning_rate': 9.433758799558222e-06, 'epoch': 0.53}
 53%|█████▎    | 8220/15436 [25:10<12:38:25,  6.31s/it] 53%|█████▎    | 8221/15436 [25:16<12:15:58,  6.12s/it]                                                       {'loss': 0.9119, 'grad_norm': 0.8318822809798443, 'learning_rate': 9.43166386668785e-06, 'epoch': 0.53}
 53%|█████▎    | 8221/15436 [25:16<12:15:58,  6.12s/it] 53%|█████▎    | 8222/15436 [25:24<13:23:39,  6.68s/it]                                                       {'loss': 0.9298, 'grad_norm': 0.7009732759447707, 'learning_rate': 9.42956895884083e-06, 'epoch': 0.53}
 53%|█████▎    | 8222/15436 [25:24<13:23:39,  6.68s/it] 53%|█████▎    | 8223/15436 [25:31<13:54:26,  6.94s/it]                                                       {'loss': 0.9228, 'grad_norm': 0.7358177979336834, 'learning_rate': 9.427474076109391e-06, 'epoch': 0.53}
 53%|█████▎    | 8223/15436 [25:31<13:54:26,  6.94s/it] 53%|█████▎    | 8224/15436 [25:37<13:02:41,  6.51s/it]                                                       {'loss': 0.9018, 'grad_norm': 0.810113206357778, 'learning_rate': 9.425379218585765e-06, 'epoch': 0.53}
 53%|█████▎    | 8224/15436 [25:37<13:02:41,  6.51s/it] 53%|█████▎    | 8225/15436 [25:45<14:05:35,  7.04s/it]                                                       {'loss': 0.8757, 'grad_norm': 0.7248844326035105, 'learning_rate': 9.423284386362196e-06, 'epoch': 0.53}
 53%|█████▎    | 8225/15436 [25:45<14:05:35,  7.04s/it] 53%|█████▎    | 8226/15436 [25:52<14:15:31,  7.12s/it]                                                       {'loss': 1.0489, 'grad_norm': 0.7101911925838675, 'learning_rate': 9.421189579530912e-06, 'epoch': 0.53}
 53%|█████▎    | 8226/15436 [25:52<14:15:31,  7.12s/it] 53%|█████▎    | 8227/15436 [26:01<15:02:36,  7.51s/it]                                                       {'loss': 0.8635, 'grad_norm': 0.7829869563646472, 'learning_rate': 9.41909479818415e-06, 'epoch': 0.53}
 53%|█████▎    | 8227/15436 [26:01<15:02:36,  7.51s/it] 53%|█████▎    | 8228/15436 [26:09<15:38:43,  7.81s/it]                                                       {'loss': 0.9027, 'grad_norm': 0.8780976158465703, 'learning_rate': 9.417000042414141e-06, 'epoch': 0.53}
 53%|█████▎    | 8228/15436 [26:09<15:38:43,  7.81s/it] 53%|█████▎    | 8229/15436 [26:19<16:55:46,  8.46s/it]                                                       {'loss': 0.8845, 'grad_norm': 0.7131995117969238, 'learning_rate': 9.414905312313107e-06, 'epoch': 0.53}
 53%|█████▎    | 8229/15436 [26:19<16:55:46,  8.46s/it] 53%|█████▎    | 8230/15436 [26:26<16:13:02,  8.10s/it]                                                       {'loss': 0.9027, 'grad_norm': 0.7235949253638193, 'learning_rate': 9.41281060797329e-06, 'epoch': 0.53}
 53%|█████▎    | 8230/15436 [26:26<16:13:02,  8.10s/it] 53%|█████▎    | 8231/15436 [26:35<16:29:55,  8.24s/it]                                                       {'loss': 0.8661, 'grad_norm': 0.700534595744378, 'learning_rate': 9.41071592948691e-06, 'epoch': 0.53}
 53%|█████▎    | 8231/15436 [26:35<16:29:55,  8.24s/it] 53%|█████▎    | 8232/15436 [26:43<16:16:31,  8.13s/it]                                                       {'loss': 0.9467, 'grad_norm': 0.73899251004877, 'learning_rate': 9.40862127694619e-06, 'epoch': 0.53}
 53%|█████▎    | 8232/15436 [26:43<16:16:31,  8.13s/it] 53%|█████▎    | 8233/15436 [26:51<16:32:41,  8.27s/it]                                                       {'loss': 0.9413, 'grad_norm': 0.6645529779107509, 'learning_rate': 9.406526650443365e-06, 'epoch': 0.53}
 53%|█████▎    | 8233/15436 [26:51<16:32:41,  8.27s/it] 53%|█████▎    | 8234/15436 [26:57<15:03:06,  7.52s/it]                                                       {'loss': 0.9108, 'grad_norm': 0.7103012824497816, 'learning_rate': 9.404432050070654e-06, 'epoch': 0.53}
 53%|█████▎    | 8234/15436 [26:57<15:03:06,  7.52s/it] 53%|█████▎    | 8235/15436 [27:03<14:10:21,  7.09s/it]                                                       {'loss': 0.8971, 'grad_norm': 0.7087398834965991, 'learning_rate': 9.40233747592028e-06, 'epoch': 0.53}
 53%|█████▎    | 8235/15436 [27:03<14:10:21,  7.09s/it] 53%|█████▎    | 8236/15436 [27:09<13:26:02,  6.72s/it]                                                       {'loss': 0.9732, 'grad_norm': 0.7914477191535785, 'learning_rate': 9.40024292808447e-06, 'epoch': 0.53}
 53%|█████▎    | 8236/15436 [27:09<13:26:02,  6.72s/it] 53%|█████▎    | 8237/15436 [27:19<15:35:00,  7.79s/it]                                                       {'loss': 0.8138, 'grad_norm': 0.7212598315010226, 'learning_rate': 9.398148406655441e-06, 'epoch': 0.53}
 53%|█████▎    | 8237/15436 [27:19<15:35:00,  7.79s/it] 53%|█████▎    | 8238/15436 [27:27<15:26:28,  7.72s/it]                                                       {'loss': 0.9371, 'grad_norm': 0.8031185315690842, 'learning_rate': 9.396053911725415e-06, 'epoch': 0.53}
 53%|█████▎    | 8238/15436 [27:27<15:26:28,  7.72s/it] 53%|█████▎    | 8239/15436 [27:35<15:20:43,  7.68s/it]                                                       {'loss': 0.8152, 'grad_norm': 0.6460445247567174, 'learning_rate': 9.39395944338661e-06, 'epoch': 0.53}
 53%|█████▎    | 8239/15436 [27:35<15:20:43,  7.68s/it] 53%|█████▎    | 8240/15436 [27:42<15:18:25,  7.66s/it]                                                       {'loss': 0.8882, 'grad_norm': 0.6921570510837194, 'learning_rate': 9.391865001731242e-06, 'epoch': 0.53}
 53%|█████▎    | 8240/15436 [27:42<15:18:25,  7.66s/it] 53%|█████▎    | 8241/15436 [27:52<16:25:45,  8.22s/it]                                                       {'loss': 0.8721, 'grad_norm': 0.7290265078057339, 'learning_rate': 9.38977058685153e-06, 'epoch': 0.53}
 53%|█████▎    | 8241/15436 [27:52<16:25:45,  8.22s/it] 53%|█████▎    | 8242/15436 [27:57<14:38:36,  7.33s/it]                                                       {'loss': 0.8735, 'grad_norm': 0.805414060607241, 'learning_rate': 9.387676198839691e-06, 'epoch': 0.53}
 53%|█████▎    | 8242/15436 [27:57<14:38:36,  7.33s/it] 53%|█████▎    | 8243/15436 [28:05<14:46:31,  7.39s/it]                                                       {'loss': 0.8297, 'grad_norm': 0.7453143296568495, 'learning_rate': 9.385581837787932e-06, 'epoch': 0.53}
 53%|█████▎    | 8243/15436 [28:05<14:46:31,  7.39s/it] 53%|█████▎    | 8244/15436 [28:10<13:51:58,  6.94s/it]                                                       {'loss': 0.8347, 'grad_norm': 0.6734823630264448, 'learning_rate': 9.383487503788473e-06, 'epoch': 0.53}
 53%|█████▎    | 8244/15436 [28:10<13:51:58,  6.94s/it] 53%|█████▎    | 8245/15436 [28:19<15:05:44,  7.56s/it]                                                       {'loss': 0.9182, 'grad_norm': 0.704371285490711, 'learning_rate': 9.381393196933524e-06, 'epoch': 0.53}
 53%|█████▎    | 8245/15436 [28:19<15:05:44,  7.56s/it] 53%|█████▎    | 8246/15436 [28:25<13:56:42,  6.98s/it]                                                       {'loss': 0.8557, 'grad_norm': 0.7693170698657515, 'learning_rate': 9.379298917315292e-06, 'epoch': 0.53}
 53%|█████▎    | 8246/15436 [28:25<13:56:42,  6.98s/it] 53%|█████▎    | 8247/15436 [28:32<14:10:24,  7.10s/it]                                                       {'loss': 0.889, 'grad_norm': 0.7471275107961177, 'learning_rate': 9.37720466502599e-06, 'epoch': 0.53}
 53%|█████▎    | 8247/15436 [28:32<14:10:24,  7.10s/it] 53%|█████▎    | 8248/15436 [28:41<14:58:08,  7.50s/it]                                                       {'loss': 0.9142, 'grad_norm': 0.726340751578079, 'learning_rate': 9.375110440157827e-06, 'epoch': 0.53}
 53%|█████▎    | 8248/15436 [28:41<14:58:08,  7.50s/it] 53%|█████▎    | 8249/15436 [28:47<14:04:43,  7.05s/it]                                                       {'loss': 0.8891, 'grad_norm': 0.6831168730023353, 'learning_rate': 9.373016242803003e-06, 'epoch': 0.53}
 53%|█████▎    | 8249/15436 [28:47<14:04:43,  7.05s/it] 53%|█████▎    | 8250/15436 [28:52<13:00:32,  6.52s/it]                                                       {'loss': 0.9154, 'grad_norm': 0.806759933889032, 'learning_rate': 9.370922073053733e-06, 'epoch': 0.53}
 53%|█████▎    | 8250/15436 [28:52<13:00:32,  6.52s/it] 53%|█████▎    | 8251/15436 [29:00<13:53:37,  6.96s/it]                                                       {'loss': 0.94, 'grad_norm': 0.7453194381033617, 'learning_rate': 9.368827931002215e-06, 'epoch': 0.53}
 53%|█████▎    | 8251/15436 [29:00<13:53:37,  6.96s/it] 53%|█████▎    | 8252/15436 [29:08<14:40:45,  7.36s/it]                                                       {'loss': 0.8611, 'grad_norm': 0.8046467306275714, 'learning_rate': 9.366733816740652e-06, 'epoch': 0.53}
 53%|█████▎    | 8252/15436 [29:08<14:40:45,  7.36s/it] 53%|█████▎    | 8253/15436 [29:16<14:46:24,  7.40s/it]                                                       {'loss': 0.9758, 'grad_norm': 0.6921248172799733, 'learning_rate': 9.364639730361252e-06, 'epoch': 0.53}
 53%|█████▎    | 8253/15436 [29:16<14:46:24,  7.40s/it] 53%|█████▎    | 8254/15436 [29:22<14:00:38,  7.02s/it]                                                       {'loss': 1.0315, 'grad_norm': 0.7618625917809775, 'learning_rate': 9.362545671956213e-06, 'epoch': 0.53}
 53%|█████▎    | 8254/15436 [29:22<14:00:38,  7.02s/it] 53%|█████▎    | 8255/15436 [29:30<14:34:52,  7.31s/it]                                                       {'loss': 0.9229, 'grad_norm': 0.7019591032707978, 'learning_rate': 9.360451641617729e-06, 'epoch': 0.53}
 53%|█████▎    | 8255/15436 [29:30<14:34:52,  7.31s/it] 53%|█████▎    | 8256/15436 [29:36<13:47:51,  6.92s/it]                                                       {'loss': 0.8619, 'grad_norm': 0.7405893768797156, 'learning_rate': 9.358357639438007e-06, 'epoch': 0.53}
 53%|█████▎    | 8256/15436 [29:36<13:47:51,  6.92s/it] 53%|█████▎    | 8257/15436 [29:45<14:57:21,  7.50s/it]                                                       {'loss': 0.9312, 'grad_norm': 0.743948133430764, 'learning_rate': 9.356263665509236e-06, 'epoch': 0.53}
 53%|█████▎    | 8257/15436 [29:45<14:57:21,  7.50s/it] 53%|█████▎    | 8258/15436 [29:53<15:32:04,  7.79s/it]                                                       {'loss': 0.8205, 'grad_norm': 0.7924383574948679, 'learning_rate': 9.354169719923618e-06, 'epoch': 0.53}
 53%|█████▎    | 8258/15436 [29:53<15:32:04,  7.79s/it] 54%|█████▎    | 8259/15436 [30:00<14:39:39,  7.35s/it]                                                       {'loss': 0.8913, 'grad_norm': 0.7151452805312367, 'learning_rate': 9.35207580277335e-06, 'epoch': 0.54}
 54%|█████▎    | 8259/15436 [30:00<14:39:39,  7.35s/it] 54%|█████▎    | 8260/15436 [30:07<14:42:59,  7.38s/it]                                                       {'loss': 0.9131, 'grad_norm': 0.7970436179444778, 'learning_rate': 9.349981914150611e-06, 'epoch': 0.54}
 54%|█████▎    | 8260/15436 [30:07<14:42:59,  7.38s/it] 54%|█████▎    | 8261/15436 [30:12<12:56:30,  6.49s/it]                                                       {'loss': 0.8434, 'grad_norm': 0.7019626607514797, 'learning_rate': 9.347888054147614e-06, 'epoch': 0.54}
 54%|█████▎    | 8261/15436 [30:12<12:56:30,  6.49s/it] 54%|█████▎    | 8262/15436 [30:16<11:43:57,  5.89s/it]                                                       {'loss': 0.9743, 'grad_norm': 0.789912321365044, 'learning_rate': 9.345794222856533e-06, 'epoch': 0.54}
 54%|█████▎    | 8262/15436 [30:16<11:43:57,  5.89s/it] 54%|█████▎    | 8263/15436 [30:24<12:54:32,  6.48s/it]                                                       {'loss': 0.8759, 'grad_norm': 0.7319067882974717, 'learning_rate': 9.343700420369563e-06, 'epoch': 0.54}
 54%|█████▎    | 8263/15436 [30:24<12:54:32,  6.48s/it] 54%|█████▎    | 8264/15436 [30:33<14:41:43,  7.38s/it]                                                       {'loss': 0.9488, 'grad_norm': 0.8314774031499215, 'learning_rate': 9.341606646778895e-06, 'epoch': 0.54}
 54%|█████▎    | 8264/15436 [30:33<14:41:43,  7.38s/it] 54%|█████▎    | 8265/15436 [30:38<12:57:35,  6.51s/it]                                                       {'loss': 0.9277, 'grad_norm': 0.8388757657825587, 'learning_rate': 9.339512902176714e-06, 'epoch': 0.54}
 54%|█████▎    | 8265/15436 [30:38<12:57:35,  6.51s/it] 54%|█████▎    | 8266/15436 [30:45<13:11:12,  6.62s/it]                                                       {'loss': 0.8772, 'grad_norm': 0.7782008384532318, 'learning_rate': 9.337419186655203e-06, 'epoch': 0.54}
 54%|█████▎    | 8266/15436 [30:45<13:11:12,  6.62s/it] 54%|█████▎    | 8267/15436 [30:50<12:31:24,  6.29s/it]                                                       {'loss': 0.891, 'grad_norm': 0.6927487845522874, 'learning_rate': 9.33532550030655e-06, 'epoch': 0.54}
 54%|█████▎    | 8267/15436 [30:50<12:31:24,  6.29s/it] 54%|█████▎    | 8268/15436 [30:55<11:36:39,  5.83s/it]                                                       {'loss': 0.9217, 'grad_norm': 0.7397895510020133, 'learning_rate': 9.333231843222938e-06, 'epoch': 0.54}
 54%|█████▎    | 8268/15436 [30:55<11:36:39,  5.83s/it] 54%|█████▎    | 8269/15436 [31:05<13:56:40,  7.00s/it]                                                       {'loss': 0.9735, 'grad_norm': 0.7246862036582216, 'learning_rate': 9.331138215496544e-06, 'epoch': 0.54}
 54%|█████▎    | 8269/15436 [31:05<13:56:40,  7.00s/it] 54%|█████▎    | 8270/15436 [31:11<13:25:25,  6.74s/it]                                                       {'loss': 0.8852, 'grad_norm': 0.704592623357486, 'learning_rate': 9.329044617219558e-06, 'epoch': 0.54}
 54%|█████▎    | 8270/15436 [31:11<13:25:25,  6.74s/it] 54%|█████▎    | 8271/15436 [31:18<13:36:13,  6.84s/it]                                                       {'loss': 0.8879, 'grad_norm': 0.7959819616287386, 'learning_rate': 9.326951048484154e-06, 'epoch': 0.54}
 54%|█████▎    | 8271/15436 [31:18<13:36:13,  6.84s/it] 54%|█████▎    | 8272/15436 [31:26<14:10:29,  7.12s/it]                                                       {'loss': 0.8619, 'grad_norm': 0.6926232913644262, 'learning_rate': 9.324857509382504e-06, 'epoch': 0.54}
 54%|█████▎    | 8272/15436 [31:26<14:10:29,  7.12s/it] 54%|█████▎    | 8273/15436 [31:31<13:13:46,  6.65s/it]                                                       {'loss': 0.8529, 'grad_norm': 0.7115898691977142, 'learning_rate': 9.322764000006798e-06, 'epoch': 0.54}
 54%|█████▎    | 8273/15436 [31:31<13:13:46,  6.65s/it] 54%|█████▎    | 8274/15436 [31:36<11:47:40,  5.93s/it]                                                       {'loss': 0.9694, 'grad_norm': 0.7334297725031527, 'learning_rate': 9.320670520449196e-06, 'epoch': 0.54}
 54%|█████▎    | 8274/15436 [31:36<11:47:40,  5.93s/it] 54%|█████▎    | 8275/15436 [31:43<12:30:08,  6.29s/it]                                                       {'loss': 0.929, 'grad_norm': 0.8048800432116204, 'learning_rate': 9.318577070801888e-06, 'epoch': 0.54}
 54%|█████▎    | 8275/15436 [31:43<12:30:08,  6.29s/it] 54%|█████▎    | 8276/15436 [31:54<15:20:41,  7.72s/it]                                                       {'loss': 0.8758, 'grad_norm': 0.6881983319333671, 'learning_rate': 9.316483651157037e-06, 'epoch': 0.54}
 54%|█████▎    | 8276/15436 [31:54<15:20:41,  7.72s/it] 54%|█████▎    | 8277/15436 [32:00<14:35:03,  7.33s/it]                                                       {'loss': 1.0112, 'grad_norm': 0.6899164260164782, 'learning_rate': 9.314390261606813e-06, 'epoch': 0.54}
 54%|█████▎    | 8277/15436 [32:00<14:35:03,  7.33s/it] 54%|█████▎    | 8278/15436 [32:05<12:55:19,  6.50s/it]                                                       {'loss': 0.915, 'grad_norm': 0.7628446402997988, 'learning_rate': 9.312296902243393e-06, 'epoch': 0.54}
 54%|█████▎    | 8278/15436 [32:05<12:55:19,  6.50s/it] 54%|█████▎    | 8279/15436 [32:11<12:37:41,  6.35s/it]                                                       {'loss': 0.8659, 'grad_norm': 0.7049930973494227, 'learning_rate': 9.310203573158941e-06, 'epoch': 0.54}
 54%|█████▎    | 8279/15436 [32:11<12:37:41,  6.35s/it] 54%|█████▎    | 8280/15436 [32:15<11:41:33,  5.88s/it]                                                       {'loss': 0.9167, 'grad_norm': 0.7279404797241547, 'learning_rate': 9.308110274445625e-06, 'epoch': 0.54}
 54%|█████▎    | 8280/15436 [32:15<11:41:33,  5.88s/it] 54%|█████▎    | 8281/15436 [32:24<13:07:53,  6.61s/it]                                                       {'loss': 0.9265, 'grad_norm': 0.773853964616937, 'learning_rate': 9.306017006195613e-06, 'epoch': 0.54}
 54%|█████▎    | 8281/15436 [32:24<13:07:53,  6.61s/it] 54%|█████▎    | 8282/15436 [32:33<14:39:16,  7.37s/it]                                                       {'loss': 0.8611, 'grad_norm': 0.730286352003094, 'learning_rate': 9.303923768501069e-06, 'epoch': 0.54}
 54%|█████▎    | 8282/15436 [32:33<14:39:16,  7.37s/it] 54%|█████▎    | 8283/15436 [32:41<14:53:47,  7.50s/it]                                                       {'loss': 0.9543, 'grad_norm': 0.7921278816071526, 'learning_rate': 9.301830561454153e-06, 'epoch': 0.54}
 54%|█████▎    | 8283/15436 [32:41<14:53:47,  7.50s/it] 54%|█████▎    | 8284/15436 [32:49<15:21:32,  7.73s/it]                                                       {'loss': 0.8053, 'grad_norm': 0.7913757377071424, 'learning_rate': 9.299737385147033e-06, 'epoch': 0.54}
 54%|█████▎    | 8284/15436 [32:49<15:21:32,  7.73s/it] 54%|█████▎    | 8285/15436 [32:54<14:00:21,  7.05s/it]                                                       {'loss': 0.863, 'grad_norm': 0.7073756201506639, 'learning_rate': 9.29764423967187e-06, 'epoch': 0.54}
 54%|█████▎    | 8285/15436 [32:54<14:00:21,  7.05s/it] 54%|█████▎    | 8286/15436 [33:00<13:14:50,  6.67s/it]                                                       {'loss': 0.8079, 'grad_norm': 0.7041919840177595, 'learning_rate': 9.295551125120812e-06, 'epoch': 0.54}
 54%|█████▎    | 8286/15436 [33:00<13:14:50,  6.67s/it] 54%|█████▎    | 8287/15436 [33:07<13:32:36,  6.82s/it]                                                       {'loss': 0.8998, 'grad_norm': 0.737121740807982, 'learning_rate': 9.293458041586033e-06, 'epoch': 0.54}
 54%|█████▎    | 8287/15436 [33:07<13:32:36,  6.82s/it] 54%|█████▎    | 8288/15436 [33:12<12:23:45,  6.24s/it]                                                       {'loss': 0.9256, 'grad_norm': 0.7652193300109316, 'learning_rate': 9.291364989159679e-06, 'epoch': 0.54}
 54%|█████▎    | 8288/15436 [33:12<12:23:45,  6.24s/it] 54%|█████▎    | 8289/15436 [33:21<13:41:09,  6.89s/it]                                                       {'loss': 0.8913, 'grad_norm': 0.7154218997985782, 'learning_rate': 9.289271967933907e-06, 'epoch': 0.54}
 54%|█████▎    | 8289/15436 [33:21<13:41:09,  6.89s/it] 54%|█████▎    | 8290/15436 [33:27<13:27:27,  6.78s/it]                                                       {'loss': 0.8179, 'grad_norm': 0.8345011416790037, 'learning_rate': 9.287178978000873e-06, 'epoch': 0.54}
 54%|█████▎    | 8290/15436 [33:27<13:27:27,  6.78s/it] 54%|█████▎    | 8291/15436 [33:36<14:45:21,  7.43s/it]                                                       {'loss': 0.9669, 'grad_norm': 0.7177227940242842, 'learning_rate': 9.285086019452729e-06, 'epoch': 0.54}
 54%|█████▎    | 8291/15436 [33:36<14:45:21,  7.43s/it] 54%|█████▎    | 8292/15436 [33:44<15:11:50,  7.66s/it]                                                       {'loss': 0.9406, 'grad_norm': 0.7673237443594805, 'learning_rate': 9.282993092381626e-06, 'epoch': 0.54}
 54%|█████▎    | 8292/15436 [33:44<15:11:50,  7.66s/it] 54%|█████▎    | 8293/15436 [33:54<16:19:00,  8.22s/it]                                                       {'loss': 0.9017, 'grad_norm': 0.7308685361283653, 'learning_rate': 9.280900196879713e-06, 'epoch': 0.54}
 54%|█████▎    | 8293/15436 [33:54<16:19:00,  8.22s/it] 54%|█████▎    | 8294/15436 [33:59<14:43:34,  7.42s/it]                                                       {'loss': 0.895, 'grad_norm': 0.679075121437456, 'learning_rate': 9.278807333039137e-06, 'epoch': 0.54}
 54%|█████▎    | 8294/15436 [33:59<14:43:34,  7.42s/it] 54%|█████▎    | 8295/15436 [34:06<14:20:00,  7.23s/it]                                                       {'loss': 0.9056, 'grad_norm': 0.8214082778785666, 'learning_rate': 9.276714500952052e-06, 'epoch': 0.54}
 54%|█████▎    | 8295/15436 [34:06<14:20:00,  7.23s/it] 54%|█████▎    | 8296/15436 [34:11<12:47:55,  6.45s/it]                                                       {'loss': 0.9423, 'grad_norm': 0.7471761267707857, 'learning_rate': 9.274621700710594e-06, 'epoch': 0.54}
 54%|█████▎    | 8296/15436 [34:11<12:47:55,  6.45s/it] 54%|█████▍    | 8297/15436 [34:16<12:16:35,  6.19s/it]                                                       {'loss': 0.8194, 'grad_norm': 0.7574442854173996, 'learning_rate': 9.272528932406912e-06, 'epoch': 0.54}
 54%|█████▍    | 8297/15436 [34:16<12:16:35,  6.19s/it] 54%|█████▍    | 8298/15436 [34:27<14:33:35,  7.34s/it]                                                       {'loss': 0.8608, 'grad_norm': 0.7470197038295721, 'learning_rate': 9.27043619613315e-06, 'epoch': 0.54}
 54%|█████▍    | 8298/15436 [34:27<14:33:35,  7.34s/it] 54%|█████▍    | 8299/15436 [34:32<13:19:58,  6.73s/it]                                                       {'loss': 0.8905, 'grad_norm': 0.686616287615317, 'learning_rate': 9.26834349198145e-06, 'epoch': 0.54}
 54%|█████▍    | 8299/15436 [34:32<13:19:58,  6.73s/it] 54%|█████▍    | 8300/15436 [34:39<13:41:32,  6.91s/it]                                                       {'loss': 0.8395, 'grad_norm': 0.7691365787575991, 'learning_rate': 9.266250820043945e-06, 'epoch': 0.54}
 54%|█████▍    | 8300/15436 [34:39<13:41:32,  6.91s/it] 54%|█████▍    | 8301/15436 [34:45<13:18:53,  6.72s/it]                                                       {'loss': 0.8331, 'grad_norm': 0.7281918237435243, 'learning_rate': 9.26415818041278e-06, 'epoch': 0.54}
 54%|█████▍    | 8301/15436 [34:45<13:18:53,  6.72s/it] 54%|█████▍    | 8302/15436 [34:52<13:10:35,  6.65s/it]                                                       {'loss': 0.7853, 'grad_norm': 0.7373203907921994, 'learning_rate': 9.262065573180094e-06, 'epoch': 0.54}
 54%|█████▍    | 8302/15436 [34:52<13:10:35,  6.65s/it] 54%|█████▍    | 8303/15436 [35:01<14:42:46,  7.43s/it]                                                       {'loss': 0.8893, 'grad_norm': 0.7632094601545094, 'learning_rate': 9.259972998438011e-06, 'epoch': 0.54}
 54%|█████▍    | 8303/15436 [35:01<14:42:46,  7.43s/it] 54%|█████▍    | 8304/15436 [35:09<15:01:25,  7.58s/it]                                                       {'loss': 0.8081, 'grad_norm': 0.7578466908608088, 'learning_rate': 9.257880456278682e-06, 'epoch': 0.54}
 54%|█████▍    | 8304/15436 [35:09<15:01:25,  7.58s/it] 54%|█████▍    | 8305/15436 [35:15<13:57:04,  7.04s/it]                                                       {'loss': 0.919, 'grad_norm': 0.702418521837734, 'learning_rate': 9.255787946794228e-06, 'epoch': 0.54}
 54%|█████▍    | 8305/15436 [35:15<13:57:04,  7.04s/it] 54%|█████▍    | 8306/15436 [35:24<15:22:40,  7.76s/it]                                                       {'loss': 0.8832, 'grad_norm': 0.7553033552408105, 'learning_rate': 9.253695470076781e-06, 'epoch': 0.54}
 54%|█████▍    | 8306/15436 [35:24<15:22:40,  7.76s/it] 54%|█████▍    | 8307/15436 [35:32<15:26:41,  7.80s/it]                                                       {'loss': 0.8862, 'grad_norm': 0.6646856344424759, 'learning_rate': 9.251603026218475e-06, 'epoch': 0.54}
 54%|█████▍    | 8307/15436 [35:32<15:26:41,  7.80s/it] 54%|█████▍    | 8308/15436 [35:36<13:20:39,  6.74s/it]                                                       {'loss': 0.9901, 'grad_norm': 0.8386582517902202, 'learning_rate': 9.249510615311434e-06, 'epoch': 0.54}
 54%|█████▍    | 8308/15436 [35:36<13:20:39,  6.74s/it] 54%|█████▍    | 8309/15436 [35:41<12:19:36,  6.23s/it]                                                       {'loss': 0.8663, 'grad_norm': 0.7609813252986054, 'learning_rate': 9.247418237447792e-06, 'epoch': 0.54}
 54%|█████▍    | 8309/15436 [35:41<12:19:36,  6.23s/it] 54%|█████▍    | 8310/15436 [35:47<12:10:33,  6.15s/it]                                                       {'loss': 0.875, 'grad_norm': 0.707528987855994, 'learning_rate': 9.24532589271967e-06, 'epoch': 0.54}
 54%|█████▍    | 8310/15436 [35:47<12:10:33,  6.15s/it] 54%|█████▍    | 8311/15436 [35:53<11:41:44,  5.91s/it]                                                       {'loss': 0.9635, 'grad_norm': 0.7409714601957672, 'learning_rate': 9.24323358121919e-06, 'epoch': 0.54}
 54%|█████▍    | 8311/15436 [35:53<11:41:44,  5.91s/it] 54%|█████▍    | 8312/15436 [36:03<14:31:51,  7.34s/it]                                                       {'loss': 0.9054, 'grad_norm': 0.7078109257610113, 'learning_rate': 9.24114130303848e-06, 'epoch': 0.54}
 54%|█████▍    | 8312/15436 [36:03<14:31:51,  7.34s/it] 54%|█████▍    | 8313/15436 [36:11<14:29:52,  7.33s/it]                                                       {'loss': 0.8661, 'grad_norm': 0.6875226470101427, 'learning_rate': 9.239049058269658e-06, 'epoch': 0.54}
 54%|█████▍    | 8313/15436 [36:11<14:29:52,  7.33s/it] 54%|█████▍    | 8314/15436 [36:16<13:30:09,  6.83s/it]                                                       {'loss': 0.8431, 'grad_norm': 0.7175747851115625, 'learning_rate': 9.236956847004842e-06, 'epoch': 0.54}
 54%|█████▍    | 8314/15436 [36:16<13:30:09,  6.83s/it] 54%|█████▍    | 8315/15436 [36:25<14:47:11,  7.48s/it]                                                       {'loss': 0.8881, 'grad_norm': 0.7165457405472503, 'learning_rate': 9.234864669336155e-06, 'epoch': 0.54}
 54%|█████▍    | 8315/15436 [36:25<14:47:11,  7.48s/it] 54%|█████▍    | 8316/15436 [36:33<14:53:25,  7.53s/it]                                                       {'loss': 0.8509, 'grad_norm': 0.7429694443525289, 'learning_rate': 9.232772525355712e-06, 'epoch': 0.54}
 54%|█████▍    | 8316/15436 [36:33<14:53:25,  7.53s/it] 54%|█████▍    | 8317/15436 [36:39<13:44:33,  6.95s/it]                                                       {'loss': 0.9772, 'grad_norm': 0.7325752875978543, 'learning_rate': 9.230680415155621e-06, 'epoch': 0.54}
 54%|█████▍    | 8317/15436 [36:39<13:44:33,  6.95s/it] 54%|█████▍    | 8318/15436 [36:45<13:11:29,  6.67s/it]                                                       {'loss': 0.9137, 'grad_norm': 0.7600005487919954, 'learning_rate': 9.228588338828011e-06, 'epoch': 0.54}
 54%|█████▍    | 8318/15436 [36:45<13:11:29,  6.67s/it] 54%|█████▍    | 8319/15436 [36:51<13:09:04,  6.65s/it]                                                       {'loss': 0.7912, 'grad_norm': 0.7390390723036298, 'learning_rate': 9.226496296464982e-06, 'epoch': 0.54}
 54%|█████▍    | 8319/15436 [36:51<13:09:04,  6.65s/it] 54%|█████▍    | 8320/15436 [36:58<13:22:48,  6.77s/it]                                                       {'loss': 0.8781, 'grad_norm': 0.6642739484657247, 'learning_rate': 9.224404288158645e-06, 'epoch': 0.54}
 54%|█████▍    | 8320/15436 [36:58<13:22:48,  6.77s/it] 54%|█████▍    | 8321/15436 [37:07<14:18:54,  7.24s/it]                                                       {'loss': 0.9288, 'grad_norm': 0.8071066377202868, 'learning_rate': 9.222312314001117e-06, 'epoch': 0.54}
 54%|█████▍    | 8321/15436 [37:07<14:18:54,  7.24s/it] 54%|█████▍    | 8322/15436 [37:13<13:52:22,  7.02s/it]                                                       {'loss': 0.91, 'grad_norm': 0.6811481118093641, 'learning_rate': 9.220220374084502e-06, 'epoch': 0.54}
 54%|█████▍    | 8322/15436 [37:13<13:52:22,  7.02s/it] 54%|█████▍    | 8323/15436 [37:22<14:45:35,  7.47s/it]                                                       {'loss': 0.8751, 'grad_norm': 0.7867048210616091, 'learning_rate': 9.218128468500902e-06, 'epoch': 0.54}
 54%|█████▍    | 8323/15436 [37:22<14:45:35,  7.47s/it] 54%|█████▍    | 8324/15436 [37:27<13:37:57,  6.90s/it]                                                       {'loss': 0.9026, 'grad_norm': 0.7625507525098256, 'learning_rate': 9.216036597342427e-06, 'epoch': 0.54}
 54%|█████▍    | 8324/15436 [37:27<13:37:57,  6.90s/it] 54%|█████▍    | 8325/15436 [37:39<16:21:08,  8.28s/it]                                                       {'loss': 0.8835, 'grad_norm': 0.8047141884088186, 'learning_rate': 9.213944760701176e-06, 'epoch': 0.54}
 54%|█████▍    | 8325/15436 [37:39<16:21:08,  8.28s/it] 54%|█████▍    | 8326/15436 [37:46<16:00:55,  8.11s/it]                                                       {'loss': 0.9343, 'grad_norm': 0.7393268592231697, 'learning_rate': 9.211852958669256e-06, 'epoch': 0.54}
 54%|█████▍    | 8326/15436 [37:46<16:00:55,  8.11s/it] 54%|█████▍    | 8327/15436 [37:54<15:56:22,  8.07s/it]                                                       {'loss': 0.9232, 'grad_norm': 0.7389363061991019, 'learning_rate': 9.209761191338767e-06, 'epoch': 0.54}
 54%|█████▍    | 8327/15436 [37:54<15:56:22,  8.07s/it] 54%|█████▍    | 8328/15436 [38:01<15:10:44,  7.69s/it]                                                       {'loss': 0.8974, 'grad_norm': 0.7808002722144515, 'learning_rate': 9.207669458801803e-06, 'epoch': 0.54}
 54%|█████▍    | 8328/15436 [38:01<15:10:44,  7.69s/it] 54%|█████▍    | 8329/15436 [38:09<14:54:43,  7.55s/it]                                                       {'loss': 0.8898, 'grad_norm': 0.7387985812914605, 'learning_rate': 9.205577761150465e-06, 'epoch': 0.54}
 54%|█████▍    | 8329/15436 [38:09<14:54:43,  7.55s/it] 54%|█████▍    | 8330/15436 [38:19<16:56:10,  8.58s/it]                                                       {'loss': 0.7874, 'grad_norm': 0.758355970018363, 'learning_rate': 9.20348609847685e-06, 'epoch': 0.54}
 54%|█████▍    | 8330/15436 [38:19<16:56:10,  8.58s/it] 54%|█████▍    | 8331/15436 [38:28<16:38:24,  8.43s/it]                                                       {'loss': 1.0197, 'grad_norm': 0.7105296626916469, 'learning_rate': 9.201394470873042e-06, 'epoch': 0.54}
 54%|█████▍    | 8331/15436 [38:28<16:38:24,  8.43s/it] 54%|█████▍    | 8332/15436 [38:33<14:47:08,  7.49s/it]                                                       {'loss': 0.9665, 'grad_norm': 0.8978904501494228, 'learning_rate': 9.199302878431146e-06, 'epoch': 0.54}
 54%|█████▍    | 8332/15436 [38:33<14:47:08,  7.49s/it] 54%|█████▍    | 8333/15436 [38:41<15:20:15,  7.77s/it]                                                       {'loss': 0.8342, 'grad_norm': 0.7026620779728248, 'learning_rate': 9.197211321243248e-06, 'epoch': 0.54}
 54%|█████▍    | 8333/15436 [38:41<15:20:15,  7.77s/it] 54%|█████▍    | 8334/15436 [38:46<13:32:26,  6.86s/it]                                                       {'loss': 1.0046, 'grad_norm': 0.7460739934476055, 'learning_rate': 9.195119799401433e-06, 'epoch': 0.54}
 54%|█████▍    | 8334/15436 [38:46<13:32:26,  6.86s/it] 54%|█████▍    | 8335/15436 [38:56<15:29:26,  7.85s/it]                                                       {'loss': 1.0125, 'grad_norm': 0.6743329965852352, 'learning_rate': 9.193028312997798e-06, 'epoch': 0.54}
 54%|█████▍    | 8335/15436 [38:56<15:29:26,  7.85s/it] 54%|█████▍    | 8336/15436 [39:02<14:05:08,  7.14s/it]                                                       {'loss': 0.9531, 'grad_norm': 0.8480115663884835, 'learning_rate': 9.190936862124424e-06, 'epoch': 0.54}
 54%|█████▍    | 8336/15436 [39:02<14:05:08,  7.14s/it] 54%|█████▍    | 8337/15436 [39:08<13:43:23,  6.96s/it]                                                       {'loss': 0.9011, 'grad_norm': 0.7140235018365012, 'learning_rate': 9.188845446873394e-06, 'epoch': 0.54}
 54%|█████▍    | 8337/15436 [39:08<13:43:23,  6.96s/it] 54%|█████▍    | 8338/15436 [39:13<12:19:12,  6.25s/it]                                                       {'loss': 0.9668, 'grad_norm': 0.7534459974799931, 'learning_rate': 9.186754067336796e-06, 'epoch': 0.54}
 54%|█████▍    | 8338/15436 [39:13<12:19:12,  6.25s/it] 54%|█████▍    | 8339/15436 [39:19<12:08:32,  6.16s/it]                                                       {'loss': 0.9367, 'grad_norm': 0.7607551353205012, 'learning_rate': 9.184662723606708e-06, 'epoch': 0.54}
 54%|█████▍    | 8339/15436 [39:19<12:08:32,  6.16s/it] 54%|█████▍    | 8340/15436 [39:25<11:55:01,  6.05s/it]                                                       {'loss': 0.9972, 'grad_norm': 0.7836887862083491, 'learning_rate': 9.182571415775209e-06, 'epoch': 0.54}
 54%|█████▍    | 8340/15436 [39:25<11:55:01,  6.05s/it] 54%|█████▍    | 8341/15436 [39:32<12:38:59,  6.42s/it]                                                       {'loss': 0.866, 'grad_norm': 0.7008648920914342, 'learning_rate': 9.18048014393438e-06, 'epoch': 0.54}
 54%|█████▍    | 8341/15436 [39:32<12:38:59,  6.42s/it] 54%|█████▍    | 8342/15436 [39:40<13:47:47,  7.00s/it]                                                       {'loss': 0.89, 'grad_norm': 0.7167361529739389, 'learning_rate': 9.178388908176299e-06, 'epoch': 0.54}
 54%|█████▍    | 8342/15436 [39:40<13:47:47,  7.00s/it] 54%|█████▍    | 8343/15436 [39:46<13:06:35,  6.65s/it]                                                       {'loss': 0.9312, 'grad_norm': 0.8271262970205209, 'learning_rate': 9.176297708593041e-06, 'epoch': 0.54}
 54%|█████▍    | 8343/15436 [39:46<13:06:35,  6.65s/it] 54%|█████▍    | 8344/15436 [39:52<12:24:45,  6.30s/it]                                                       {'loss': 0.8796, 'grad_norm': 0.7144474175137322, 'learning_rate': 9.174206545276678e-06, 'epoch': 0.54}
 54%|█████▍    | 8344/15436 [39:52<12:24:45,  6.30s/it] 54%|█████▍    | 8345/15436 [39:57<11:56:35,  6.06s/it]                                                       {'loss': 0.9181, 'grad_norm': 0.744650523502866, 'learning_rate': 9.17211541831928e-06, 'epoch': 0.54}
 54%|█████▍    | 8345/15436 [39:57<11:56:35,  6.06s/it] 54%|█████▍    | 8346/15436 [40:02<11:02:10,  5.60s/it]                                                       {'loss': 0.9172, 'grad_norm': 0.782104961588639, 'learning_rate': 9.170024327812924e-06, 'epoch': 0.54}
 54%|█████▍    | 8346/15436 [40:02<11:02:10,  5.60s/it] 54%|█████▍    | 8347/15436 [40:08<11:32:47,  5.86s/it]                                                       {'loss': 0.9301, 'grad_norm': 0.6849826424846864, 'learning_rate': 9.167933273849675e-06, 'epoch': 0.54}
 54%|█████▍    | 8347/15436 [40:08<11:32:47,  5.86s/it] 54%|█████▍    | 8348/15436 [40:18<13:48:18,  7.01s/it]                                                       {'loss': 0.955, 'grad_norm': 0.7087282815449778, 'learning_rate': 9.165842256521595e-06, 'epoch': 0.54}
 54%|█████▍    | 8348/15436 [40:18<13:48:18,  7.01s/it] 54%|█████▍    | 8349/15436 [40:24<13:10:16,  6.69s/it]                                                       {'loss': 0.8965, 'grad_norm': 0.7887222765569368, 'learning_rate': 9.163751275920762e-06, 'epoch': 0.54}
 54%|█████▍    | 8349/15436 [40:24<13:10:16,  6.69s/it] 54%|█████▍    | 8350/15436 [40:32<14:15:40,  7.25s/it]                                                       {'loss': 0.8587, 'grad_norm': 0.6993313636206437, 'learning_rate': 9.16166033213923e-06, 'epoch': 0.54}
 54%|█████▍    | 8350/15436 [40:32<14:15:40,  7.25s/it] 54%|█████▍    | 8351/15436 [40:38<13:35:50,  6.91s/it]                                                       {'loss': 0.8012, 'grad_norm': 0.6729279322116082, 'learning_rate': 9.15956942526906e-06, 'epoch': 0.54}
 54%|█████▍    | 8351/15436 [40:38<13:35:50,  6.91s/it] 54%|█████▍    | 8352/15436 [40:44<12:46:56,  6.50s/it]                                                       {'loss': 0.9202, 'grad_norm': 0.7194690615695536, 'learning_rate': 9.157478555402324e-06, 'epoch': 0.54}
 54%|█████▍    | 8352/15436 [40:44<12:46:56,  6.50s/it] 54%|█████▍    | 8353/15436 [40:52<13:54:15,  7.07s/it]                                                       {'loss': 0.9047, 'grad_norm': 0.6820637283352307, 'learning_rate': 9.155387722631071e-06, 'epoch': 0.54}
 54%|█████▍    | 8353/15436 [40:52<13:54:15,  7.07s/it] 54%|█████▍    | 8354/15436 [40:58<13:20:48,  6.78s/it]                                                       {'loss': 0.8497, 'grad_norm': 0.7677521779661712, 'learning_rate': 9.153296927047362e-06, 'epoch': 0.54}
 54%|█████▍    | 8354/15436 [40:58<13:20:48,  6.78s/it] 54%|█████▍    | 8355/15436 [41:03<12:21:35,  6.28s/it]                                                       {'loss': 0.8495, 'grad_norm': 0.7223307403435194, 'learning_rate': 9.151206168743254e-06, 'epoch': 0.54}
 54%|█████▍    | 8355/15436 [41:03<12:21:35,  6.28s/it] 54%|█████▍    | 8356/15436 [41:14<15:05:01,  7.67s/it]                                                       {'loss': 0.9236, 'grad_norm': 0.8108800929335483, 'learning_rate': 9.149115447810798e-06, 'epoch': 0.54}
 54%|█████▍    | 8356/15436 [41:14<15:05:01,  7.67s/it] 54%|█████▍    | 8357/15436 [41:21<14:21:25,  7.30s/it]                                                       {'loss': 0.91, 'grad_norm': 0.7317358489287009, 'learning_rate': 9.147024764342052e-06, 'epoch': 0.54}
 54%|█████▍    | 8357/15436 [41:21<14:21:25,  7.30s/it] 54%|█████▍    | 8358/15436 [41:29<15:02:31,  7.65s/it]                                                       {'loss': 0.9492, 'grad_norm': 0.7131701398017631, 'learning_rate': 9.144934118429063e-06, 'epoch': 0.54}
 54%|█████▍    | 8358/15436 [41:29<15:02:31,  7.65s/it] 54%|█████▍    | 8359/15436 [41:35<13:50:32,  7.04s/it]                                                       {'loss': 0.9286, 'grad_norm': 0.7446122157396583, 'learning_rate': 9.142843510163878e-06, 'epoch': 0.54}
 54%|█████▍    | 8359/15436 [41:35<13:50:32,  7.04s/it] 54%|█████▍    | 8360/15436 [41:41<13:02:23,  6.63s/it]                                                       {'loss': 0.9011, 'grad_norm': 0.669312365933452, 'learning_rate': 9.14075293963855e-06, 'epoch': 0.54}
 54%|█████▍    | 8360/15436 [41:41<13:02:23,  6.63s/it] 54%|█████▍    | 8361/15436 [41:48<13:12:29,  6.72s/it]                                                       {'loss': 0.9498, 'grad_norm': 0.7130820674930133, 'learning_rate': 9.138662406945127e-06, 'epoch': 0.54}
 54%|█████▍    | 8361/15436 [41:48<13:12:29,  6.72s/it] 54%|█████▍    | 8362/15436 [41:53<12:13:01,  6.22s/it]                                                       {'loss': 0.9241, 'grad_norm': 0.7745477039799876, 'learning_rate': 9.136571912175641e-06, 'epoch': 0.54}
 54%|█████▍    | 8362/15436 [41:53<12:13:01,  6.22s/it] 54%|█████▍    | 8363/15436 [42:02<13:52:42,  7.06s/it]                                                       {'loss': 0.8366, 'grad_norm': 0.7410653666270017, 'learning_rate': 9.13448145542215e-06, 'epoch': 0.54}
 54%|█████▍    | 8363/15436 [42:02<13:52:42,  7.06s/it] 54%|█████▍    | 8364/15436 [42:11<15:01:28,  7.65s/it]                                                       {'loss': 0.9044, 'grad_norm': 0.6552368334590176, 'learning_rate': 9.132391036776686e-06, 'epoch': 0.54}
 54%|█████▍    | 8364/15436 [42:11<15:01:28,  7.65s/it] 54%|█████▍    | 8365/15436 [42:17<14:23:54,  7.33s/it]                                                       {'loss': 0.9494, 'grad_norm': 0.7387159259252675, 'learning_rate': 9.130300656331287e-06, 'epoch': 0.54}
 54%|█████▍    | 8365/15436 [42:17<14:23:54,  7.33s/it] 54%|█████▍    | 8366/15436 [42:22<12:57:05,  6.59s/it]                                                       {'loss': 0.898, 'grad_norm': 0.7136832078480557, 'learning_rate': 9.128210314177996e-06, 'epoch': 0.54}
 54%|█████▍    | 8366/15436 [42:22<12:57:05,  6.59s/it] 54%|█████▍    | 8367/15436 [42:32<14:59:50,  7.64s/it]                                                       {'loss': 0.8816, 'grad_norm': 0.7270254012658595, 'learning_rate': 9.126120010408846e-06, 'epoch': 0.54}
 54%|█████▍    | 8367/15436 [42:32<14:59:50,  7.64s/it] 54%|█████▍    | 8368/15436 [42:39<14:39:54,  7.47s/it]                                                       {'loss': 0.9001, 'grad_norm': 0.7866842988484024, 'learning_rate': 9.124029745115872e-06, 'epoch': 0.54}
 54%|█████▍    | 8368/15436 [42:39<14:39:54,  7.47s/it] 54%|█████▍    | 8369/15436 [42:47<15:01:35,  7.65s/it]                                                       {'loss': 0.983, 'grad_norm': 0.7725417442546778, 'learning_rate': 9.121939518391106e-06, 'epoch': 0.54}
 54%|█████▍    | 8369/15436 [42:47<15:01:35,  7.65s/it] 54%|█████▍    | 8370/15436 [42:53<14:01:21,  7.14s/it]                                                       {'loss': 0.9583, 'grad_norm': 0.7364442482912678, 'learning_rate': 9.11984933032658e-06, 'epoch': 0.54}
 54%|█████▍    | 8370/15436 [42:53<14:01:21,  7.14s/it] 54%|█████▍    | 8371/15436 [43:02<14:43:29,  7.50s/it]                                                       {'loss': 0.9392, 'grad_norm': 0.7573320892457829, 'learning_rate': 9.117759181014321e-06, 'epoch': 0.54}
 54%|█████▍    | 8371/15436 [43:02<14:43:29,  7.50s/it] 54%|█████▍    | 8372/15436 [43:07<13:30:59,  6.89s/it]                                                       {'loss': 0.8587, 'grad_norm': 0.7982930804082725, 'learning_rate': 9.11566907054636e-06, 'epoch': 0.54}
 54%|█████▍    | 8372/15436 [43:07<13:30:59,  6.89s/it] 54%|█████▍    | 8373/15436 [43:11<11:58:29,  6.10s/it]                                                       {'loss': 0.9598, 'grad_norm': 0.7654903203033772, 'learning_rate': 9.113578999014719e-06, 'epoch': 0.54}
 54%|█████▍    | 8373/15436 [43:11<11:58:29,  6.10s/it] 54%|█████▍    | 8374/15436 [43:17<11:29:25,  5.86s/it]                                                       {'loss': 0.8256, 'grad_norm': 0.7056214712609519, 'learning_rate': 9.111488966511425e-06, 'epoch': 0.54}
 54%|█████▍    | 8374/15436 [43:17<11:29:25,  5.86s/it] 54%|█████▍    | 8375/15436 [43:24<12:39:09,  6.45s/it]                                                       {'loss': 0.8401, 'grad_norm': 0.7617404053898074, 'learning_rate': 9.109398973128502e-06, 'epoch': 0.54}
 54%|█████▍    | 8375/15436 [43:24<12:39:09,  6.45s/it] 54%|█████▍    | 8376/15436 [43:31<12:41:27,  6.47s/it]                                                       {'loss': 0.8317, 'grad_norm': 0.7129963097706535, 'learning_rate': 9.10730901895796e-06, 'epoch': 0.54}
 54%|█████▍    | 8376/15436 [43:31<12:41:27,  6.47s/it] 54%|█████▍    | 8377/15436 [43:38<13:16:34,  6.77s/it]                                                       {'loss': 0.9475, 'grad_norm': 0.6796165433438832, 'learning_rate': 9.10521910409183e-06, 'epoch': 0.54}
 54%|█████▍    | 8377/15436 [43:38<13:16:34,  6.77s/it] 54%|█████▍    | 8378/15436 [43:45<13:22:15,  6.82s/it]                                                       {'loss': 0.8394, 'grad_norm': 0.7483908668125772, 'learning_rate': 9.103129228622128e-06, 'epoch': 0.54}
 54%|█████▍    | 8378/15436 [43:45<13:22:15,  6.82s/it] 54%|█████▍    | 8379/15436 [43:51<12:41:57,  6.48s/it]                                                       {'loss': 1.0042, 'grad_norm': 0.7300783198524132, 'learning_rate': 9.101039392640859e-06, 'epoch': 0.54}
 54%|█████▍    | 8379/15436 [43:51<12:41:57,  6.48s/it] 54%|█████▍    | 8380/15436 [43:58<13:03:15,  6.66s/it]                                                       {'loss': 1.0717, 'grad_norm': 0.7546923602054492, 'learning_rate': 9.09894959624005e-06, 'epoch': 0.54}
 54%|█████▍    | 8380/15436 [43:58<13:03:15,  6.66s/it] 54%|█████▍    | 8381/15436 [44:04<12:33:10,  6.41s/it]                                                       {'loss': 0.857, 'grad_norm': 0.8552350732826198, 'learning_rate': 9.096859839511705e-06, 'epoch': 0.54}
 54%|█████▍    | 8381/15436 [44:04<12:33:10,  6.41s/it] 54%|█████▍    | 8382/15436 [44:14<14:51:34,  7.58s/it]                                                       {'loss': 0.9051, 'grad_norm': 0.7430349986765717, 'learning_rate': 9.094770122547833e-06, 'epoch': 0.54}
 54%|█████▍    | 8382/15436 [44:14<14:51:34,  7.58s/it] 54%|█████▍    | 8383/15436 [44:22<14:45:03,  7.53s/it]                                                       {'loss': 0.9932, 'grad_norm': 0.784448852076496, 'learning_rate': 9.092680445440448e-06, 'epoch': 0.54}
 54%|█████▍    | 8383/15436 [44:22<14:45:03,  7.53s/it] 54%|█████▍    | 8384/15436 [44:31<15:31:46,  7.93s/it]                                                       {'loss': 0.9189, 'grad_norm': 0.8189290316548983, 'learning_rate': 9.090590808281554e-06, 'epoch': 0.54}
 54%|█████▍    | 8384/15436 [44:31<15:31:46,  7.93s/it] 54%|█████▍    | 8385/15436 [44:36<14:11:13,  7.24s/it]                                                       {'loss': 0.9142, 'grad_norm': 0.7368899408899704, 'learning_rate': 9.088501211163153e-06, 'epoch': 0.54}
 54%|█████▍    | 8385/15436 [44:36<14:11:13,  7.24s/it] 54%|█████▍    | 8386/15436 [44:43<13:44:16,  7.02s/it]                                                       {'loss': 0.9224, 'grad_norm': 0.738772847366463, 'learning_rate': 9.086411654177254e-06, 'epoch': 0.54}
 54%|█████▍    | 8386/15436 [44:43<13:44:16,  7.02s/it] 54%|█████▍    | 8387/15436 [44:52<15:13:00,  7.77s/it]                                                       {'loss': 0.9955, 'grad_norm': 0.7290397823327688, 'learning_rate': 9.084322137415855e-06, 'epoch': 0.54}
 54%|█████▍    | 8387/15436 [44:52<15:13:00,  7.77s/it] 54%|█████▍    | 8388/15436 [45:03<16:51:38,  8.61s/it]                                                       {'loss': 0.9538, 'grad_norm': 0.7538980929105927, 'learning_rate': 9.082232660970953e-06, 'epoch': 0.54}
 54%|█████▍    | 8388/15436 [45:03<16:51:38,  8.61s/it] 54%|█████▍    | 8389/15436 [45:09<15:34:01,  7.95s/it]                                                       {'loss': 0.8679, 'grad_norm': 0.7072679901313718, 'learning_rate': 9.080143224934552e-06, 'epoch': 0.54}
 54%|█████▍    | 8389/15436 [45:09<15:34:01,  7.95s/it] 54%|█████▍    | 8390/15436 [45:15<14:07:53,  7.22s/it]                                                       {'loss': 0.9672, 'grad_norm': 0.8289473793812657, 'learning_rate': 9.078053829398641e-06, 'epoch': 0.54}
 54%|█████▍    | 8390/15436 [45:15<14:07:53,  7.22s/it] 54%|█████▍    | 8391/15436 [45:21<13:26:32,  6.87s/it]                                                       {'loss': 0.8661, 'grad_norm': 0.7109295542484511, 'learning_rate': 9.075964474455221e-06, 'epoch': 0.54}
 54%|█████▍    | 8391/15436 [45:21<13:26:32,  6.87s/it] 54%|█████▍    | 8392/15436 [45:27<12:55:44,  6.61s/it]                                                       {'loss': 0.8295, 'grad_norm': 0.7048985107013304, 'learning_rate': 9.073875160196283e-06, 'epoch': 0.54}
 54%|█████▍    | 8392/15436 [45:27<12:55:44,  6.61s/it] 54%|█████▍    | 8393/15436 [45:33<12:50:12,  6.56s/it]                                                       {'loss': 0.8733, 'grad_norm': 0.808386369976729, 'learning_rate': 9.07178588671381e-06, 'epoch': 0.54}
 54%|█████▍    | 8393/15436 [45:33<12:50:12,  6.56s/it] 54%|█████▍    | 8394/15436 [45:39<12:08:22,  6.21s/it]                                                       {'loss': 0.8683, 'grad_norm': 1.154033269615227, 'learning_rate': 9.069696654099806e-06, 'epoch': 0.54}
 54%|█████▍    | 8394/15436 [45:39<12:08:22,  6.21s/it] 54%|█████▍    | 8395/15436 [45:44<11:47:37,  6.03s/it]                                                       {'loss': 0.9294, 'grad_norm': 0.731995831976011, 'learning_rate': 9.067607462446245e-06, 'epoch': 0.54}
 54%|█████▍    | 8395/15436 [45:44<11:47:37,  6.03s/it] 54%|█████▍    | 8396/15436 [45:51<12:11:47,  6.24s/it]                                                       {'loss': 0.9733, 'grad_norm': 0.627183145687783, 'learning_rate': 9.065518311845114e-06, 'epoch': 0.54}
 54%|█████▍    | 8396/15436 [45:51<12:11:47,  6.24s/it] 54%|█████▍    | 8397/15436 [45:57<12:00:22,  6.14s/it]                                                       {'loss': 0.9471, 'grad_norm': 0.7927383604884464, 'learning_rate': 9.063429202388402e-06, 'epoch': 0.54}
 54%|█████▍    | 8397/15436 [45:57<12:00:22,  6.14s/it] 54%|█████▍    | 8398/15436 [46:04<12:42:02,  6.50s/it]                                                       {'loss': 0.8781, 'grad_norm': 0.8364609262000913, 'learning_rate': 9.061340134168088e-06, 'epoch': 0.54}
 54%|█████▍    | 8398/15436 [46:04<12:42:02,  6.50s/it] 54%|█████▍    | 8399/15436 [46:09<11:31:51,  5.90s/it]                                                       {'loss': 0.9784, 'grad_norm': 0.7989319305946995, 'learning_rate': 9.059251107276148e-06, 'epoch': 0.54}
 54%|█████▍    | 8399/15436 [46:09<11:31:51,  5.90s/it] 54%|█████▍    | 8400/15436 [46:18<13:32:14,  6.93s/it]                                                       {'loss': 0.8997, 'grad_norm': 0.7745847026392247, 'learning_rate': 9.057162121804567e-06, 'epoch': 0.54}
 54%|█████▍    | 8400/15436 [46:18<13:32:14,  6.93s/it] 54%|█████▍    | 8401/15436 [46:23<12:35:24,  6.44s/it]                                                       {'loss': 0.9638, 'grad_norm': 0.7290656803020596, 'learning_rate': 9.055073177845316e-06, 'epoch': 0.54}
 54%|█████▍    | 8401/15436 [46:23<12:35:24,  6.44s/it] 54%|█████▍    | 8402/15436 [46:29<11:57:16,  6.12s/it]                                                       {'loss': 0.9718, 'grad_norm': 0.7807949215571428, 'learning_rate': 9.052984275490372e-06, 'epoch': 0.54}
 54%|█████▍    | 8402/15436 [46:29<11:57:16,  6.12s/it] 54%|█████▍    | 8403/15436 [46:34<11:28:46,  5.88s/it]                                                       {'loss': 0.8754, 'grad_norm': 0.6977944282852209, 'learning_rate': 9.050895414831707e-06, 'epoch': 0.54}
 54%|█████▍    | 8403/15436 [46:34<11:28:46,  5.88s/it] 54%|█████▍    | 8404/15436 [46:39<11:02:45,  5.65s/it]                                                       {'loss': 0.8986, 'grad_norm': 0.7551947575125407, 'learning_rate': 9.048806595961294e-06, 'epoch': 0.54}
 54%|█████▍    | 8404/15436 [46:39<11:02:45,  5.65s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2198 > 2048). Running this sequence through the model will result in indexing errors
 54%|█████▍    | 8405/15436 [46:45<10:52:53,  5.57s/it]                                                       {'loss': 0.8647, 'grad_norm': 0.7175355547688653, 'learning_rate': 9.046717818971092e-06, 'epoch': 0.54}
 54%|█████▍    | 8405/15436 [46:45<10:52:53,  5.57s/it] 54%|█████▍    | 8406/15436 [46:51<11:25:02,  5.85s/it]                                                       {'loss': 0.9246, 'grad_norm': 0.700602802384528, 'learning_rate': 9.044629083953082e-06, 'epoch': 0.54}
 54%|█████▍    | 8406/15436 [46:51<11:25:02,  5.85s/it] 54%|█████▍    | 8407/15436 [46:59<12:36:47,  6.46s/it]                                                       {'loss': 0.8904, 'grad_norm': 0.7482825785380562, 'learning_rate': 9.042540390999217e-06, 'epoch': 0.54}
 54%|█████▍    | 8407/15436 [46:59<12:36:47,  6.46s/it] 54%|█████▍    | 8408/15436 [47:04<11:35:55,  5.94s/it]                                                       {'loss': 0.9037, 'grad_norm': 0.8065514196758254, 'learning_rate': 9.040451740201473e-06, 'epoch': 0.54}
 54%|█████▍    | 8408/15436 [47:04<11:35:55,  5.94s/it] 54%|█████▍    | 8409/15436 [47:13<13:19:25,  6.83s/it]                                                       {'loss': 0.8508, 'grad_norm': 0.7344393635094948, 'learning_rate': 9.0383631316518e-06, 'epoch': 0.54}
 54%|█████▍    | 8409/15436 [47:13<13:19:25,  6.83s/it] 54%|█████▍    | 8410/15436 [47:18<12:44:42,  6.53s/it]                                                       {'loss': 0.957, 'grad_norm': 0.742805530591898, 'learning_rate': 9.036274565442159e-06, 'epoch': 0.54}
 54%|█████▍    | 8410/15436 [47:18<12:44:42,  6.53s/it] 54%|█████▍    | 8411/15436 [47:26<13:37:39,  6.98s/it]                                                       {'loss': 0.9907, 'grad_norm': 0.7419506567029619, 'learning_rate': 9.034186041664518e-06, 'epoch': 0.54}
 54%|█████▍    | 8411/15436 [47:26<13:37:39,  6.98s/it] 54%|█████▍    | 8412/15436 [47:32<12:49:12,  6.57s/it]                                                       {'loss': 0.8518, 'grad_norm': 0.6661809408213992, 'learning_rate': 9.032097560410821e-06, 'epoch': 0.54}
 54%|█████▍    | 8412/15436 [47:32<12:49:12,  6.57s/it] 55%|█████▍    | 8413/15436 [47:37<12:01:13,  6.16s/it]                                                       {'loss': 0.8492, 'grad_norm': 0.671730044504925, 'learning_rate': 9.030009121773027e-06, 'epoch': 0.55}
 55%|█████▍    | 8413/15436 [47:37<12:01:13,  6.16s/it] 55%|█████▍    | 8414/15436 [47:43<11:48:10,  6.05s/it]                                                       {'loss': 1.0062, 'grad_norm': 0.7042726201584095, 'learning_rate': 9.027920725843088e-06, 'epoch': 0.55}
 55%|█████▍    | 8414/15436 [47:43<11:48:10,  6.05s/it] 55%|█████▍    | 8415/15436 [47:50<12:36:40,  6.47s/it]                                                       {'loss': 0.8876, 'grad_norm': 0.6842400022799336, 'learning_rate': 9.025832372712955e-06, 'epoch': 0.55}
 55%|█████▍    | 8415/15436 [47:50<12:36:40,  6.47s/it] 55%|█████▍    | 8416/15436 [47:59<13:32:36,  6.95s/it]                                                       {'loss': 0.8679, 'grad_norm': 0.7090596704031663, 'learning_rate': 9.023744062474574e-06, 'epoch': 0.55}
 55%|█████▍    | 8416/15436 [47:59<13:32:36,  6.95s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2760 > 2048). Running this sequence through the model will result in indexing errors
 55%|█████▍    | 8417/15436 [48:04<12:54:20,  6.62s/it]                                                       {'loss': 0.8512, 'grad_norm': 0.8388257538670735, 'learning_rate': 9.021655795219893e-06, 'epoch': 0.55}
 55%|█████▍    | 8417/15436 [48:04<12:54:20,  6.62s/it] 55%|█████▍    | 8418/15436 [48:11<13:09:13,  6.75s/it]                                                       {'loss': 0.897, 'grad_norm': 0.7535754388139185, 'learning_rate': 9.019567571040859e-06, 'epoch': 0.55}
 55%|█████▍    | 8418/15436 [48:11<13:09:13,  6.75s/it] 55%|█████▍    | 8419/15436 [48:17<12:32:36,  6.44s/it]                                                       {'loss': 0.885, 'grad_norm': 0.8161952919271875, 'learning_rate': 9.017479390029408e-06, 'epoch': 0.55}
 55%|█████▍    | 8419/15436 [48:17<12:32:36,  6.44s/it] 55%|█████▍    | 8420/15436 [48:23<12:10:38,  6.25s/it]                                                       {'loss': 0.9781, 'grad_norm': 0.668730005396827, 'learning_rate': 9.015391252277488e-06, 'epoch': 0.55}
 55%|█████▍    | 8420/15436 [48:23<12:10:38,  6.25s/it] 55%|█████▍    | 8421/15436 [48:30<12:53:43,  6.62s/it]                                                       {'loss': 0.8617, 'grad_norm': 0.7592697746211742, 'learning_rate': 9.013303157877037e-06, 'epoch': 0.55}
 55%|█████▍    | 8421/15436 [48:30<12:53:43,  6.62s/it] 55%|█████▍    | 8422/15436 [48:40<14:53:53,  7.65s/it]                                                       {'loss': 0.8739, 'grad_norm': 0.7132541008676245, 'learning_rate': 9.011215106919982e-06, 'epoch': 0.55}
 55%|█████▍    | 8422/15436 [48:40<14:53:53,  7.65s/it] 55%|█████▍    | 8423/15436 [48:45<13:21:11,  6.85s/it]                                                       {'loss': 0.9635, 'grad_norm': 0.7311006792995575, 'learning_rate': 9.009127099498274e-06, 'epoch': 0.55}
 55%|█████▍    | 8423/15436 [48:45<13:21:11,  6.85s/it] 55%|█████▍    | 8424/15436 [48:51<12:41:13,  6.51s/it]                                                       {'loss': 0.9011, 'grad_norm': 0.7007138921296485, 'learning_rate': 9.00703913570383e-06, 'epoch': 0.55}
 55%|█████▍    | 8424/15436 [48:51<12:41:13,  6.51s/it] 55%|█████▍    | 8425/15436 [48:57<12:06:31,  6.22s/it]                                                       {'loss': 0.846, 'grad_norm': 0.7355120716944933, 'learning_rate': 9.004951215628597e-06, 'epoch': 0.55}
 55%|█████▍    | 8425/15436 [48:57<12:06:31,  6.22s/it] 55%|█████▍    | 8426/15436 [49:07<14:14:34,  7.31s/it]                                                       {'loss': 0.8179, 'grad_norm': 0.6932633266230147, 'learning_rate': 9.002863339364494e-06, 'epoch': 0.55}
 55%|█████▍    | 8426/15436 [49:07<14:14:34,  7.31s/it] 55%|█████▍    | 8427/15436 [49:16<15:20:34,  7.88s/it]                                                       {'loss': 0.8547, 'grad_norm': 0.6758400542813956, 'learning_rate': 9.000775507003448e-06, 'epoch': 0.55}
 55%|█████▍    | 8427/15436 [49:16<15:20:34,  7.88s/it] 55%|█████▍    | 8428/15436 [49:26<16:39:41,  8.56s/it]                                                       {'loss': 0.8821, 'grad_norm': 0.7264871664062108, 'learning_rate': 8.998687718637391e-06, 'epoch': 0.55}
 55%|█████▍    | 8428/15436 [49:26<16:39:41,  8.56s/it] 55%|█████▍    | 8429/15436 [49:32<15:06:00,  7.76s/it]                                                       {'loss': 0.9132, 'grad_norm': 0.7200691367759577, 'learning_rate': 8.996599974358242e-06, 'epoch': 0.55}
 55%|█████▍    | 8429/15436 [49:32<15:06:00,  7.76s/it] 55%|█████▍    | 8430/15436 [49:42<16:16:35,  8.36s/it]                                                       {'loss': 0.8809, 'grad_norm': 0.8336899707998562, 'learning_rate': 8.994512274257921e-06, 'epoch': 0.55}
 55%|█████▍    | 8430/15436 [49:42<16:16:35,  8.36s/it] 55%|█████▍    | 8431/15436 [49:49<15:29:24,  7.96s/it]                                                       {'loss': 0.8293, 'grad_norm': 0.7692634091799061, 'learning_rate': 8.992424618428353e-06, 'epoch': 0.55}
 55%|█████▍    | 8431/15436 [49:49<15:29:24,  7.96s/it] 55%|█████▍    | 8432/15436 [49:58<16:22:36,  8.42s/it]                                                       {'loss': 0.9088, 'grad_norm': 0.7564215471316005, 'learning_rate': 8.990337006961454e-06, 'epoch': 0.55}
 55%|█████▍    | 8432/15436 [49:58<16:22:36,  8.42s/it] 55%|█████▍    | 8433/15436 [50:05<15:38:39,  8.04s/it]                                                       {'loss': 0.8691, 'grad_norm': 0.7334080589753912, 'learning_rate': 8.988249439949135e-06, 'epoch': 0.55}
 55%|█████▍    | 8433/15436 [50:05<15:38:39,  8.04s/it] 55%|█████▍    | 8434/15436 [50:10<13:34:30,  6.98s/it]                                                       {'loss': 1.0103, 'grad_norm': 0.8144164234625461, 'learning_rate': 8.986161917483316e-06, 'epoch': 0.55}
 55%|█████▍    | 8434/15436 [50:10<13:34:30,  6.98s/it] 55%|█████▍    | 8435/15436 [50:16<13:07:39,  6.75s/it]                                                       {'loss': 0.9836, 'grad_norm': 0.7437361231845558, 'learning_rate': 8.984074439655907e-06, 'epoch': 0.55}
 55%|█████▍    | 8435/15436 [50:16<13:07:39,  6.75s/it] 55%|█████▍    | 8436/15436 [50:21<11:58:04,  6.15s/it]                                                       {'loss': 0.9444, 'grad_norm': 0.7677608340517804, 'learning_rate': 8.98198700655881e-06, 'epoch': 0.55}
 55%|█████▍    | 8436/15436 [50:21<11:58:04,  6.15s/it] 55%|█████▍    | 8437/15436 [50:31<14:06:27,  7.26s/it]                                                       {'loss': 0.9025, 'grad_norm': 0.7032796733762324, 'learning_rate': 8.979899618283947e-06, 'epoch': 0.55}
 55%|█████▍    | 8437/15436 [50:31<14:06:27,  7.26s/it] 55%|█████▍    | 8438/15436 [50:43<17:06:05,  8.80s/it]                                                       {'loss': 0.8327, 'grad_norm': 0.7681325211612416, 'learning_rate': 8.977812274923215e-06, 'epoch': 0.55}
 55%|█████▍    | 8438/15436 [50:43<17:06:05,  8.80s/it] 55%|█████▍    | 8439/15436 [50:47<14:36:49,  7.52s/it]                                                       {'loss': 0.9044, 'grad_norm': 0.785520165361762, 'learning_rate': 8.975724976568517e-06, 'epoch': 0.55}
 55%|█████▍    | 8439/15436 [50:47<14:36:49,  7.52s/it] 55%|█████▍    | 8440/15436 [50:58<16:21:58,  8.42s/it]                                                       {'loss': 0.9143, 'grad_norm': 0.6570050381239235, 'learning_rate': 8.973637723311759e-06, 'epoch': 0.55}
 55%|█████▍    | 8440/15436 [50:58<16:21:58,  8.42s/it] 55%|█████▍    | 8441/15436 [51:05<15:26:09,  7.94s/it]                                                       {'loss': 0.8082, 'grad_norm': 0.7547774224014173, 'learning_rate': 8.971550515244838e-06, 'epoch': 0.55}
 55%|█████▍    | 8441/15436 [51:05<15:26:09,  7.94s/it] 55%|█████▍    | 8442/15436 [51:11<14:09:40,  7.29s/it]                                                       {'loss': 0.8529, 'grad_norm': 0.8224150329599569, 'learning_rate': 8.969463352459655e-06, 'epoch': 0.55}
 55%|█████▍    | 8442/15436 [51:11<14:09:40,  7.29s/it] 55%|█████▍    | 8443/15436 [51:15<12:40:44,  6.53s/it]                                                       {'loss': 0.9265, 'grad_norm': 0.7064164919096537, 'learning_rate': 8.967376235048105e-06, 'epoch': 0.55}
 55%|█████▍    | 8443/15436 [51:15<12:40:44,  6.53s/it] 55%|█████▍    | 8444/15436 [51:21<12:22:09,  6.37s/it]                                                       {'loss': 0.8528, 'grad_norm': 0.7125449630580054, 'learning_rate': 8.96528916310208e-06, 'epoch': 0.55}
 55%|█████▍    | 8444/15436 [51:21<12:22:09,  6.37s/it] 55%|█████▍    | 8445/15436 [51:27<11:42:23,  6.03s/it]                                                       {'loss': 0.9227, 'grad_norm': 0.7092114656604063, 'learning_rate': 8.963202136713472e-06, 'epoch': 0.55}
 55%|█████▍    | 8445/15436 [51:27<11:42:23,  6.03s/it] 55%|█████▍    | 8446/15436 [51:34<12:13:46,  6.30s/it]                                                       {'loss': 0.993, 'grad_norm': 0.6936218647315673, 'learning_rate': 8.961115155974176e-06, 'epoch': 0.55}
 55%|█████▍    | 8446/15436 [51:34<12:13:46,  6.30s/it] 55%|█████▍    | 8447/15436 [51:41<13:08:45,  6.77s/it]                                                       {'loss': 0.9169, 'grad_norm': 0.7277617746317798, 'learning_rate': 8.959028220976071e-06, 'epoch': 0.55}
 55%|█████▍    | 8447/15436 [51:41<13:08:45,  6.77s/it] 55%|█████▍    | 8448/15436 [51:50<14:14:07,  7.33s/it]                                                       {'loss': 0.7898, 'grad_norm': 0.8150203940710237, 'learning_rate': 8.956941331811052e-06, 'epoch': 0.55}
 55%|█████▍    | 8448/15436 [51:50<14:14:07,  7.33s/it] 55%|█████▍    | 8449/15436 [51:55<12:40:49,  6.53s/it]                                                       {'loss': 0.8808, 'grad_norm': 0.7333533175487766, 'learning_rate': 8.954854488571002e-06, 'epoch': 0.55}
 55%|█████▍    | 8449/15436 [51:55<12:40:49,  6.53s/it] 55%|█████▍    | 8450/15436 [52:04<14:03:50,  7.25s/it]                                                       {'loss': 0.8361, 'grad_norm': 0.7140014576203378, 'learning_rate': 8.952767691347792e-06, 'epoch': 0.55}
 55%|█████▍    | 8450/15436 [52:04<14:03:50,  7.25s/it] 55%|█████▍    | 8451/15436 [52:08<12:39:17,  6.52s/it]                                                       {'loss': 0.9617, 'grad_norm': 0.7931220501105105, 'learning_rate': 8.950680940233317e-06, 'epoch': 0.55}
 55%|█████▍    | 8451/15436 [52:08<12:39:17,  6.52s/it] 55%|█████▍    | 8452/15436 [52:13<11:31:46,  5.94s/it]                                                       {'loss': 0.9394, 'grad_norm': 0.8107189047547069, 'learning_rate': 8.948594235319444e-06, 'epoch': 0.55}
 55%|█████▍    | 8452/15436 [52:13<11:31:46,  5.94s/it] 55%|█████▍    | 8453/15436 [52:21<12:32:39,  6.47s/it]                                                       {'loss': 0.9681, 'grad_norm': 0.7874819121780524, 'learning_rate': 8.946507576698048e-06, 'epoch': 0.55}
 55%|█████▍    | 8453/15436 [52:21<12:32:39,  6.47s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2670 > 2048). Running this sequence through the model will result in indexing errors
 55%|█████▍    | 8454/15436 [52:27<12:27:57,  6.43s/it]                                                       {'loss': 0.8856, 'grad_norm': 0.7503781368599789, 'learning_rate': 8.944420964461014e-06, 'epoch': 0.55}
 55%|█████▍    | 8454/15436 [52:27<12:27:57,  6.43s/it] 55%|█████▍    | 8455/15436 [52:35<13:09:25,  6.78s/it]                                                       {'loss': 0.9085, 'grad_norm': 0.7605958262260573, 'learning_rate': 8.942334398700206e-06, 'epoch': 0.55}
 55%|█████▍    | 8455/15436 [52:35<13:09:25,  6.78s/it] 55%|█████▍    | 8456/15436 [52:44<14:21:04,  7.40s/it]                                                       {'loss': 0.945, 'grad_norm': 0.74483637285822, 'learning_rate': 8.940247879507492e-06, 'epoch': 0.55}
 55%|█████▍    | 8456/15436 [52:44<14:21:04,  7.40s/it] 55%|█████▍    | 8457/15436 [52:50<13:58:16,  7.21s/it]                                                       {'loss': 0.9066, 'grad_norm': 0.7359289331783624, 'learning_rate': 8.938161406974745e-06, 'epoch': 0.55}
 55%|█████▍    | 8457/15436 [52:50<13:58:16,  7.21s/it] 55%|█████▍    | 8458/15436 [52:59<14:44:51,  7.61s/it]                                                       {'loss': 0.8671, 'grad_norm': 0.7186618415727747, 'learning_rate': 8.936074981193824e-06, 'epoch': 0.55}
 55%|█████▍    | 8458/15436 [52:59<14:44:51,  7.61s/it] 55%|█████▍    | 8459/15436 [53:08<15:39:31,  8.08s/it]                                                       {'loss': 0.8596, 'grad_norm': 0.6996714404650003, 'learning_rate': 8.933988602256599e-06, 'epoch': 0.55}
 55%|█████▍    | 8459/15436 [53:08<15:39:31,  8.08s/it] 55%|█████▍    | 8460/15436 [53:14<14:10:58,  7.32s/it]                                                       {'loss': 0.9801, 'grad_norm': 0.8370113069676143, 'learning_rate': 8.931902270254929e-06, 'epoch': 0.55}
 55%|█████▍    | 8460/15436 [53:14<14:10:58,  7.32s/it] 55%|█████▍    | 8461/15436 [53:24<15:46:52,  8.15s/it]                                                       {'loss': 0.9463, 'grad_norm': 0.8139804446629096, 'learning_rate': 8.92981598528067e-06, 'epoch': 0.55}
 55%|█████▍    | 8461/15436 [53:24<15:46:52,  8.15s/it] 55%|█████▍    | 8462/15436 [53:29<13:55:06,  7.18s/it]                                                       {'loss': 0.9973, 'grad_norm': 0.7280585648583688, 'learning_rate': 8.927729747425686e-06, 'epoch': 0.55}
 55%|█████▍    | 8462/15436 [53:29<13:55:06,  7.18s/it] 55%|█████▍    | 8463/15436 [53:36<13:52:45,  7.17s/it]                                                       {'loss': 0.9538, 'grad_norm': 0.7139871372408502, 'learning_rate': 8.925643556781828e-06, 'epoch': 0.55}
 55%|█████▍    | 8463/15436 [53:36<13:52:45,  7.17s/it] 55%|█████▍    | 8464/15436 [53:42<13:11:16,  6.81s/it]                                                       {'loss': 0.9235, 'grad_norm': 0.7033005405612935, 'learning_rate': 8.923557413440947e-06, 'epoch': 0.55}
 55%|█████▍    | 8464/15436 [53:42<13:11:16,  6.81s/it] 55%|█████▍    | 8465/15436 [53:46<11:48:40,  6.10s/it]                                                       {'loss': 0.906, 'grad_norm': 0.6948786142155049, 'learning_rate': 8.921471317494901e-06, 'epoch': 0.55}
 55%|█████▍    | 8465/15436 [53:46<11:48:40,  6.10s/it] 55%|█████▍    | 8466/15436 [53:51<11:20:40,  5.86s/it]                                                       {'loss': 0.8659, 'grad_norm': 1.0107433441246312, 'learning_rate': 8.919385269035535e-06, 'epoch': 0.55}
 55%|█████▍    | 8466/15436 [53:51<11:20:40,  5.86s/it] 55%|█████▍    | 8467/15436 [54:00<12:49:26,  6.62s/it]                                                       {'loss': 0.9356, 'grad_norm': 0.7556777682919052, 'learning_rate': 8.91729926815469e-06, 'epoch': 0.55}
 55%|█████▍    | 8467/15436 [54:00<12:49:26,  6.62s/it] 55%|█████▍    | 8468/15436 [54:04<11:36:56,  6.00s/it]                                                       {'loss': 0.887, 'grad_norm': 0.7318042265046064, 'learning_rate': 8.915213314944224e-06, 'epoch': 0.55}
 55%|█████▍    | 8468/15436 [54:04<11:36:56,  6.00s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2345 > 2048). Running this sequence through the model will result in indexing errors
 55%|█████▍    | 8469/15436 [54:10<11:14:31,  5.81s/it]                                                       {'loss': 1.0335, 'grad_norm': 0.7133942227465742, 'learning_rate': 8.91312740949597e-06, 'epoch': 0.55}
 55%|█████▍    | 8469/15436 [54:10<11:14:31,  5.81s/it] 55%|█████▍    | 8470/15436 [54:20<13:43:18,  7.09s/it]                                                       {'loss': 0.7814, 'grad_norm': 0.730294266069274, 'learning_rate': 8.911041551901766e-06, 'epoch': 0.55}
 55%|█████▍    | 8470/15436 [54:20<13:43:18,  7.09s/it] 55%|█████▍    | 8471/15436 [54:32<16:33:58,  8.56s/it]                                                       {'loss': 0.8278, 'grad_norm': 0.6979030692665248, 'learning_rate': 8.90895574225346e-06, 'epoch': 0.55}
 55%|█████▍    | 8471/15436 [54:32<16:33:58,  8.56s/it] 55%|█████▍    | 8472/15436 [54:40<16:35:24,  8.58s/it]                                                       {'loss': 0.9295, 'grad_norm': 0.7292640827957799, 'learning_rate': 8.906869980642882e-06, 'epoch': 0.55}
 55%|█████▍    | 8472/15436 [54:40<16:35:24,  8.58s/it] 55%|█████▍    | 8473/15436 [54:51<17:45:43,  9.18s/it]                                                       {'loss': 0.843, 'grad_norm': 0.698137660277093, 'learning_rate': 8.904784267161868e-06, 'epoch': 0.55}
 55%|█████▍    | 8473/15436 [54:51<17:45:43,  9.18s/it] 55%|█████▍    | 8474/15436 [54:57<15:58:56,  8.26s/it]                                                       {'loss': 0.9275, 'grad_norm': 0.7129630457768723, 'learning_rate': 8.902698601902253e-06, 'epoch': 0.55}
 55%|█████▍    | 8474/15436 [54:57<15:58:56,  8.26s/it] 55%|█████▍    | 8475/15436 [55:04<15:21:07,  7.94s/it]                                                       {'loss': 0.847, 'grad_norm': 0.8012120260755082, 'learning_rate': 8.90061298495586e-06, 'epoch': 0.55}
 55%|█████▍    | 8475/15436 [55:04<15:21:07,  7.94s/it] 55%|█████▍    | 8476/15436 [55:10<14:01:59,  7.26s/it]                                                       {'loss': 0.957, 'grad_norm': 0.6911667264310678, 'learning_rate': 8.898527416414524e-06, 'epoch': 0.55}
 55%|█████▍    | 8476/15436 [55:10<14:01:59,  7.26s/it] 55%|█████▍    | 8477/15436 [55:16<13:22:03,  6.92s/it]                                                       {'loss': 0.9787, 'grad_norm': 0.8057309515410906, 'learning_rate': 8.896441896370069e-06, 'epoch': 0.55}
 55%|█████▍    | 8477/15436 [55:16<13:22:03,  6.92s/it] 55%|█████▍    | 8478/15436 [55:22<13:03:39,  6.76s/it]                                                       {'loss': 0.9587, 'grad_norm': 0.6987651917870225, 'learning_rate': 8.894356424914314e-06, 'epoch': 0.55}
 55%|█████▍    | 8478/15436 [55:22<13:03:39,  6.76s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (2785 > 2048). Running this sequence through the model will result in indexing errors
 55%|█████▍    | 8479/15436 [55:33<15:22:04,  7.95s/it]                                                       {'loss': 0.9268, 'grad_norm': 0.765861508931529, 'learning_rate': 8.892271002139085e-06, 'epoch': 0.55}
 55%|█████▍    | 8479/15436 [55:33<15:22:04,  7.95s/it] 55%|█████▍    | 8480/15436 [55:39<14:03:34,  7.28s/it]                                                       {'loss': 0.9276, 'grad_norm': 0.7251947809286494, 'learning_rate': 8.890185628136204e-06, 'epoch': 0.55}
 55%|█████▍    | 8480/15436 [55:39<14:03:34,  7.28s/it] 55%|█████▍    | 8481/15436 [55:48<15:00:44,  7.77s/it]                                                       {'loss': 0.8714, 'grad_norm': 0.7518978549257543, 'learning_rate': 8.888100302997479e-06, 'epoch': 0.55}
 55%|█████▍    | 8481/15436 [55:48<15:00:44,  7.77s/it] 55%|█████▍    | 8482/15436 [55:53<13:44:38,  7.12s/it]                                                       {'loss': 0.8907, 'grad_norm': 0.771199996858005, 'learning_rate': 8.886015026814736e-06, 'epoch': 0.55}
 55%|█████▍    | 8482/15436 [55:53<13:44:38,  7.12s/it] 55%|█████▍    | 8483/15436 [56:02<14:43:31,  7.62s/it]                                                       {'loss': 0.8765, 'grad_norm': 0.7986160077330355, 'learning_rate': 8.88392979967978e-06, 'epoch': 0.55}
 55%|█████▍    | 8483/15436 [56:02<14:43:31,  7.62s/it] 55%|█████▍    | 8484/15436 [56:09<14:15:08,  7.38s/it]                                                       {'loss': 0.9429, 'grad_norm': 0.7238704504776627, 'learning_rate': 8.881844621684423e-06, 'epoch': 0.55}
 55%|█████▍    | 8484/15436 [56:09<14:15:08,  7.38s/it] 55%|█████▍    | 8485/15436 [56:14<12:55:40,  6.70s/it]                                                       {'loss': 0.9324, 'grad_norm': 0.6745673176684445, 'learning_rate': 8.879759492920477e-06, 'epoch': 0.55}
 55%|█████▍    | 8485/15436 [56:14<12:55:40,  6.70s/it] 55%|█████▍    | 8486/15436 [56:24<14:53:35,  7.71s/it]                                                       {'loss': 0.9209, 'grad_norm': 0.7045584186506547, 'learning_rate': 8.877674413479745e-06, 'epoch': 0.55}
 55%|█████▍    | 8486/15436 [56:24<14:53:35,  7.71s/it] 55%|█████▍    | 8487/15436 [56:36<17:00:48,  8.81s/it]                                                       {'loss': 0.9439, 'grad_norm': 0.7752673225469915, 'learning_rate': 8.875589383454031e-06, 'epoch': 0.55}
 55%|█████▍    | 8487/15436 [56:36<17:00:48,  8.81s/it] 55%|█████▍    | 8488/15436 [56:40<14:23:54,  7.46s/it]                                                       {'loss': 0.9705, 'grad_norm': 0.7526038094009773, 'learning_rate': 8.873504402935142e-06, 'epoch': 0.55}
 55%|█████▍    | 8488/15436 [56:40<14:23:54,  7.46s/it] 55%|█████▍    | 8489/15436 [56:46<13:47:22,  7.15s/it]                                                       {'loss': 0.8892, 'grad_norm': 0.7362774407002514, 'learning_rate': 8.871419472014871e-06, 'epoch': 0.55}
 55%|█████▍    | 8489/15436 [56:46<13:47:22,  7.15s/it] 55%|█████▌    | 8490/15436 [56:52<12:45:22,  6.61s/it]                                                       {'loss': 0.8701, 'grad_norm': 0.7415454788588867, 'learning_rate': 8.869334590785021e-06, 'epoch': 0.55}
 55%|█████▌    | 8490/15436 [56:52<12:45:22,  6.61s/it] 55%|█████▌    | 8491/15436 [57:02<14:40:57,  7.61s/it]                                                       {'loss': 0.8847, 'grad_norm': 0.775533122523874, 'learning_rate': 8.867249759337386e-06, 'epoch': 0.55}
 55%|█████▌    | 8491/15436 [57:02<14:40:57,  7.61s/it] 55%|█████▌    | 8492/15436 [57:07<13:22:11,  6.93s/it]                                                       {'loss': 0.8653, 'grad_norm': 0.6729499843138241, 'learning_rate': 8.865164977763756e-06, 'epoch': 0.55}
 55%|█████▌    | 8492/15436 [57:07<13:22:11,  6.93s/it] 55%|█████▌    | 8493/15436 [57:13<12:53:00,  6.68s/it]                                                       {'loss': 0.8586, 'grad_norm': 0.7363381040118558, 'learning_rate': 8.863080246155928e-06, 'epoch': 0.55}
 55%|█████▌    | 8493/15436 [57:13<12:53:00,  6.68s/it] 55%|█████▌    | 8494/15436 [57:22<14:06:37,  7.32s/it]                                                       {'loss': 0.8457, 'grad_norm': 0.7231187142278669, 'learning_rate': 8.86099556460569e-06, 'epoch': 0.55}
 55%|█████▌    | 8494/15436 [57:22<14:06:37,  7.32s/it] 55%|█████▌    | 8495/15436 [57:26<12:27:28,  6.46s/it]                                                       {'loss': 0.8525, 'grad_norm': 0.8022425048224732, 'learning_rate': 8.85891093320482e-06, 'epoch': 0.55}
 55%|█████▌    | 8495/15436 [57:26<12:27:28,  6.46s/it]