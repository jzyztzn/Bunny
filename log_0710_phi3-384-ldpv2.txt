nohup: ignoring input
[2024-07-12 17:53:07,739] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 17:53:09,566] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-12 17:53:09,566] [INFO] [runner.py:568:main] cmd = /root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None bunny/train/train.py --deepspeed ./script/deepspeed/zero2.json --model_name_or_path /root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct --model_type phi-3 --version phi3 --data_path ./data/pretrain/bunny_pretrain_laion_2m.json --image_folder ./data/pretrain/images --vision_tower /root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384 --mm_projector_type ldpv2 --tune_mm_mlp_adapter True --image_aspect_ratio square --bf16 True --output_dir ./checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2 --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 5e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
[2024-07-12 17:53:10,909] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 17:53:12,673] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-12 17:53:12,673] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-12 17:53:12,673] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-12 17:53:12,673] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-12 17:53:12,673] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-12 17:53:12,674] [INFO] [launch.py:256:main] process 773466 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=0', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:12,674] [INFO] [launch.py:256:main] process 773467 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=1', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:12,674] [INFO] [launch.py:256:main] process 773468 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=2', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:12,675] [INFO] [launch.py:256:main] process 773469 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=3', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:12,675] [INFO] [launch.py:256:main] process 773470 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=4', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:12,675] [INFO] [launch.py:256:main] process 773471 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=5', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:12,676] [INFO] [launch.py:256:main] process 773472 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=6', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:12,676] [INFO] [launch.py:256:main] process 773473 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=7', '--deepspeed', './script/deepspeed/zero2.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/pretrain/bunny_pretrain_laion_2m.json', '--image_folder', './data/pretrain/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--mm_projector_type', 'ldpv2', '--tune_mm_mlp_adapter', 'True', '--image_aspect_ratio', 'square', '--bf16', 'True', '--output_dir', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '4', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '5e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 17:53:18,173] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 17:53:18,308] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 17:53:18,333] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 17:53:18,375] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 17:53:18,381] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 17:53:18,384] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 17:53:18,389] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-07-12 17:53:18,398] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 17:53:18,408] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 17:53:18,543] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 17:53:18,568] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 17:53:18,615] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 17:53:18,615] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-12 17:53:18,619] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 17:53:18,621] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 17:53:18,625] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 17:53:18,635] [INFO] [comm.py:637:init_distributed] cdb=None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.33s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.15s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.59s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.99s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.77s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.74s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.54s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.89s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.78s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.98s/it]
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Formatting inputs...Skip in lazy mode
Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Formatting inputs...Skip in lazy mode
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
You are not running the flash-attention implementation, expect numerical differences.
  0%|          | 0/7812 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.
  0%|          | 1/7812 [00:03<7:32:44,  3.48s/it]                                                  {'loss': 6.4297, 'grad_norm': 52.63005828857422, 'learning_rate': 2.1276595744680853e-06, 'epoch': 0.0}
  0%|          | 1/7812 [00:04<7:32:44,  3.48s/it]  0%|          | 2/7812 [00:06<7:27:06,  3.43s/it]                                                  {'loss': 6.743, 'grad_norm': 56.496803283691406, 'learning_rate': 4.255319148936171e-06, 'epoch': 0.0}
  0%|          | 2/7812 [00:06<7:27:06,  3.43s/it]  0%|          | 3/7812 [00:08<5:42:44,  2.63s/it]                                                  {'loss': 7.031, 'grad_norm': 55.470829010009766, 'learning_rate': 6.3829787234042555e-06, 'epoch': 0.0}
  0%|          | 3/7812 [00:08<5:42:44,  2.63s/it]  0%|          | 4/7812 [00:10<4:57:52,  2.29s/it]                                                  {'loss': 6.7344, 'grad_norm': 58.745086669921875, 'learning_rate': 8.510638297872341e-06, 'epoch': 0.0}
  0%|          | 4/7812 [00:10<4:57:52,  2.29s/it]  0%|          | 5/7812 [00:12<4:37:47,  2.13s/it]                                                  {'loss': 6.3627, 'grad_norm': 47.41704177856445, 'learning_rate': 1.0638297872340426e-05, 'epoch': 0.0}
  0%|          | 5/7812 [00:12<4:37:47,  2.13s/it]  0%|          | 6/7812 [00:14<4:30:38,  2.08s/it]                                                  {'loss': 5.9737, 'grad_norm': 34.47594451904297, 'learning_rate': 1.2765957446808511e-05, 'epoch': 0.0}
  0%|          | 6/7812 [00:14<4:30:38,  2.08s/it]  0%|          | 7/7812 [00:15<4:18:54,  1.99s/it]                                                  {'loss': 5.362, 'grad_norm': 20.792375564575195, 'learning_rate': 1.4893617021276598e-05, 'epoch': 0.0}
  0%|          | 7/7812 [00:15<4:18:54,  1.99s/it]  0%|          | 8/7812 [00:17<4:13:08,  1.95s/it]                                                  {'loss': 5.2114, 'grad_norm': 17.293115615844727, 'learning_rate': 1.7021276595744682e-05, 'epoch': 0.0}
  0%|          | 8/7812 [00:17<4:13:08,  1.95s/it]  0%|          | 9/7812 [00:19<4:06:35,  1.90s/it]                                                  {'loss': 5.0572, 'grad_norm': 13.655712127685547, 'learning_rate': 1.9148936170212766e-05, 'epoch': 0.0}
  0%|          | 9/7812 [00:19<4:06:35,  1.90s/it]  0%|          | 10/7812 [00:21<4:02:34,  1.87s/it]                                                   {'loss': 4.7941, 'grad_norm': 10.589371681213379, 'learning_rate': 2.1276595744680852e-05, 'epoch': 0.0}
  0%|          | 10/7812 [00:21<4:02:34,  1.87s/it]  0%|          | 11/7812 [00:23<4:08:11,  1.91s/it]                                                   {'loss': 4.5759, 'grad_norm': 7.815810680389404, 'learning_rate': 2.3404255319148935e-05, 'epoch': 0.0}
  0%|          | 11/7812 [00:23<4:08:11,  1.91s/it]  0%|          | 12/7812 [00:25<4:04:03,  1.88s/it]                                                   {'loss': 4.5142, 'grad_norm': 6.955226421356201, 'learning_rate': 2.5531914893617022e-05, 'epoch': 0.0}
  0%|          | 12/7812 [00:25<4:04:03,  1.88s/it]  0%|          | 13/7812 [00:27<4:05:23,  1.89s/it]                                                   {'loss': 4.4694, 'grad_norm': 6.442249774932861, 'learning_rate': 2.7659574468085105e-05, 'epoch': 0.0}
  0%|          | 13/7812 [00:27<4:05:23,  1.89s/it]  0%|          | 14/7812 [00:28<4:02:04,  1.86s/it]                                                   {'loss': 4.3842, 'grad_norm': 5.999450206756592, 'learning_rate': 2.9787234042553195e-05, 'epoch': 0.0}
  0%|          | 14/7812 [00:28<4:02:04,  1.86s/it]  0%|          | 15/7812 [00:30<4:09:56,  1.92s/it]                                                   {'loss': 4.2896, 'grad_norm': 5.624732494354248, 'learning_rate': 3.1914893617021275e-05, 'epoch': 0.0}
  0%|          | 15/7812 [00:30<4:09:56,  1.92s/it]  0%|          | 16/7812 [00:32<4:06:02,  1.89s/it]                                                   {'loss': 4.119, 'grad_norm': 5.5541486740112305, 'learning_rate': 3.4042553191489365e-05, 'epoch': 0.0}
  0%|          | 16/7812 [00:32<4:06:02,  1.89s/it]  0%|          | 17/7812 [00:34<4:00:38,  1.85s/it]                                                   {'loss': 4.2268, 'grad_norm': 5.11347770690918, 'learning_rate': 3.617021276595744e-05, 'epoch': 0.0}
  0%|          | 17/7812 [00:34<4:00:38,  1.85s/it]  0%|          | 18/7812 [00:36<4:06:11,  1.90s/it]                                                   {'loss': 4.1233, 'grad_norm': 4.336978912353516, 'learning_rate': 3.829787234042553e-05, 'epoch': 0.0}
  0%|          | 18/7812 [00:36<4:06:11,  1.90s/it]  0%|          | 19/7812 [00:38<4:09:43,  1.92s/it]                                                   {'loss': 4.2081, 'grad_norm': 4.202629089355469, 'learning_rate': 4.042553191489362e-05, 'epoch': 0.0}
  0%|          | 19/7812 [00:38<4:09:43,  1.92s/it]  0%|          | 20/7812 [00:40<4:09:09,  1.92s/it]                                                   {'loss': 4.069, 'grad_norm': 3.927377939224243, 'learning_rate': 4.2553191489361704e-05, 'epoch': 0.0}
  0%|          | 20/7812 [00:40<4:09:09,  1.92s/it]  0%|          | 21/7812 [00:42<4:06:52,  1.90s/it]                                                   {'loss': 3.9448, 'grad_norm': 3.7077298164367676, 'learning_rate': 4.468085106382979e-05, 'epoch': 0.0}
  0%|          | 21/7812 [00:42<4:06:52,  1.90s/it]  0%|          | 22/7812 [00:44<4:08:56,  1.92s/it]                                                   {'loss': 3.8903, 'grad_norm': 3.2846202850341797, 'learning_rate': 4.680851063829787e-05, 'epoch': 0.0}
  0%|          | 22/7812 [00:44<4:08:56,  1.92s/it]  0%|          | 23/7812 [00:46<4:11:14,  1.94s/it]                                                   {'loss': 3.9736, 'grad_norm': 3.1360526084899902, 'learning_rate': 4.893617021276596e-05, 'epoch': 0.0}
  0%|          | 23/7812 [00:46<4:11:14,  1.94s/it]  0%|          | 24/7812 [00:48<4:03:48,  1.88s/it]                                                   {'loss': 3.859, 'grad_norm': 2.6201882362365723, 'learning_rate': 5.1063829787234044e-05, 'epoch': 0.0}
  0%|          | 24/7812 [00:48<4:03:48,  1.88s/it]  0%|          | 25/7812 [00:49<3:59:59,  1.85s/it]                                                   {'loss': 3.9722, 'grad_norm': 2.624680995941162, 'learning_rate': 5.319148936170213e-05, 'epoch': 0.0}
  0%|          | 25/7812 [00:49<3:59:59,  1.85s/it]  0%|          | 26/7812 [00:51<4:07:17,  1.91s/it]                                                   {'loss': 3.75, 'grad_norm': 2.02661395072937, 'learning_rate': 5.531914893617021e-05, 'epoch': 0.0}
  0%|          | 26/7812 [00:51<4:07:17,  1.91s/it]  0%|          | 27/7812 [00:53<4:11:13,  1.94s/it]                                                   {'loss': 3.731, 'grad_norm': 1.7944267988204956, 'learning_rate': 5.74468085106383e-05, 'epoch': 0.0}
  0%|          | 27/7812 [00:53<4:11:13,  1.94s/it]  0%|          | 28/7812 [00:55<4:09:20,  1.92s/it]                                                   {'loss': 3.7818, 'grad_norm': 2.006253957748413, 'learning_rate': 5.957446808510639e-05, 'epoch': 0.0}
  0%|          | 28/7812 [00:55<4:09:20,  1.92s/it]  0%|          | 29/7812 [00:57<4:10:26,  1.93s/it]                                                   {'loss': 3.8908, 'grad_norm': 1.8152049779891968, 'learning_rate': 6.170212765957447e-05, 'epoch': 0.0}
  0%|          | 29/7812 [00:57<4:10:26,  1.93s/it]  0%|          | 30/7812 [00:59<4:10:30,  1.93s/it]                                                   {'loss': 3.7599, 'grad_norm': 1.778726577758789, 'learning_rate': 6.382978723404255e-05, 'epoch': 0.0}
  0%|          | 30/7812 [00:59<4:10:30,  1.93s/it]  0%|          | 31/7812 [01:01<4:06:22,  1.90s/it]                                                   {'loss': 3.7194, 'grad_norm': 1.627263069152832, 'learning_rate': 6.595744680851063e-05, 'epoch': 0.0}
  0%|          | 31/7812 [01:01<4:06:22,  1.90s/it]  0%|          | 32/7812 [01:03<4:04:21,  1.88s/it]                                                   {'loss': 3.7685, 'grad_norm': 1.6862599849700928, 'learning_rate': 6.808510638297873e-05, 'epoch': 0.0}
  0%|          | 32/7812 [01:03<4:04:21,  1.88s/it]  0%|          | 33/7812 [01:05<4:00:29,  1.85s/it]                                                   {'loss': 3.8435, 'grad_norm': 1.609696626663208, 'learning_rate': 7.021276595744681e-05, 'epoch': 0.0}
  0%|          | 33/7812 [01:05<4:00:29,  1.85s/it]  0%|          | 34/7812 [01:06<4:01:23,  1.86s/it]                                                   {'loss': 3.7508, 'grad_norm': 1.654986023902893, 'learning_rate': 7.234042553191488e-05, 'epoch': 0.0}
  0%|          | 34/7812 [01:06<4:01:23,  1.86s/it]  0%|          | 35/7812 [01:08<3:58:51,  1.84s/it]                                                   {'loss': 3.6748, 'grad_norm': 1.1148648262023926, 'learning_rate': 7.446808510638298e-05, 'epoch': 0.0}
  0%|          | 35/7812 [01:08<3:58:51,  1.84s/it]  0%|          | 36/7812 [01:10<4:02:20,  1.87s/it]                                                   {'loss': 3.7854, 'grad_norm': 1.3743093013763428, 'learning_rate': 7.659574468085106e-05, 'epoch': 0.0}
  0%|          | 36/7812 [01:10<4:02:20,  1.87s/it]  0%|          | 37/7812 [01:12<3:59:55,  1.85s/it]                                                   {'loss': 3.7211, 'grad_norm': 1.0742518901824951, 'learning_rate': 7.872340425531916e-05, 'epoch': 0.0}
  0%|          | 37/7812 [01:12<3:59:55,  1.85s/it]  0%|          | 38/7812 [01:14<4:05:06,  1.89s/it]                                                   {'loss': 3.5711, 'grad_norm': 0.9207404851913452, 'learning_rate': 8.085106382978724e-05, 'epoch': 0.0}
  0%|          | 38/7812 [01:14<4:05:06,  1.89s/it]  0%|          | 39/7812 [01:16<4:08:20,  1.92s/it]                                                   {'loss': 3.717, 'grad_norm': 1.0812976360321045, 'learning_rate': 8.297872340425531e-05, 'epoch': 0.0}
  0%|          | 39/7812 [01:16<4:08:20,  1.92s/it]  1%|          | 40/7812 [01:18<4:09:51,  1.93s/it]                                                   {'loss': 3.6426, 'grad_norm': 0.9150527715682983, 'learning_rate': 8.510638297872341e-05, 'epoch': 0.01}
  1%|          | 40/7812 [01:18<4:09:51,  1.93s/it]  1%|          | 41/7812 [01:20<4:02:25,  1.87s/it]                                                   {'loss': 3.7514, 'grad_norm': 1.0426942110061646, 'learning_rate': 8.723404255319149e-05, 'epoch': 0.01}
  1%|          | 41/7812 [01:20<4:02:25,  1.87s/it]  1%|          | 42/7812 [01:21<3:58:51,  1.84s/it]                                                   {'loss': 3.8171, 'grad_norm': 1.0139790773391724, 'learning_rate': 8.936170212765958e-05, 'epoch': 0.01}
  1%|          | 42/7812 [01:21<3:58:51,  1.84s/it]  1%|          | 43/7812 [01:23<3:55:36,  1.82s/it]                                                   {'loss': 3.7166, 'grad_norm': 0.8207293152809143, 'learning_rate': 9.148936170212766e-05, 'epoch': 0.01}
  1%|          | 43/7812 [01:23<3:55:36,  1.82s/it]  1%|          | 44/7812 [01:25<4:00:06,  1.85s/it]                                                   {'loss': 3.5043, 'grad_norm': 0.6963664293289185, 'learning_rate': 9.361702127659574e-05, 'epoch': 0.01}
  1%|          | 44/7812 [01:25<4:00:06,  1.85s/it]  1%|          | 45/7812 [01:27<4:03:35,  1.88s/it]                                                   {'loss': 3.6697, 'grad_norm': 0.8219686150550842, 'learning_rate': 9.574468085106382e-05, 'epoch': 0.01}
  1%|          | 45/7812 [01:27<4:03:35,  1.88s/it]  1%|          | 46/7812 [01:29<3:58:23,  1.84s/it]                                                   {'loss': 3.7099, 'grad_norm': 0.9546076655387878, 'learning_rate': 9.787234042553192e-05, 'epoch': 0.01}
  1%|          | 46/7812 [01:29<3:58:23,  1.84s/it]  1%|          | 47/7812 [01:31<4:18:21,  2.00s/it]                                                   {'loss': 3.4626, 'grad_norm': 0.8241696953773499, 'learning_rate': 0.0001, 'epoch': 0.01}
  1%|          | 47/7812 [01:31<4:18:21,  2.00s/it]  1%|          | 48/7812 [01:33<4:14:09,  1.96s/it]                                                   {'loss': 3.6563, 'grad_norm': 0.7575504183769226, 'learning_rate': 0.00010212765957446809, 'epoch': 0.01}
  1%|          | 48/7812 [01:33<4:14:09,  1.96s/it]  1%|          | 49/7812 [01:35<4:09:03,  1.93s/it]                                                   {'loss': 3.5938, 'grad_norm': 0.6961519122123718, 'learning_rate': 0.00010425531914893617, 'epoch': 0.01}
  1%|          | 49/7812 [01:35<4:09:03,  1.93s/it]  1%|          | 50/7812 [01:37<4:12:58,  1.96s/it]                                                   {'loss': 3.6723, 'grad_norm': 0.7485433220863342, 'learning_rate': 0.00010638297872340425, 'epoch': 0.01}
  1%|          | 50/7812 [01:37<4:12:58,  1.96s/it]  1%|          | 51/7812 [01:39<4:13:53,  1.96s/it]                                                   {'loss': 3.586, 'grad_norm': 0.6159757375717163, 'learning_rate': 0.00010851063829787235, 'epoch': 0.01}
  1%|          | 51/7812 [01:39<4:13:53,  1.96s/it]  1%|          | 52/7812 [01:41<4:21:02,  2.02s/it]                                                   {'loss': 3.7147, 'grad_norm': 0.7811460494995117, 'learning_rate': 0.00011063829787234042, 'epoch': 0.01}
  1%|          | 52/7812 [01:41<4:21:02,  2.02s/it]  1%|          | 53/7812 [01:43<4:17:06,  1.99s/it]                                                   {'loss': 3.7814, 'grad_norm': 0.8009023070335388, 'learning_rate': 0.0001127659574468085, 'epoch': 0.01}
  1%|          | 53/7812 [01:43<4:17:06,  1.99s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (3817 > 2048). Running this sequence through the model will result in indexing errors
  1%|          | 54/7812 [01:45<4:25:38,  2.05s/it]                                                   {'loss': 3.4941, 'grad_norm': 0.6503137946128845, 'learning_rate': 0.0001148936170212766, 'epoch': 0.01}
  1%|          | 54/7812 [01:45<4:25:38,  2.05s/it]  1%|          | 55/7812 [01:47<4:15:38,  1.98s/it]                                                   {'loss': 3.602, 'grad_norm': 0.6529762744903564, 'learning_rate': 0.00011702127659574468, 'epoch': 0.01}
  1%|          | 55/7812 [01:47<4:15:38,  1.98s/it]  1%|          | 56/7812 [01:54<7:35:30,  3.52s/it]                                                   {'loss': 3.5561, 'grad_norm': 0.6886874437332153, 'learning_rate': 0.00011914893617021278, 'epoch': 0.01}
  1%|          | 56/7812 [01:54<7:35:30,  3.52s/it]  1%|          | 57/7812 [01:56<6:24:56,  2.98s/it]                                                   {'loss': 3.6214, 'grad_norm': 0.6362906694412231, 'learning_rate': 0.00012127659574468085, 'epoch': 0.01}
  1%|          | 57/7812 [01:56<6:24:56,  2.98s/it]  1%|          | 58/7812 [01:58<5:45:10,  2.67s/it]                                                   {'loss': 3.6118, 'grad_norm': 0.6494067907333374, 'learning_rate': 0.00012340425531914893, 'epoch': 0.01}
  1%|          | 58/7812 [01:58<5:45:10,  2.67s/it]  1%|          | 59/7812 [02:00<5:18:10,  2.46s/it]                                                   {'loss': 3.5132, 'grad_norm': 0.6232632398605347, 'learning_rate': 0.00012553191489361702, 'epoch': 0.01}
  1%|          | 59/7812 [02:00<5:18:10,  2.46s/it]  1%|          | 60/7812 [02:02<4:52:48,  2.27s/it]                                                   {'loss': 3.5537, 'grad_norm': 0.5877596735954285, 'learning_rate': 0.0001276595744680851, 'epoch': 0.01}
  1%|          | 60/7812 [02:02<4:52:48,  2.27s/it]  1%|          | 61/7812 [02:03<4:38:22,  2.15s/it]                                                   {'loss': 3.6597, 'grad_norm': 0.6677610874176025, 'learning_rate': 0.00012978723404255318, 'epoch': 0.01}
  1%|          | 61/7812 [02:03<4:38:22,  2.15s/it]  1%|          | 62/7812 [02:05<4:32:42,  2.11s/it]                                                   {'loss': 3.6566, 'grad_norm': 0.548021137714386, 'learning_rate': 0.00013191489361702127, 'epoch': 0.01}
  1%|          | 62/7812 [02:05<4:32:42,  2.11s/it]  1%|          | 63/7812 [02:07<4:18:58,  2.01s/it]                                                   {'loss': 3.7564, 'grad_norm': 0.6705326437950134, 'learning_rate': 0.00013404255319148938, 'epoch': 0.01}
  1%|          | 63/7812 [02:07<4:18:58,  2.01s/it]  1%|          | 64/7812 [02:09<4:17:25,  1.99s/it]                                                   {'loss': 3.5586, 'grad_norm': 0.6111128330230713, 'learning_rate': 0.00013617021276595746, 'epoch': 0.01}
  1%|          | 64/7812 [02:09<4:17:25,  1.99s/it]  1%|          | 65/7812 [02:11<4:17:49,  2.00s/it]                                                   {'loss': 3.6193, 'grad_norm': 0.5834950804710388, 'learning_rate': 0.00013829787234042554, 'epoch': 0.01}
  1%|          | 65/7812 [02:11<4:17:49,  2.00s/it]  1%|          | 66/7812 [02:13<4:08:18,  1.92s/it]                                                   {'loss': 3.5675, 'grad_norm': 0.6366112232208252, 'learning_rate': 0.00014042553191489363, 'epoch': 0.01}
  1%|          | 66/7812 [02:13<4:08:18,  1.92s/it]  1%|          | 67/7812 [02:15<4:00:03,  1.86s/it]                                                   {'loss': 3.6599, 'grad_norm': 0.6139064431190491, 'learning_rate': 0.0001425531914893617, 'epoch': 0.01}
  1%|          | 67/7812 [02:15<4:00:03,  1.86s/it]  1%|          | 68/7812 [02:17<4:23:20,  2.04s/it]                                                   {'loss': 3.6194, 'grad_norm': 0.5388036966323853, 'learning_rate': 0.00014468085106382977, 'epoch': 0.01}
  1%|          | 68/7812 [02:17<4:23:20,  2.04s/it]  1%|          | 69/7812 [02:19<4:15:03,  1.98s/it]                                                   {'loss': 3.7528, 'grad_norm': 0.7472404837608337, 'learning_rate': 0.00014680851063829788, 'epoch': 0.01}
  1%|          | 69/7812 [02:19<4:15:03,  1.98s/it]  1%|          | 70/7812 [02:21<4:09:13,  1.93s/it]                                                   {'loss': 3.4973, 'grad_norm': 0.555511474609375, 'learning_rate': 0.00014893617021276596, 'epoch': 0.01}
  1%|          | 70/7812 [02:21<4:09:13,  1.93s/it]  1%|          | 71/7812 [02:23<4:08:50,  1.93s/it]                                                   {'loss': 3.501, 'grad_norm': 0.5201550722122192, 'learning_rate': 0.00015106382978723404, 'epoch': 0.01}
  1%|          | 71/7812 [02:23<4:08:50,  1.93s/it]  1%|          | 72/7812 [02:25<4:09:56,  1.94s/it]                                                   {'loss': 3.6778, 'grad_norm': 0.7839365601539612, 'learning_rate': 0.00015319148936170213, 'epoch': 0.01}
  1%|          | 72/7812 [02:25<4:09:56,  1.94s/it]  1%|          | 73/7812 [02:27<4:38:09,  2.16s/it]                                                   {'loss': 3.4377, 'grad_norm': 0.5750218629837036, 'learning_rate': 0.0001553191489361702, 'epoch': 0.01}
  1%|          | 73/7812 [02:27<4:38:09,  2.16s/it]  1%|          | 74/7812 [02:29<4:25:53,  2.06s/it]                                                   {'loss': 3.5127, 'grad_norm': 0.732241153717041, 'learning_rate': 0.00015744680851063832, 'epoch': 0.01}
  1%|          | 74/7812 [02:29<4:25:53,  2.06s/it]  1%|          | 75/7812 [02:31<4:18:17,  2.00s/it]                                                   {'loss': 3.569, 'grad_norm': 0.5220596194267273, 'learning_rate': 0.0001595744680851064, 'epoch': 0.01}
  1%|          | 75/7812 [02:31<4:18:17,  2.00s/it]  1%|          | 76/7812 [02:33<4:23:29,  2.04s/it]                                                   {'loss': 3.4496, 'grad_norm': 0.6773393750190735, 'learning_rate': 0.00016170212765957449, 'epoch': 0.01}
  1%|          | 76/7812 [02:33<4:23:29,  2.04s/it]  1%|          | 77/7812 [02:35<4:12:54,  1.96s/it]                                                   {'loss': 3.6708, 'grad_norm': 0.7314962148666382, 'learning_rate': 0.00016382978723404254, 'epoch': 0.01}
  1%|          | 77/7812 [02:35<4:12:54,  1.96s/it]  1%|          | 78/7812 [02:37<4:04:08,  1.89s/it]                                                   {'loss': 3.6494, 'grad_norm': 0.739276647567749, 'learning_rate': 0.00016595744680851062, 'epoch': 0.01}
  1%|          | 78/7812 [02:37<4:04:08,  1.89s/it]  1%|          | 79/7812 [02:38<3:58:58,  1.85s/it]                                                   {'loss': 3.5787, 'grad_norm': 0.6074225306510925, 'learning_rate': 0.0001680851063829787, 'epoch': 0.01}
  1%|          | 79/7812 [02:38<3:58:58,  1.85s/it]  1%|          | 80/7812 [02:40<4:02:29,  1.88s/it]                                                   {'loss': 3.5785, 'grad_norm': 0.6338098049163818, 'learning_rate': 0.00017021276595744682, 'epoch': 0.01}
  1%|          | 80/7812 [02:40<4:02:29,  1.88s/it]  1%|          | 81/7812 [02:42<4:03:00,  1.89s/it]                                                   {'loss': 3.5196, 'grad_norm': 0.7701720595359802, 'learning_rate': 0.0001723404255319149, 'epoch': 0.01}
  1%|          | 81/7812 [02:42<4:03:00,  1.89s/it]  1%|          | 82/7812 [02:44<4:05:56,  1.91s/it]                                                   {'loss': 3.5151, 'grad_norm': 0.6808153390884399, 'learning_rate': 0.00017446808510638298, 'epoch': 0.01}
  1%|          | 82/7812 [02:44<4:05:56,  1.91s/it]  1%|          | 83/7812 [02:46<4:00:13,  1.86s/it]                                                   {'loss': 3.5808, 'grad_norm': 0.5838326811790466, 'learning_rate': 0.00017659574468085107, 'epoch': 0.01}
  1%|          | 83/7812 [02:46<4:00:13,  1.86s/it]  1%|          | 84/7812 [02:48<3:56:20,  1.83s/it]                                                   {'loss': 3.5846, 'grad_norm': 0.6333130598068237, 'learning_rate': 0.00017872340425531915, 'epoch': 0.01}
  1%|          | 84/7812 [02:48<3:56:20,  1.83s/it]  1%|          | 85/7812 [02:50<3:55:07,  1.83s/it]                                                   {'loss': 3.6825, 'grad_norm': 0.7441490888595581, 'learning_rate': 0.00018085106382978726, 'epoch': 0.01}
  1%|          | 85/7812 [02:50<3:55:07,  1.83s/it]  1%|          | 86/7812 [02:52<4:23:49,  2.05s/it]                                                   {'loss': 3.4217, 'grad_norm': 0.7035565972328186, 'learning_rate': 0.00018297872340425532, 'epoch': 0.01}
  1%|          | 86/7812 [02:52<4:23:49,  2.05s/it]  1%|          | 87/7812 [02:54<4:14:20,  1.98s/it]                                                   {'loss': 3.5895, 'grad_norm': 0.6492860913276672, 'learning_rate': 0.0001851063829787234, 'epoch': 0.01}
  1%|          | 87/7812 [02:54<4:14:20,  1.98s/it]  1%|          | 88/7812 [02:56<4:14:02,  1.97s/it]                                                   {'loss': 3.4847, 'grad_norm': 0.7384438514709473, 'learning_rate': 0.00018723404255319148, 'epoch': 0.01}
  1%|          | 88/7812 [02:56<4:14:02,  1.97s/it]  1%|          | 89/7812 [02:58<4:06:58,  1.92s/it]                                                   {'loss': 3.487, 'grad_norm': 0.6301203966140747, 'learning_rate': 0.00018936170212765957, 'epoch': 0.01}
  1%|          | 89/7812 [02:58<4:06:58,  1.92s/it]  1%|          | 90/7812 [02:59<4:00:21,  1.87s/it]                                                   {'loss': 3.6716, 'grad_norm': 0.7560556530952454, 'learning_rate': 0.00019148936170212765, 'epoch': 0.01}
  1%|          | 90/7812 [02:59<4:00:21,  1.87s/it]  1%|          | 91/7812 [03:01<4:01:24,  1.88s/it]                                                   {'loss': 3.5396, 'grad_norm': 0.693010151386261, 'learning_rate': 0.00019361702127659576, 'epoch': 0.01}
  1%|          | 91/7812 [03:01<4:01:24,  1.88s/it]  1%|          | 92/7812 [03:03<4:01:26,  1.88s/it]                                                   {'loss': 3.5208, 'grad_norm': 0.782139241695404, 'learning_rate': 0.00019574468085106384, 'epoch': 0.01}
  1%|          | 92/7812 [03:03<4:01:26,  1.88s/it]  1%|          | 93/7812 [03:05<4:02:22,  1.88s/it]                                                   {'loss': 3.5619, 'grad_norm': 0.7496026158332825, 'learning_rate': 0.00019787234042553193, 'epoch': 0.01}
  1%|          | 93/7812 [03:05<4:02:22,  1.88s/it]  1%|          | 94/7812 [03:07<4:17:02,  2.00s/it]                                                   {'loss': 3.5344, 'grad_norm': 0.9628985524177551, 'learning_rate': 0.0002, 'epoch': 0.01}
  1%|          | 94/7812 [03:07<4:17:02,  2.00s/it]  1%|          | 95/7812 [03:09<4:08:00,  1.93s/it]                                                   {'loss': 3.5175, 'grad_norm': 0.8304139375686646, 'learning_rate': 0.00020212765957446807, 'epoch': 0.01}
  1%|          | 95/7812 [03:09<4:08:00,  1.93s/it]  1%|          | 96/7812 [03:11<4:03:54,  1.90s/it]                                                   {'loss': 3.3902, 'grad_norm': 0.8151047229766846, 'learning_rate': 0.00020425531914893618, 'epoch': 0.01}
  1%|          | 96/7812 [03:11<4:03:54,  1.90s/it]  1%|          | 97/7812 [03:13<4:00:10,  1.87s/it]                                                   {'loss': 3.4815, 'grad_norm': 0.7370854020118713, 'learning_rate': 0.00020638297872340426, 'epoch': 0.01}
  1%|          | 97/7812 [03:13<4:00:10,  1.87s/it]  1%|▏         | 98/7812 [03:15<4:00:46,  1.87s/it]                                                   {'loss': 3.4879, 'grad_norm': 0.7472042441368103, 'learning_rate': 0.00020851063829787234, 'epoch': 0.01}
  1%|▏         | 98/7812 [03:15<4:00:46,  1.87s/it]  1%|▏         | 99/7812 [03:17<4:02:12,  1.88s/it]                                                   {'loss': 3.6582, 'grad_norm': 0.809266984462738, 'learning_rate': 0.00021063829787234043, 'epoch': 0.01}
  1%|▏         | 99/7812 [03:17<4:02:12,  1.88s/it]  1%|▏         | 100/7812 [03:18<3:57:49,  1.85s/it]                                                    {'loss': 3.5452, 'grad_norm': 0.692685067653656, 'learning_rate': 0.0002127659574468085, 'epoch': 0.01}
  1%|▏         | 100/7812 [03:18<3:57:49,  1.85s/it]  1%|▏         | 101/7812 [03:20<4:09:35,  1.94s/it]                                                    {'loss': 3.5425, 'grad_norm': 0.8676126599311829, 'learning_rate': 0.0002148936170212766, 'epoch': 0.01}
  1%|▏         | 101/7812 [03:20<4:09:35,  1.94s/it]  1%|▏         | 102/7812 [03:22<4:04:20,  1.90s/it]                                                    {'loss': 3.4408, 'grad_norm': 0.8819239735603333, 'learning_rate': 0.0002170212765957447, 'epoch': 0.01}
  1%|▏         | 102/7812 [03:22<4:04:20,  1.90s/it]  1%|▏         | 103/7812 [03:24<4:02:19,  1.89s/it]                                                    {'loss': 3.5157, 'grad_norm': 0.8666806817054749, 'learning_rate': 0.00021914893617021279, 'epoch': 0.01}
  1%|▏         | 103/7812 [03:24<4:02:19,  1.89s/it]  1%|▏         | 104/7812 [03:26<3:58:01,  1.85s/it]                                                    {'loss': 3.6925, 'grad_norm': 0.8335676193237305, 'learning_rate': 0.00022127659574468084, 'epoch': 0.01}
  1%|▏         | 104/7812 [03:26<3:58:01,  1.85s/it]  1%|▏         | 105/7812 [03:28<4:03:56,  1.90s/it]                                                    {'loss': 3.5192, 'grad_norm': 0.8156301379203796, 'learning_rate': 0.00022340425531914892, 'epoch': 0.01}
  1%|▏         | 105/7812 [03:28<4:03:56,  1.90s/it]  1%|▏         | 106/7812 [03:30<3:58:46,  1.86s/it]                                                    {'loss': 3.587, 'grad_norm': 0.9005736708641052, 'learning_rate': 0.000225531914893617, 'epoch': 0.01}
  1%|▏         | 106/7812 [03:30<3:58:46,  1.86s/it]  1%|▏         | 107/7812 [03:32<4:22:58,  2.05s/it]                                                    {'loss': 3.4601, 'grad_norm': 0.8236328959465027, 'learning_rate': 0.00022765957446808512, 'epoch': 0.01}
  1%|▏         | 107/7812 [03:32<4:22:58,  2.05s/it]  1%|▏         | 108/7812 [03:34<4:27:03,  2.08s/it]                                                    {'loss': 3.4694, 'grad_norm': 0.9061717987060547, 'learning_rate': 0.0002297872340425532, 'epoch': 0.01}
  1%|▏         | 108/7812 [03:34<4:27:03,  2.08s/it]  1%|▏         | 109/7812 [03:37<4:34:16,  2.14s/it]                                                    {'loss': 3.4758, 'grad_norm': 1.1101419925689697, 'learning_rate': 0.00023191489361702128, 'epoch': 0.01}
  1%|▏         | 109/7812 [03:37<4:34:16,  2.14s/it]  1%|▏         | 110/7812 [03:38<4:22:17,  2.04s/it]                                                    {'loss': 3.4614, 'grad_norm': 0.7981634140014648, 'learning_rate': 0.00023404255319148937, 'epoch': 0.01}
  1%|▏         | 110/7812 [03:38<4:22:17,  2.04s/it]  1%|▏         | 111/7812 [03:40<4:15:15,  1.99s/it]                                                    {'loss': 3.5769, 'grad_norm': 0.9415490627288818, 'learning_rate': 0.00023617021276595745, 'epoch': 0.01}
  1%|▏         | 111/7812 [03:40<4:15:15,  1.99s/it]  1%|▏         | 112/7812 [03:42<4:12:30,  1.97s/it]                                                    {'loss': 3.4571, 'grad_norm': 0.8091601729393005, 'learning_rate': 0.00023829787234042556, 'epoch': 0.01}
  1%|▏         | 112/7812 [03:42<4:12:30,  1.97s/it]  1%|▏         | 113/7812 [03:44<4:15:44,  1.99s/it]                                                    {'loss': 3.5049, 'grad_norm': 0.8497127890586853, 'learning_rate': 0.00024042553191489362, 'epoch': 0.01}
  1%|▏         | 113/7812 [03:44<4:15:44,  1.99s/it]  1%|▏         | 114/7812 [03:46<4:10:47,  1.95s/it]                                                    {'loss': 3.4289, 'grad_norm': 0.9336117506027222, 'learning_rate': 0.0002425531914893617, 'epoch': 0.01}
  1%|▏         | 114/7812 [03:46<4:10:47,  1.95s/it]  1%|▏         | 115/7812 [03:48<4:18:30,  2.02s/it]                                                    {'loss': 3.4768, 'grad_norm': 0.8640954494476318, 'learning_rate': 0.0002446808510638298, 'epoch': 0.01}
  1%|▏         | 115/7812 [03:48<4:18:30,  2.02s/it]  1%|▏         | 116/7812 [03:50<4:22:40,  2.05s/it]                                                    {'loss': 3.4748, 'grad_norm': 0.7722256779670715, 'learning_rate': 0.00024680851063829787, 'epoch': 0.01}
  1%|▏         | 116/7812 [03:50<4:22:40,  2.05s/it]  1%|▏         | 117/7812 [03:52<4:11:23,  1.96s/it]                                                    {'loss': 3.556, 'grad_norm': 0.9609205722808838, 'learning_rate': 0.000248936170212766, 'epoch': 0.01}
  1%|▏         | 117/7812 [03:52<4:11:23,  1.96s/it]  2%|▏         | 118/7812 [03:54<4:21:16,  2.04s/it]                                                    {'loss': 3.5695, 'grad_norm': 0.946794331073761, 'learning_rate': 0.00025106382978723403, 'epoch': 0.02}
  2%|▏         | 118/7812 [03:54<4:21:16,  2.04s/it]  2%|▏         | 119/7812 [03:56<4:16:35,  2.00s/it]                                                    {'loss': 3.4099, 'grad_norm': 1.1813009977340698, 'learning_rate': 0.0002531914893617021, 'epoch': 0.02}
  2%|▏         | 119/7812 [03:56<4:16:35,  2.00s/it]  2%|▏         | 120/7812 [03:58<4:21:57,  2.04s/it]                                                    {'loss': 3.5015, 'grad_norm': 1.0171167850494385, 'learning_rate': 0.0002553191489361702, 'epoch': 0.02}
  2%|▏         | 120/7812 [03:58<4:21:57,  2.04s/it]  2%|▏         | 121/7812 [04:00<4:13:00,  1.97s/it]                                                    {'loss': 3.4805, 'grad_norm': 1.0087699890136719, 'learning_rate': 0.0002574468085106383, 'epoch': 0.02}
  2%|▏         | 121/7812 [04:00<4:13:00,  1.97s/it]  2%|▏         | 122/7812 [04:02<4:06:58,  1.93s/it]                                                    {'loss': 3.4491, 'grad_norm': 0.9968607425689697, 'learning_rate': 0.00025957446808510637, 'epoch': 0.02}
  2%|▏         | 122/7812 [04:02<4:06:58,  1.93s/it]  2%|▏         | 123/7812 [04:04<4:10:58,  1.96s/it]                                                    {'loss': 3.5036, 'grad_norm': 1.0995694398880005, 'learning_rate': 0.0002617021276595745, 'epoch': 0.02}
  2%|▏         | 123/7812 [04:04<4:10:58,  1.96s/it]  2%|▏         | 124/7812 [04:06<4:03:58,  1.90s/it]                                                    {'loss': 3.458, 'grad_norm': 1.0153226852416992, 'learning_rate': 0.00026382978723404253, 'epoch': 0.02}
  2%|▏         | 124/7812 [04:06<4:03:58,  1.90s/it]  2%|▏         | 125/7812 [04:08<4:01:57,  1.89s/it]                                                    {'loss': 3.4678, 'grad_norm': 1.2013462781906128, 'learning_rate': 0.00026595744680851064, 'epoch': 0.02}
  2%|▏         | 125/7812 [04:08<4:01:57,  1.89s/it]  2%|▏         | 126/7812 [04:10<4:02:51,  1.90s/it]                                                    {'loss': 3.5425, 'grad_norm': 0.9774401783943176, 'learning_rate': 0.00026808510638297875, 'epoch': 0.02}
  2%|▏         | 126/7812 [04:10<4:02:51,  1.90s/it]  2%|▏         | 127/7812 [04:12<4:06:37,  1.93s/it]                                                    {'loss': 3.4091, 'grad_norm': 1.3498914241790771, 'learning_rate': 0.0002702127659574468, 'epoch': 0.02}
  2%|▏         | 127/7812 [04:12<4:06:37,  1.93s/it]  2%|▏         | 128/7812 [04:17<6:17:27,  2.95s/it]                                                    {'loss': 3.3315, 'grad_norm': 0.9875137805938721, 'learning_rate': 0.0002723404255319149, 'epoch': 0.02}
  2%|▏         | 128/7812 [04:17<6:17:27,  2.95s/it]  2%|▏         | 129/7812 [04:19<5:36:31,  2.63s/it]                                                    {'loss': 3.3972, 'grad_norm': 1.436453938484192, 'learning_rate': 0.000274468085106383, 'epoch': 0.02}
  2%|▏         | 129/7812 [04:19<5:36:31,  2.63s/it]  2%|▏         | 130/7812 [04:21<5:15:35,  2.46s/it]                                                    {'loss': 3.4396, 'grad_norm': 1.0148545503616333, 'learning_rate': 0.0002765957446808511, 'epoch': 0.02}
  2%|▏         | 130/7812 [04:21<5:15:35,  2.46s/it]  2%|▏         | 131/7812 [04:23<4:48:52,  2.26s/it]                                                    {'loss': 3.5882, 'grad_norm': 1.284839391708374, 'learning_rate': 0.00027872340425531914, 'epoch': 0.02}
  2%|▏         | 131/7812 [04:23<4:48:52,  2.26s/it]  2%|▏         | 132/7812 [04:25<4:32:27,  2.13s/it]                                                    {'loss': 3.357, 'grad_norm': 1.3334928750991821, 'learning_rate': 0.00028085106382978725, 'epoch': 0.02}
  2%|▏         | 132/7812 [04:25<4:32:27,  2.13s/it]  2%|▏         | 133/7812 [04:27<4:28:42,  2.10s/it]                                                    {'loss': 3.5183, 'grad_norm': 1.4675250053405762, 'learning_rate': 0.00028297872340425536, 'epoch': 0.02}
  2%|▏         | 133/7812 [04:27<4:28:42,  2.10s/it]  2%|▏         | 134/7812 [04:28<4:17:06,  2.01s/it]                                                    {'loss': 3.5414, 'grad_norm': 1.7621830701828003, 'learning_rate': 0.0002851063829787234, 'epoch': 0.02}
  2%|▏         | 134/7812 [04:28<4:17:06,  2.01s/it]  2%|▏         | 135/7812 [04:30<4:07:28,  1.93s/it]                                                    {'loss': 3.4932, 'grad_norm': 1.3934476375579834, 'learning_rate': 0.00028723404255319153, 'epoch': 0.02}
  2%|▏         | 135/7812 [04:30<4:07:28,  1.93s/it]  2%|▏         | 136/7812 [04:32<4:00:38,  1.88s/it]                                                    {'loss': 3.3924, 'grad_norm': 1.8584741353988647, 'learning_rate': 0.00028936170212765953, 'epoch': 0.02}
  2%|▏         | 136/7812 [04:32<4:00:38,  1.88s/it]  2%|▏         | 137/7812 [04:34<4:14:38,  1.99s/it]                                                    {'loss': 3.3782, 'grad_norm': 1.217151165008545, 'learning_rate': 0.00029148936170212764, 'epoch': 0.02}
  2%|▏         | 137/7812 [04:34<4:14:38,  1.99s/it]  2%|▏         | 138/7812 [04:36<4:11:52,  1.97s/it]                                                    {'loss': 3.4083, 'grad_norm': 1.8223450183868408, 'learning_rate': 0.00029361702127659575, 'epoch': 0.02}
  2%|▏         | 138/7812 [04:36<4:11:52,  1.97s/it]  2%|▏         | 139/7812 [04:38<4:16:14,  2.00s/it]                                                    {'loss': 3.4567, 'grad_norm': 1.5175632238388062, 'learning_rate': 0.0002957446808510638, 'epoch': 0.02}
  2%|▏         | 139/7812 [04:38<4:16:14,  2.00s/it]  2%|▏         | 140/7812 [04:40<4:20:00,  2.03s/it]                                                    {'loss': 3.4065, 'grad_norm': 1.4363634586334229, 'learning_rate': 0.0002978723404255319, 'epoch': 0.02}
  2%|▏         | 140/7812 [04:40<4:20:00,  2.03s/it]  2%|▏         | 141/7812 [04:42<4:08:34,  1.94s/it]                                                    {'loss': 3.5318, 'grad_norm': 1.28728187084198, 'learning_rate': 0.0003, 'epoch': 0.02}
  2%|▏         | 141/7812 [04:42<4:08:34,  1.94s/it]  2%|▏         | 142/7812 [04:45<4:35:44,  2.16s/it]                                                    {'loss': 3.3725, 'grad_norm': 1.2908275127410889, 'learning_rate': 0.0003021276595744681, 'epoch': 0.02}
  2%|▏         | 142/7812 [04:45<4:35:44,  2.16s/it]  2%|▏         | 143/7812 [04:46<4:24:00,  2.07s/it]                                                    {'loss': 3.3885, 'grad_norm': 1.201578974723816, 'learning_rate': 0.0003042553191489362, 'epoch': 0.02}
  2%|▏         | 143/7812 [04:46<4:24:00,  2.07s/it]  2%|▏         | 144/7812 [04:48<4:14:49,  1.99s/it]                                                    {'loss': 3.4067, 'grad_norm': 1.8043428659439087, 'learning_rate': 0.00030638297872340425, 'epoch': 0.02}
  2%|▏         | 144/7812 [04:48<4:14:49,  1.99s/it]  2%|▏         | 145/7812 [04:50<4:18:27,  2.02s/it]                                                    {'loss': 3.3921, 'grad_norm': 1.5019831657409668, 'learning_rate': 0.00030851063829787236, 'epoch': 0.02}
  2%|▏         | 145/7812 [04:50<4:18:27,  2.02s/it]  2%|▏         | 146/7812 [04:53<4:32:49,  2.14s/it]                                                    {'loss': 3.3572, 'grad_norm': 1.3367712497711182, 'learning_rate': 0.0003106382978723404, 'epoch': 0.02}
  2%|▏         | 146/7812 [04:53<4:32:49,  2.14s/it]  2%|▏         | 147/7812 [04:55<4:30:12,  2.12s/it]                                                    {'loss': 3.4076, 'grad_norm': 2.0830485820770264, 'learning_rate': 0.0003127659574468085, 'epoch': 0.02}
  2%|▏         | 147/7812 [04:55<4:30:12,  2.12s/it]  2%|▏         | 148/7812 [04:57<4:43:38,  2.22s/it]                                                    {'loss': 3.372, 'grad_norm': 1.0908197164535522, 'learning_rate': 0.00031489361702127664, 'epoch': 0.02}
  2%|▏         | 148/7812 [04:57<4:43:38,  2.22s/it]  2%|▏         | 149/7812 [04:59<4:26:43,  2.09s/it]                                                    {'loss': 3.4702, 'grad_norm': 1.9203399419784546, 'learning_rate': 0.0003170212765957447, 'epoch': 0.02}
  2%|▏         | 149/7812 [04:59<4:26:43,  2.09s/it]  2%|▏         | 150/7812 [05:01<4:16:59,  2.01s/it]                                                    {'loss': 3.3441, 'grad_norm': 1.852338433265686, 'learning_rate': 0.0003191489361702128, 'epoch': 0.02}
  2%|▏         | 150/7812 [05:01<4:16:59,  2.01s/it]  2%|▏         | 151/7812 [05:03<4:08:19,  1.94s/it]                                                    {'loss': 3.5429, 'grad_norm': 1.6959632635116577, 'learning_rate': 0.00032127659574468086, 'epoch': 0.02}
  2%|▏         | 151/7812 [05:03<4:08:19,  1.94s/it]  2%|▏         | 152/7812 [05:05<4:07:09,  1.94s/it]                                                    {'loss': 3.3779, 'grad_norm': 1.6193193197250366, 'learning_rate': 0.00032340425531914897, 'epoch': 0.02}
  2%|▏         | 152/7812 [05:05<4:07:09,  1.94s/it]  2%|▏         | 153/7812 [05:07<4:09:28,  1.95s/it]                                                    {'loss': 3.3569, 'grad_norm': 1.1895495653152466, 'learning_rate': 0.0003255319148936171, 'epoch': 0.02}
  2%|▏         | 153/7812 [05:07<4:09:28,  1.95s/it]  2%|▏         | 154/7812 [05:09<4:12:36,  1.98s/it]                                                    {'loss': 3.4474, 'grad_norm': 1.3613827228546143, 'learning_rate': 0.0003276595744680851, 'epoch': 0.02}
  2%|▏         | 154/7812 [05:09<4:12:36,  1.98s/it]  2%|▏         | 155/7812 [05:10<4:03:53,  1.91s/it]                                                    {'loss': 3.43, 'grad_norm': 1.17707359790802, 'learning_rate': 0.0003297872340425532, 'epoch': 0.02}
  2%|▏         | 155/7812 [05:10<4:03:53,  1.91s/it]  2%|▏         | 156/7812 [05:12<3:59:58,  1.88s/it]                                                    {'loss': 3.4136, 'grad_norm': 1.4800997972488403, 'learning_rate': 0.00033191489361702125, 'epoch': 0.02}
  2%|▏         | 156/7812 [05:12<3:59:58,  1.88s/it]  2%|▏         | 157/7812 [05:14<3:58:46,  1.87s/it]                                                    {'loss': 3.44, 'grad_norm': 1.794135570526123, 'learning_rate': 0.00033404255319148936, 'epoch': 0.02}
  2%|▏         | 157/7812 [05:14<3:58:46,  1.87s/it]  2%|▏         | 158/7812 [05:16<4:04:45,  1.92s/it]                                                    {'loss': 3.407, 'grad_norm': 1.5031377077102661, 'learning_rate': 0.0003361702127659574, 'epoch': 0.02}
  2%|▏         | 158/7812 [05:16<4:04:45,  1.92s/it]  2%|▏         | 159/7812 [05:18<3:56:46,  1.86s/it]                                                    {'loss': 3.3308, 'grad_norm': 1.6089707612991333, 'learning_rate': 0.0003382978723404255, 'epoch': 0.02}
  2%|▏         | 159/7812 [05:18<3:56:46,  1.86s/it]  2%|▏         | 160/7812 [05:20<3:59:18,  1.88s/it]                                                    {'loss': 3.3354, 'grad_norm': 1.4220521450042725, 'learning_rate': 0.00034042553191489364, 'epoch': 0.02}
  2%|▏         | 160/7812 [05:20<3:59:18,  1.88s/it]  2%|▏         | 161/7812 [05:22<3:58:25,  1.87s/it]                                                    {'loss': 3.457, 'grad_norm': 1.3926228284835815, 'learning_rate': 0.0003425531914893617, 'epoch': 0.02}
  2%|▏         | 161/7812 [05:22<3:58:25,  1.87s/it]  2%|▏         | 162/7812 [05:24<4:07:11,  1.94s/it]                                                    {'loss': 3.352, 'grad_norm': 1.1408625841140747, 'learning_rate': 0.0003446808510638298, 'epoch': 0.02}
  2%|▏         | 162/7812 [05:24<4:07:11,  1.94s/it]  2%|▏         | 163/7812 [05:25<4:00:51,  1.89s/it]                                                    {'loss': 3.4529, 'grad_norm': 1.065470814704895, 'learning_rate': 0.00034680851063829786, 'epoch': 0.02}
  2%|▏         | 163/7812 [05:25<4:00:51,  1.89s/it]  2%|▏         | 164/7812 [05:27<4:01:41,  1.90s/it]                                                    {'loss': 3.3854, 'grad_norm': 1.3458442687988281, 'learning_rate': 0.00034893617021276597, 'epoch': 0.02}
  2%|▏         | 164/7812 [05:27<4:01:41,  1.90s/it]  2%|▏         | 165/7812 [05:30<4:11:42,  1.98s/it]                                                    {'loss': 3.2744, 'grad_norm': 1.5303637981414795, 'learning_rate': 0.0003510638297872341, 'epoch': 0.02}
  2%|▏         | 165/7812 [05:30<4:11:42,  1.98s/it]  2%|▏         | 166/7812 [05:32<4:16:13,  2.01s/it]                                                    {'loss': 3.4419, 'grad_norm': 1.2227894067764282, 'learning_rate': 0.00035319148936170213, 'epoch': 0.02}
  2%|▏         | 166/7812 [05:32<4:16:13,  2.01s/it]  2%|▏         | 167/7812 [05:33<4:04:16,  1.92s/it]                                                    {'loss': 3.4779, 'grad_norm': 1.444299340248108, 'learning_rate': 0.00035531914893617025, 'epoch': 0.02}
  2%|▏         | 167/7812 [05:33<4:04:16,  1.92s/it]  2%|▏         | 168/7812 [05:35<4:03:29,  1.91s/it]                                                    {'loss': 3.3495, 'grad_norm': 1.4298349618911743, 'learning_rate': 0.0003574468085106383, 'epoch': 0.02}
  2%|▏         | 168/7812 [05:35<4:03:29,  1.91s/it]  2%|▏         | 169/7812 [05:37<3:58:05,  1.87s/it]                                                    {'loss': 3.3062, 'grad_norm': 1.5104598999023438, 'learning_rate': 0.0003595744680851064, 'epoch': 0.02}
  2%|▏         | 169/7812 [05:37<3:58:05,  1.87s/it]  2%|▏         | 170/7812 [05:39<4:10:44,  1.97s/it]                                                    {'loss': 3.4297, 'grad_norm': 1.5708773136138916, 'learning_rate': 0.0003617021276595745, 'epoch': 0.02}
  2%|▏         | 170/7812 [05:39<4:10:44,  1.97s/it]  2%|▏         | 171/7812 [05:41<4:10:17,  1.97s/it]                                                    {'loss': 3.4128, 'grad_norm': 1.4599655866622925, 'learning_rate': 0.0003638297872340426, 'epoch': 0.02}
  2%|▏         | 171/7812 [05:41<4:10:17,  1.97s/it]  2%|▏         | 172/7812 [05:43<4:02:38,  1.91s/it]                                                    {'loss': 3.3841, 'grad_norm': 1.4755200147628784, 'learning_rate': 0.00036595744680851063, 'epoch': 0.02}
  2%|▏         | 172/7812 [05:43<4:02:38,  1.91s/it]  2%|▏         | 173/7812 [05:45<3:58:49,  1.88s/it]                                                    {'loss': 3.4646, 'grad_norm': 1.4937717914581299, 'learning_rate': 0.0003680851063829787, 'epoch': 0.02}
  2%|▏         | 173/7812 [05:45<3:58:49,  1.88s/it]  2%|▏         | 174/7812 [05:47<3:58:49,  1.88s/it]                                                    {'loss': 3.218, 'grad_norm': 1.4660884141921997, 'learning_rate': 0.0003702127659574468, 'epoch': 0.02}
  2%|▏         | 174/7812 [05:47<3:58:49,  1.88s/it]  2%|▏         | 175/7812 [05:49<4:29:13,  2.12s/it]                                                    {'loss': 3.2536, 'grad_norm': 1.4458953142166138, 'learning_rate': 0.0003723404255319149, 'epoch': 0.02}
  2%|▏         | 175/7812 [05:49<4:29:13,  2.12s/it]  2%|▏         | 176/7812 [05:51<4:17:06,  2.02s/it]                                                    {'loss': 3.457, 'grad_norm': 1.3325176239013672, 'learning_rate': 0.00037446808510638297, 'epoch': 0.02}
  2%|▏         | 176/7812 [05:51<4:17:06,  2.02s/it]  2%|▏         | 177/7812 [05:53<4:07:30,  1.95s/it]                                                    {'loss': 3.4387, 'grad_norm': 1.4492666721343994, 'learning_rate': 0.0003765957446808511, 'epoch': 0.02}
  2%|▏         | 177/7812 [05:53<4:07:30,  1.95s/it]  2%|▏         | 178/7812 [05:55<4:09:03,  1.96s/it]                                                    {'loss': 3.3315, 'grad_norm': 1.2078725099563599, 'learning_rate': 0.00037872340425531913, 'epoch': 0.02}
  2%|▏         | 178/7812 [05:55<4:09:03,  1.96s/it]  2%|▏         | 179/7812 [05:57<4:06:46,  1.94s/it]                                                    {'loss': 3.3102, 'grad_norm': 1.3192780017852783, 'learning_rate': 0.00038085106382978724, 'epoch': 0.02}
  2%|▏         | 179/7812 [05:57<4:06:46,  1.94s/it]  2%|▏         | 180/7812 [05:59<4:14:20,  2.00s/it]                                                    {'loss': 3.2172, 'grad_norm': 1.0254971981048584, 'learning_rate': 0.0003829787234042553, 'epoch': 0.02}
  2%|▏         | 180/7812 [05:59<4:14:20,  2.00s/it]  2%|▏         | 181/7812 [06:01<4:12:23,  1.98s/it]                                                    {'loss': 3.3432, 'grad_norm': 1.3359670639038086, 'learning_rate': 0.0003851063829787234, 'epoch': 0.02}
  2%|▏         | 181/7812 [06:01<4:12:23,  1.98s/it]  2%|▏         | 182/7812 [06:03<4:28:02,  2.11s/it]                                                    {'loss': 3.2409, 'grad_norm': 1.1891933679580688, 'learning_rate': 0.0003872340425531915, 'epoch': 0.02}
  2%|▏         | 182/7812 [06:03<4:28:02,  2.11s/it]  2%|▏         | 183/7812 [06:05<4:32:54,  2.15s/it]                                                    {'loss': 3.234, 'grad_norm': 1.117810606956482, 'learning_rate': 0.0003893617021276596, 'epoch': 0.02}
  2%|▏         | 183/7812 [06:05<4:32:54,  2.15s/it]  2%|▏         | 184/7812 [06:07<4:18:54,  2.04s/it]                                                    {'loss': 3.3499, 'grad_norm': 1.3749775886535645, 'learning_rate': 0.0003914893617021277, 'epoch': 0.02}
  2%|▏         | 184/7812 [06:07<4:18:54,  2.04s/it]  2%|▏         | 185/7812 [06:09<4:21:29,  2.06s/it]                                                    {'loss': 3.256, 'grad_norm': 1.3354586362838745, 'learning_rate': 0.00039361702127659574, 'epoch': 0.02}
  2%|▏         | 185/7812 [06:09<4:21:29,  2.06s/it]  2%|▏         | 186/7812 [06:11<4:10:40,  1.97s/it]                                                    {'loss': 3.4012, 'grad_norm': 1.1829216480255127, 'learning_rate': 0.00039574468085106385, 'epoch': 0.02}
  2%|▏         | 186/7812 [06:11<4:10:40,  1.97s/it]  2%|▏         | 187/7812 [06:13<4:15:53,  2.01s/it]                                                    {'loss': 3.3618, 'grad_norm': 1.387764811515808, 'learning_rate': 0.00039787234042553196, 'epoch': 0.02}
  2%|▏         | 187/7812 [06:13<4:15:53,  2.01s/it]  2%|▏         | 188/7812 [06:15<4:10:41,  1.97s/it]                                                    {'loss': 3.3334, 'grad_norm': 1.055184006690979, 'learning_rate': 0.0004, 'epoch': 0.02}
  2%|▏         | 188/7812 [06:15<4:10:41,  1.97s/it]  2%|▏         | 189/7812 [06:17<4:07:40,  1.95s/it]                                                    {'loss': 3.331, 'grad_norm': 1.2572908401489258, 'learning_rate': 0.00040212765957446813, 'epoch': 0.02}
  2%|▏         | 189/7812 [06:17<4:07:40,  1.95s/it]  2%|▏         | 190/7812 [06:19<4:10:44,  1.97s/it]                                                    {'loss': 3.3301, 'grad_norm': 1.013497233390808, 'learning_rate': 0.00040425531914893613, 'epoch': 0.02}
  2%|▏         | 190/7812 [06:19<4:10:44,  1.97s/it]  2%|▏         | 191/7812 [06:21<4:17:08,  2.02s/it]                                                    {'loss': 3.1272, 'grad_norm': 0.9797726273536682, 'learning_rate': 0.00040638297872340424, 'epoch': 0.02}
  2%|▏         | 191/7812 [06:21<4:17:08,  2.02s/it]  2%|▏         | 192/7812 [06:23<4:07:15,  1.95s/it]                                                    {'loss': 3.3258, 'grad_norm': 1.1164186000823975, 'learning_rate': 0.00040851063829787235, 'epoch': 0.02}
  2%|▏         | 192/7812 [06:23<4:07:15,  1.95s/it]  2%|▏         | 193/7812 [06:25<4:09:36,  1.97s/it]                                                    {'loss': 3.2416, 'grad_norm': 1.154945731163025, 'learning_rate': 0.0004106382978723404, 'epoch': 0.02}
  2%|▏         | 193/7812 [06:25<4:09:36,  1.97s/it]  2%|▏         | 194/7812 [06:27<4:24:51,  2.09s/it]                                                    {'loss': 3.2457, 'grad_norm': 1.2100725173950195, 'learning_rate': 0.0004127659574468085, 'epoch': 0.02}
  2%|▏         | 194/7812 [06:27<4:24:51,  2.09s/it]  2%|▏         | 195/7812 [06:29<4:21:17,  2.06s/it]                                                    {'loss': 3.2668, 'grad_norm': 1.2533538341522217, 'learning_rate': 0.0004148936170212766, 'epoch': 0.02}
  2%|▏         | 195/7812 [06:29<4:21:17,  2.06s/it]  3%|▎         | 196/7812 [06:31<4:16:36,  2.02s/it]                                                    {'loss': 3.3015, 'grad_norm': 1.1229835748672485, 'learning_rate': 0.0004170212765957447, 'epoch': 0.03}
  3%|▎         | 196/7812 [06:31<4:16:36,  2.02s/it]  3%|▎         | 197/7812 [06:33<4:10:06,  1.97s/it]                                                    {'loss': 3.2318, 'grad_norm': 1.0951884984970093, 'learning_rate': 0.0004191489361702128, 'epoch': 0.03}
  3%|▎         | 197/7812 [06:33<4:10:06,  1.97s/it]  3%|▎         | 198/7812 [06:35<4:10:06,  1.97s/it]                                                    {'loss': 3.2956, 'grad_norm': 1.215875267982483, 'learning_rate': 0.00042127659574468085, 'epoch': 0.03}
  3%|▎         | 198/7812 [06:35<4:10:06,  1.97s/it]  3%|▎         | 199/7812 [06:37<4:08:04,  1.96s/it]                                                    {'loss': 3.3989, 'grad_norm': 1.014697790145874, 'learning_rate': 0.00042340425531914896, 'epoch': 0.03}
  3%|▎         | 199/7812 [06:37<4:08:04,  1.96s/it]  3%|▎         | 200/7812 [06:39<4:16:56,  2.03s/it]                                                    {'loss': 3.1462, 'grad_norm': 1.4201910495758057, 'learning_rate': 0.000425531914893617, 'epoch': 0.03}
  3%|▎         | 200/7812 [06:39<4:16:56,  2.03s/it]  3%|▎         | 201/7812 [06:41<4:14:12,  2.00s/it]                                                    {'loss': 3.238, 'grad_norm': 1.1130503416061401, 'learning_rate': 0.00042765957446808513, 'epoch': 0.03}
  3%|▎         | 201/7812 [06:41<4:14:12,  2.00s/it]  3%|▎         | 202/7812 [06:43<4:18:39,  2.04s/it]                                                    {'loss': 3.2639, 'grad_norm': 1.4079804420471191, 'learning_rate': 0.0004297872340425532, 'epoch': 0.03}
  3%|▎         | 202/7812 [06:43<4:18:39,  2.04s/it]  3%|▎         | 203/7812 [06:45<4:14:24,  2.01s/it]                                                    {'loss': 3.2839, 'grad_norm': 1.2243263721466064, 'learning_rate': 0.0004319148936170213, 'epoch': 0.03}
  3%|▎         | 203/7812 [06:45<4:14:24,  2.01s/it]  3%|▎         | 204/7812 [06:47<4:13:08,  2.00s/it]                                                    {'loss': 3.3696, 'grad_norm': 1.4195752143859863, 'learning_rate': 0.0004340425531914894, 'epoch': 0.03}
  3%|▎         | 204/7812 [06:47<4:13:08,  2.00s/it]  3%|▎         | 205/7812 [06:49<4:04:06,  1.93s/it]                                                    {'loss': 3.3222, 'grad_norm': 1.1512045860290527, 'learning_rate': 0.00043617021276595746, 'epoch': 0.03}
  3%|▎         | 205/7812 [06:49<4:04:06,  1.93s/it]  3%|▎         | 206/7812 [06:51<4:00:41,  1.90s/it]                                                    {'loss': 3.2526, 'grad_norm': 1.219497561454773, 'learning_rate': 0.00043829787234042557, 'epoch': 0.03}
  3%|▎         | 206/7812 [06:51<4:00:41,  1.90s/it]  3%|▎         | 207/7812 [06:53<4:01:00,  1.90s/it]                                                    {'loss': 3.1765, 'grad_norm': 1.2279331684112549, 'learning_rate': 0.0004404255319148936, 'epoch': 0.03}
  3%|▎         | 207/7812 [06:53<4:01:00,  1.90s/it]  3%|▎         | 208/7812 [06:55<4:03:36,  1.92s/it]                                                    {'loss': 3.2978, 'grad_norm': 1.0116832256317139, 'learning_rate': 0.0004425531914893617, 'epoch': 0.03}
  3%|▎         | 208/7812 [06:55<4:03:36,  1.92s/it]  3%|▎         | 209/7812 [06:56<3:59:19,  1.89s/it]                                                    {'loss': 3.3537, 'grad_norm': 1.165724277496338, 'learning_rate': 0.0004446808510638298, 'epoch': 0.03}
  3%|▎         | 209/7812 [06:56<3:59:19,  1.89s/it]  3%|▎         | 210/7812 [06:58<4:03:20,  1.92s/it]                                                    {'loss': 3.2034, 'grad_norm': 1.1833584308624268, 'learning_rate': 0.00044680851063829785, 'epoch': 0.03}
  3%|▎         | 210/7812 [06:58<4:03:20,  1.92s/it]  3%|▎         | 211/7812 [07:00<4:05:16,  1.94s/it]                                                    {'loss': 3.2398, 'grad_norm': 1.2777042388916016, 'learning_rate': 0.00044893617021276596, 'epoch': 0.03}
  3%|▎         | 211/7812 [07:00<4:05:16,  1.94s/it]  3%|▎         | 212/7812 [07:02<4:00:56,  1.90s/it]                                                    {'loss': 3.2204, 'grad_norm': 1.2012625932693481, 'learning_rate': 0.000451063829787234, 'epoch': 0.03}
  3%|▎         | 212/7812 [07:02<4:00:56,  1.90s/it]  3%|▎         | 213/7812 [07:04<4:08:11,  1.96s/it]                                                    {'loss': 3.1607, 'grad_norm': 1.1408525705337524, 'learning_rate': 0.0004531914893617021, 'epoch': 0.03}
  3%|▎         | 213/7812 [07:04<4:08:11,  1.96s/it]  3%|▎         | 214/7812 [07:07<4:38:04,  2.20s/it]                                                    {'loss': 3.1165, 'grad_norm': 1.007991075515747, 'learning_rate': 0.00045531914893617024, 'epoch': 0.03}
  3%|▎         | 214/7812 [07:07<4:38:04,  2.20s/it]  3%|▎         | 215/7812 [07:09<4:26:00,  2.10s/it]                                                    {'loss': 3.1894, 'grad_norm': 1.1135332584381104, 'learning_rate': 0.0004574468085106383, 'epoch': 0.03}
  3%|▎         | 215/7812 [07:09<4:26:00,  2.10s/it]  3%|▎         | 216/7812 [07:11<4:17:36,  2.03s/it]                                                    {'loss': 3.2008, 'grad_norm': 1.001686692237854, 'learning_rate': 0.0004595744680851064, 'epoch': 0.03}
  3%|▎         | 216/7812 [07:11<4:17:36,  2.03s/it]  3%|▎         | 217/7812 [07:13<4:20:00,  2.05s/it]                                                    {'loss': 3.253, 'grad_norm': 1.2182964086532593, 'learning_rate': 0.00046170212765957446, 'epoch': 0.03}
  3%|▎         | 217/7812 [07:13<4:20:00,  2.05s/it]  3%|▎         | 218/7812 [07:15<4:13:16,  2.00s/it]                                                    {'loss': 3.3021, 'grad_norm': 1.0593204498291016, 'learning_rate': 0.00046382978723404257, 'epoch': 0.03}
  3%|▎         | 218/7812 [07:15<4:13:16,  2.00s/it]  3%|▎         | 219/7812 [07:17<4:11:34,  1.99s/it]                                                    {'loss': 3.2639, 'grad_norm': 1.1461213827133179, 'learning_rate': 0.0004659574468085107, 'epoch': 0.03}
  3%|▎         | 219/7812 [07:17<4:11:34,  1.99s/it]  3%|▎         | 220/7812 [07:19<4:09:52,  1.97s/it]                                                    {'loss': 3.261, 'grad_norm': 1.1392040252685547, 'learning_rate': 0.00046808510638297874, 'epoch': 0.03}
  3%|▎         | 220/7812 [07:19<4:09:52,  1.97s/it]  3%|▎         | 221/7812 [07:21<4:11:43,  1.99s/it]                                                    {'loss': 3.1386, 'grad_norm': 1.2377912998199463, 'learning_rate': 0.00047021276595744685, 'epoch': 0.03}
  3%|▎         | 221/7812 [07:21<4:11:43,  1.99s/it]  3%|▎         | 222/7812 [07:22<4:02:47,  1.92s/it]                                                    {'loss': 3.3433, 'grad_norm': 1.1869338750839233, 'learning_rate': 0.0004723404255319149, 'epoch': 0.03}
  3%|▎         | 222/7812 [07:22<4:02:47,  1.92s/it]  3%|▎         | 223/7812 [07:25<4:07:20,  1.96s/it]                                                    {'loss': 3.3183, 'grad_norm': 1.1554677486419678, 'learning_rate': 0.000474468085106383, 'epoch': 0.03}
  3%|▎         | 223/7812 [07:25<4:07:20,  1.96s/it]  3%|▎         | 224/7812 [07:27<4:12:55,  2.00s/it]                                                    {'loss': 3.1716, 'grad_norm': 1.0997644662857056, 'learning_rate': 0.0004765957446808511, 'epoch': 0.03}
  3%|▎         | 224/7812 [07:27<4:12:55,  2.00s/it]  3%|▎         | 225/7812 [07:29<4:13:50,  2.01s/it]                                                    {'loss': 3.1919, 'grad_norm': 1.096577525138855, 'learning_rate': 0.0004787234042553192, 'epoch': 0.03}
  3%|▎         | 225/7812 [07:29<4:13:50,  2.01s/it]  3%|▎         | 226/7812 [07:31<4:10:21,  1.98s/it]                                                    {'loss': 3.3383, 'grad_norm': 1.1686686277389526, 'learning_rate': 0.00048085106382978723, 'epoch': 0.03}
  3%|▎         | 226/7812 [07:31<4:10:21,  1.98s/it]  3%|▎         | 227/7812 [07:32<4:04:28,  1.93s/it]                                                    {'loss': 3.1913, 'grad_norm': 1.0288487672805786, 'learning_rate': 0.0004829787234042553, 'epoch': 0.03}
  3%|▎         | 227/7812 [07:32<4:04:28,  1.93s/it]  3%|▎         | 228/7812 [07:34<4:09:12,  1.97s/it]                                                    {'loss': 3.2987, 'grad_norm': 1.2240666151046753, 'learning_rate': 0.0004851063829787234, 'epoch': 0.03}
  3%|▎         | 228/7812 [07:34<4:09:12,  1.97s/it]  3%|▎         | 229/7812 [07:36<4:03:08,  1.92s/it]                                                    {'loss': 3.3379, 'grad_norm': 1.188189148902893, 'learning_rate': 0.00048723404255319146, 'epoch': 0.03}
  3%|▎         | 229/7812 [07:36<4:03:08,  1.92s/it]  3%|▎         | 230/7812 [07:38<3:56:49,  1.87s/it]                                                    {'loss': 3.3249, 'grad_norm': 1.1941189765930176, 'learning_rate': 0.0004893617021276596, 'epoch': 0.03}
  3%|▎         | 230/7812 [07:38<3:56:49,  1.87s/it]  3%|▎         | 231/7812 [07:40<3:57:05,  1.88s/it]                                                    {'loss': 3.2681, 'grad_norm': 1.2223196029663086, 'learning_rate': 0.0004914893617021277, 'epoch': 0.03}
  3%|▎         | 231/7812 [07:40<3:57:05,  1.88s/it]  3%|▎         | 232/7812 [07:42<3:58:17,  1.89s/it]                                                    {'loss': 3.2866, 'grad_norm': 1.1747846603393555, 'learning_rate': 0.0004936170212765957, 'epoch': 0.03}
  3%|▎         | 232/7812 [07:42<3:58:17,  1.89s/it]  3%|▎         | 233/7812 [07:44<3:55:35,  1.87s/it]                                                    {'loss': 3.236, 'grad_norm': 1.2709516286849976, 'learning_rate': 0.0004957446808510638, 'epoch': 0.03}
  3%|▎         | 233/7812 [07:44<3:55:35,  1.87s/it]  3%|▎         | 234/7812 [07:46<3:59:02,  1.89s/it]                                                    {'loss': 3.3041, 'grad_norm': 1.180548906326294, 'learning_rate': 0.000497872340425532, 'epoch': 0.03}
  3%|▎         | 234/7812 [07:46<3:59:02,  1.89s/it]  3%|▎         | 235/7812 [07:47<3:57:50,  1.88s/it]                                                    {'loss': 3.2557, 'grad_norm': 1.1408777236938477, 'learning_rate': 0.0005, 'epoch': 0.03}
  3%|▎         | 235/7812 [07:47<3:57:50,  1.88s/it]  3%|▎         | 236/7812 [07:49<3:55:25,  1.86s/it]                                                    {'loss': 3.3661, 'grad_norm': 1.3146344423294067, 'learning_rate': 0.0004999999785110509, 'epoch': 0.03}
  3%|▎         | 236/7812 [07:49<3:55:25,  1.86s/it]  3%|▎         | 237/7812 [07:51<3:55:05,  1.86s/it]                                                    {'loss': 3.2653, 'grad_norm': 1.121633529663086, 'learning_rate': 0.0004999999140442072, 'epoch': 0.03}
  3%|▎         | 237/7812 [07:51<3:55:05,  1.86s/it]  3%|▎         | 238/7812 [07:53<3:58:51,  1.89s/it]                                                    {'loss': 3.3521, 'grad_norm': 1.2996350526809692, 'learning_rate': 0.0004999998065994801, 'epoch': 0.03}
  3%|▎         | 238/7812 [07:53<3:58:51,  1.89s/it]  3%|▎         | 239/7812 [07:55<4:12:52,  2.00s/it]                                                    {'loss': 3.2405, 'grad_norm': 1.0878474712371826, 'learning_rate': 0.0004999996561768879, 'epoch': 0.03}
  3%|▎         | 239/7812 [07:55<4:12:52,  2.00s/it]  3%|▎         | 240/7812 [07:58<4:18:49,  2.05s/it]                                                    {'loss': 3.1287, 'grad_norm': 1.0766221284866333, 'learning_rate': 0.0004999994627764566, 'epoch': 0.03}
  3%|▎         | 240/7812 [07:58<4:18:49,  2.05s/it]  3%|▎         | 241/7812 [08:00<4:18:09,  2.05s/it]                                                    {'loss': 3.3115, 'grad_norm': 1.2857482433319092, 'learning_rate': 0.0004999992263982194, 'epoch': 0.03}
  3%|▎         | 241/7812 [08:00<4:18:09,  2.05s/it]  3%|▎         | 242/7812 [08:02<4:23:08,  2.09s/it]                                                    {'loss': 3.2352, 'grad_norm': 1.0495829582214355, 'learning_rate': 0.000499998947042217, 'epoch': 0.03}
  3%|▎         | 242/7812 [08:02<4:23:08,  2.09s/it]  3%|▎         | 243/7812 [08:04<4:11:49,  2.00s/it]                                                    {'loss': 3.1537, 'grad_norm': 1.1069058179855347, 'learning_rate': 0.0004999986247084974, 'epoch': 0.03}
  3%|▎         | 243/7812 [08:04<4:11:49,  2.00s/it]  3%|▎         | 244/7812 [08:05<4:10:43,  1.99s/it]                                                    {'loss': 3.1839, 'grad_norm': 1.11911141872406, 'learning_rate': 0.0004999982593971157, 'epoch': 0.03}
  3%|▎         | 244/7812 [08:05<4:10:43,  1.99s/it]  3%|▎         | 245/7812 [08:08<4:14:47,  2.02s/it]                                                    {'loss': 3.2195, 'grad_norm': 1.0577739477157593, 'learning_rate': 0.0004999978511081353, 'epoch': 0.03}
  3%|▎         | 245/7812 [08:08<4:14:47,  2.02s/it]  3%|▎         | 246/7812 [08:09<4:08:07,  1.97s/it]                                                    {'loss': 3.1175, 'grad_norm': 1.2336807250976562, 'learning_rate': 0.0004999973998416259, 'epoch': 0.03}
  3%|▎         | 246/7812 [08:09<4:08:07,  1.97s/it]  3%|▎         | 247/7812 [08:12<4:31:28,  2.15s/it]                                                    {'loss': 3.1421, 'grad_norm': 0.9720466136932373, 'learning_rate': 0.0004999969055976653, 'epoch': 0.03}
  3%|▎         | 247/7812 [08:12<4:31:28,  2.15s/it]  3%|▎         | 248/7812 [08:14<4:21:39,  2.08s/it]                                                    {'loss': 3.2, 'grad_norm': 1.0476176738739014, 'learning_rate': 0.0004999963683763384, 'epoch': 0.03}
  3%|▎         | 248/7812 [08:14<4:21:39,  2.08s/it]  3%|▎         | 249/7812 [08:16<4:17:33,  2.04s/it]                                                    {'loss': 3.1448, 'grad_norm': 1.1754024028778076, 'learning_rate': 0.0004999957881777376, 'epoch': 0.03}
  3%|▎         | 249/7812 [08:16<4:17:33,  2.04s/it]  3%|▎         | 250/7812 [08:18<4:22:12,  2.08s/it]                                                    {'loss': 3.2167, 'grad_norm': 0.9716894030570984, 'learning_rate': 0.0004999951650019627, 'epoch': 0.03}
  3%|▎         | 250/7812 [08:18<4:22:12,  2.08s/it]  3%|▎         | 251/7812 [08:20<4:12:40,  2.01s/it]                                                    {'loss': 3.2538, 'grad_norm': 1.1380442380905151, 'learning_rate': 0.0004999944988491207, 'epoch': 0.03}
  3%|▎         | 251/7812 [08:20<4:12:40,  2.01s/it]  3%|▎         | 252/7812 [08:22<4:07:56,  1.97s/it]                                                    {'loss': 3.2947, 'grad_norm': 0.963387131690979, 'learning_rate': 0.000499993789719326, 'epoch': 0.03}
  3%|▎         | 252/7812 [08:22<4:07:56,  1.97s/it]  3%|▎         | 253/7812 [08:24<4:02:15,  1.92s/it]                                                    {'loss': 3.2892, 'grad_norm': 1.0954186916351318, 'learning_rate': 0.0004999930376127007, 'epoch': 0.03}
  3%|▎         | 253/7812 [08:24<4:02:15,  1.92s/it]  3%|▎         | 254/7812 [08:26<4:02:46,  1.93s/it]                                                    {'loss': 3.3027, 'grad_norm': 0.8959102630615234, 'learning_rate': 0.0004999922425293743, 'epoch': 0.03}
  3%|▎         | 254/7812 [08:26<4:02:46,  1.93s/it]  3%|▎         | 255/7812 [08:28<4:05:49,  1.95s/it]                                                    {'loss': 3.085, 'grad_norm': 0.8915219306945801, 'learning_rate': 0.000499991404469483, 'epoch': 0.03}
  3%|▎         | 255/7812 [08:28<4:05:49,  1.95s/it]  3%|▎         | 256/7812 [08:29<4:00:15,  1.91s/it]                                                    {'loss': 3.2732, 'grad_norm': 1.0826343297958374, 'learning_rate': 0.0004999905234331712, 'epoch': 0.03}
  3%|▎         | 256/7812 [08:29<4:00:15,  1.91s/it]  3%|▎         | 257/7812 [08:31<3:55:41,  1.87s/it]                                                    {'loss': 3.1201, 'grad_norm': 0.9333148002624512, 'learning_rate': 0.0004999895994205903, 'epoch': 0.03}
  3%|▎         | 257/7812 [08:31<3:55:41,  1.87s/it]  3%|▎         | 258/7812 [08:33<3:57:26,  1.89s/it]                                                    {'loss': 3.2044, 'grad_norm': 1.0866812467575073, 'learning_rate': 0.0004999886324318992, 'epoch': 0.03}
  3%|▎         | 258/7812 [08:33<3:57:26,  1.89s/it]  3%|▎         | 259/7812 [08:35<4:02:58,  1.93s/it]                                                    {'loss': 3.1879, 'grad_norm': 1.0722250938415527, 'learning_rate': 0.000499987622467264, 'epoch': 0.03}
  3%|▎         | 259/7812 [08:35<4:02:58,  1.93s/it]  3%|▎         | 260/7812 [08:37<4:00:19,  1.91s/it]                                                    {'loss': 3.0953, 'grad_norm': 0.9913153648376465, 'learning_rate': 0.0004999865695268584, 'epoch': 0.03}
  3%|▎         | 260/7812 [08:37<4:00:19,  1.91s/it]  3%|▎         | 261/7812 [08:40<4:31:11,  2.15s/it]                                                    {'loss': 3.1977, 'grad_norm': 0.9935021996498108, 'learning_rate': 0.0004999854736108633, 'epoch': 0.03}
  3%|▎         | 261/7812 [08:40<4:31:11,  2.15s/it]  3%|▎         | 262/7812 [08:42<4:21:01,  2.07s/it]                                                    {'loss': 3.1418, 'grad_norm': 1.0601704120635986, 'learning_rate': 0.0004999843347194674, 'epoch': 0.03}
  3%|▎         | 262/7812 [08:42<4:21:01,  2.07s/it]  3%|▎         | 263/7812 [08:43<4:10:11,  1.99s/it]                                                    {'loss': 3.2373, 'grad_norm': 0.9229297041893005, 'learning_rate': 0.0004999831528528662, 'epoch': 0.03}
  3%|▎         | 263/7812 [08:43<4:10:11,  1.99s/it]  3%|▎         | 264/7812 [08:45<4:05:09,  1.95s/it]                                                    {'loss': 3.2124, 'grad_norm': 0.8930876851081848, 'learning_rate': 0.0004999819280112629, 'epoch': 0.03}
  3%|▎         | 264/7812 [08:45<4:05:09,  1.95s/it]  3%|▎         | 265/7812 [08:47<4:00:28,  1.91s/it]                                                    {'loss': 3.146, 'grad_norm': 1.0582590103149414, 'learning_rate': 0.0004999806601948682, 'epoch': 0.03}
  3%|▎         | 265/7812 [08:47<4:00:28,  1.91s/it]  3%|▎         | 266/7812 [08:49<3:55:45,  1.87s/it]                                                    {'loss': 3.3019, 'grad_norm': 0.9964078664779663, 'learning_rate': 0.0004999793494039, 'epoch': 0.03}
  3%|▎         | 266/7812 [08:49<3:55:45,  1.87s/it]  3%|▎         | 267/7812 [08:51<3:59:50,  1.91s/it]                                                    {'loss': 3.1573, 'grad_norm': 1.1042146682739258, 'learning_rate': 0.0004999779956385836, 'epoch': 0.03}
  3%|▎         | 267/7812 [08:51<3:59:50,  1.91s/it]  3%|▎         | 268/7812 [08:53<4:04:59,  1.95s/it]                                                    {'loss': 3.1831, 'grad_norm': 0.8864492177963257, 'learning_rate': 0.0004999765988991518, 'epoch': 0.03}
  3%|▎         | 268/7812 [08:53<4:04:59,  1.95s/it]  3%|▎         | 269/7812 [08:55<4:05:44,  1.95s/it]                                                    {'loss': 3.1248, 'grad_norm': 1.002585530281067, 'learning_rate': 0.0004999751591858447, 'epoch': 0.03}
  3%|▎         | 269/7812 [08:55<4:05:44,  1.95s/it]  3%|▎         | 270/7812 [08:58<4:53:47,  2.34s/it]                                                    {'loss': 3.162, 'grad_norm': 0.9198265671730042, 'learning_rate': 0.0004999736764989096, 'epoch': 0.03}
  3%|▎         | 270/7812 [08:58<4:53:47,  2.34s/it]  3%|▎         | 271/7812 [09:00<4:38:56,  2.22s/it]                                                    {'loss': 3.0748, 'grad_norm': 0.8425856232643127, 'learning_rate': 0.0004999721508386018, 'epoch': 0.03}
  3%|▎         | 271/7812 [09:00<4:38:56,  2.22s/it]  3%|▎         | 272/7812 [09:02<4:27:49,  2.13s/it]                                                    {'loss': 3.1911, 'grad_norm': 1.1116467714309692, 'learning_rate': 0.0004999705822051832, 'epoch': 0.03}
  3%|▎         | 272/7812 [09:02<4:27:49,  2.13s/it]  3%|▎         | 273/7812 [09:04<4:21:56,  2.08s/it]                                                    {'loss': 3.0491, 'grad_norm': 0.8809595108032227, 'learning_rate': 0.0004999689705989237, 'epoch': 0.03}
  3%|▎         | 273/7812 [09:04<4:21:56,  2.08s/it]  4%|▎         | 274/7812 [09:06<4:10:18,  1.99s/it]                                                    {'loss': 3.1718, 'grad_norm': 0.9991055130958557, 'learning_rate': 0.0004999673160201001, 'epoch': 0.04}
  4%|▎         | 274/7812 [09:06<4:10:18,  1.99s/it]  4%|▎         | 275/7812 [09:08<4:08:42,  1.98s/it]                                                    {'loss': 3.1932, 'grad_norm': 1.0885379314422607, 'learning_rate': 0.0004999656184689972, 'epoch': 0.04}
  4%|▎         | 275/7812 [09:08<4:08:42,  1.98s/it]  4%|▎         | 276/7812 [09:10<4:08:43,  1.98s/it]                                                    {'loss': 3.111, 'grad_norm': 0.8874768018722534, 'learning_rate': 0.0004999638779459065, 'epoch': 0.04}
  4%|▎         | 276/7812 [09:10<4:08:43,  1.98s/it]  4%|▎         | 277/7812 [09:12<4:14:32,  2.03s/it]                                                    {'loss': 3.0643, 'grad_norm': 1.1640793085098267, 'learning_rate': 0.0004999620944511274, 'epoch': 0.04}
  4%|▎         | 277/7812 [09:12<4:14:32,  2.03s/it]  4%|▎         | 278/7812 [09:14<4:07:21,  1.97s/it]                                                    {'loss': 3.136, 'grad_norm': 0.9334332346916199, 'learning_rate': 0.0004999602679849665, 'epoch': 0.04}
  4%|▎         | 278/7812 [09:14<4:07:21,  1.97s/it]  4%|▎         | 279/7812 [09:15<3:58:43,  1.90s/it]                                                    {'loss': 3.1149, 'grad_norm': 1.1715296506881714, 'learning_rate': 0.0004999583985477377, 'epoch': 0.04}
  4%|▎         | 279/7812 [09:15<3:58:43,  1.90s/it]  4%|▎         | 280/7812 [09:17<4:06:13,  1.96s/it]                                                    {'loss': 3.2631, 'grad_norm': 1.124061107635498, 'learning_rate': 0.0004999564861397624, 'epoch': 0.04}
  4%|▎         | 280/7812 [09:17<4:06:13,  1.96s/it]  4%|▎         | 281/7812 [09:19<4:01:39,  1.93s/it]                                                    {'loss': 3.1548, 'grad_norm': 0.9701387882232666, 'learning_rate': 0.0004999545307613695, 'epoch': 0.04}
  4%|▎         | 281/7812 [09:19<4:01:39,  1.93s/it]  4%|▎         | 282/7812 [09:21<3:57:00,  1.89s/it]                                                    {'loss': 3.1679, 'grad_norm': 1.0920135974884033, 'learning_rate': 0.0004999525324128949, 'epoch': 0.04}
  4%|▎         | 282/7812 [09:21<3:57:00,  1.89s/it]  4%|▎         | 283/7812 [09:23<4:05:04,  1.95s/it]                                                    {'loss': 3.1063, 'grad_norm': 0.9221906065940857, 'learning_rate': 0.0004999504910946824, 'epoch': 0.04}
  4%|▎         | 283/7812 [09:23<4:05:04,  1.95s/it]  4%|▎         | 284/7812 [09:25<4:11:10,  2.00s/it]                                                    {'loss': 3.1066, 'grad_norm': 0.8327930569648743, 'learning_rate': 0.0004999484068070827, 'epoch': 0.04}
  4%|▎         | 284/7812 [09:25<4:11:10,  2.00s/it]  4%|▎         | 285/7812 [09:27<4:16:12,  2.04s/it]                                                    {'loss': 3.0675, 'grad_norm': 0.9571911096572876, 'learning_rate': 0.0004999462795504542, 'epoch': 0.04}
  4%|▎         | 285/7812 [09:27<4:16:12,  2.04s/it]  4%|▎         | 286/7812 [09:30<4:21:40,  2.09s/it]                                                    {'loss': 3.0621, 'grad_norm': 0.8695428967475891, 'learning_rate': 0.0004999441093251627, 'epoch': 0.04}
  4%|▎         | 286/7812 [09:30<4:21:40,  2.09s/it]  4%|▎         | 287/7812 [09:32<4:19:52,  2.07s/it]                                                    {'loss': 3.2001, 'grad_norm': 0.9332185387611389, 'learning_rate': 0.0004999418961315812, 'epoch': 0.04}
  4%|▎         | 287/7812 [09:32<4:19:52,  2.07s/it]  4%|▎         | 288/7812 [09:34<4:26:05,  2.12s/it]                                                    {'loss': 3.1705, 'grad_norm': 0.9217270612716675, 'learning_rate': 0.0004999396399700902, 'epoch': 0.04}
  4%|▎         | 288/7812 [09:34<4:26:05,  2.12s/it]  4%|▎         | 289/7812 [09:36<4:17:14,  2.05s/it]                                                    {'loss': 3.1176, 'grad_norm': 1.0171387195587158, 'learning_rate': 0.0004999373408410775, 'epoch': 0.04}
  4%|▎         | 289/7812 [09:36<4:17:14,  2.05s/it]  4%|▎         | 290/7812 [09:38<4:06:33,  1.97s/it]                                                    {'loss': 3.1552, 'grad_norm': 0.9834141135215759, 'learning_rate': 0.0004999349987449384, 'epoch': 0.04}
  4%|▎         | 290/7812 [09:38<4:06:33,  1.97s/it]  4%|▎         | 291/7812 [09:40<4:09:10,  1.99s/it]                                                    {'loss': 3.1241, 'grad_norm': 0.8300170302391052, 'learning_rate': 0.0004999326136820754, 'epoch': 0.04}
  4%|▎         | 291/7812 [09:40<4:09:10,  1.99s/it]  4%|▎         | 292/7812 [09:42<4:08:36,  1.98s/it]                                                    {'loss': 3.1119, 'grad_norm': 0.7940613031387329, 'learning_rate': 0.0004999301856528989, 'epoch': 0.04}
  4%|▎         | 292/7812 [09:42<4:08:36,  1.98s/it]  4%|▍         | 293/7812 [09:43<4:01:40,  1.93s/it]                                                    {'loss': 3.103, 'grad_norm': 0.9324507713317871, 'learning_rate': 0.0004999277146578258, 'epoch': 0.04}
  4%|▍         | 293/7812 [09:43<4:01:40,  1.93s/it]  4%|▍         | 294/7812 [09:46<4:32:58,  2.18s/it]                                                    {'loss': 3.117, 'grad_norm': 0.9646312594413757, 'learning_rate': 0.0004999252006972813, 'epoch': 0.04}
  4%|▍         | 294/7812 [09:46<4:32:58,  2.18s/it]  4%|▍         | 295/7812 [09:48<4:16:02,  2.04s/it]                                                    {'loss': 3.2047, 'grad_norm': 1.054712176322937, 'learning_rate': 0.0004999226437716974, 'epoch': 0.04}
  4%|▍         | 295/7812 [09:48<4:16:02,  2.04s/it]  4%|▍         | 296/7812 [09:50<4:06:17,  1.97s/it]                                                    {'loss': 3.2588, 'grad_norm': 0.9186089038848877, 'learning_rate': 0.0004999200438815136, 'epoch': 0.04}
  4%|▍         | 296/7812 [09:50<4:06:17,  1.97s/it]  4%|▍         | 297/7812 [09:52<4:13:35,  2.02s/it]                                                    {'loss': 3.2299, 'grad_norm': 0.993134081363678, 'learning_rate': 0.000499917401027177, 'epoch': 0.04}
  4%|▍         | 297/7812 [09:52<4:13:35,  2.02s/it]  4%|▍         | 298/7812 [09:54<4:08:45,  1.99s/it]                                                    {'loss': 3.1001, 'grad_norm': 0.9920868873596191, 'learning_rate': 0.0004999147152091419, 'epoch': 0.04}
  4%|▍         | 298/7812 [09:54<4:08:45,  1.99s/it]  4%|▍         | 299/7812 [09:56<4:22:38,  2.10s/it]                                                    {'loss': 3.0716, 'grad_norm': 0.9673355221748352, 'learning_rate': 0.0004999119864278699, 'epoch': 0.04}
  4%|▍         | 299/7812 [09:56<4:22:38,  2.10s/it]  4%|▍         | 300/7812 [09:58<4:09:41,  1.99s/it]                                                    {'loss': 3.2254, 'grad_norm': 0.9313256144523621, 'learning_rate': 0.0004999092146838302, 'epoch': 0.04}
  4%|▍         | 300/7812 [09:58<4:09:41,  1.99s/it]  4%|▍         | 301/7812 [10:00<4:11:26,  2.01s/it]                                                    {'loss': 3.2138, 'grad_norm': 0.9034249186515808, 'learning_rate': 0.0004999063999774994, 'epoch': 0.04}
  4%|▍         | 301/7812 [10:00<4:11:26,  2.01s/it]  4%|▍         | 302/7812 [10:02<4:26:06,  2.13s/it]                                                    {'loss': 3.1584, 'grad_norm': 0.9248456358909607, 'learning_rate': 0.0004999035423093612, 'epoch': 0.04}
  4%|▍         | 302/7812 [10:02<4:26:06,  2.13s/it]  4%|▍         | 303/7812 [10:04<4:22:33,  2.10s/it]                                                    {'loss': 3.2152, 'grad_norm': 0.8755984902381897, 'learning_rate': 0.000499900641679907, 'epoch': 0.04}
  4%|▍         | 303/7812 [10:04<4:22:33,  2.10s/it]  4%|▍         | 304/7812 [10:06<4:14:53,  2.04s/it]                                                    {'loss': 3.2228, 'grad_norm': 0.9401273131370544, 'learning_rate': 0.0004998976980896354, 'epoch': 0.04}
  4%|▍         | 304/7812 [10:06<4:14:53,  2.04s/it]  4%|▍         | 305/7812 [10:08<4:04:34,  1.95s/it]                                                    {'loss': 3.2013, 'grad_norm': 0.935157299041748, 'learning_rate': 0.0004998947115390524, 'epoch': 0.04}
  4%|▍         | 305/7812 [10:08<4:04:34,  1.95s/it]  4%|▍         | 306/7812 [10:10<4:02:17,  1.94s/it]                                                    {'loss': 3.1529, 'grad_norm': 0.924612820148468, 'learning_rate': 0.0004998916820286714, 'epoch': 0.04}
  4%|▍         | 306/7812 [10:10<4:02:17,  1.94s/it]  4%|▍         | 307/7812 [10:12<4:02:36,  1.94s/it]                                                    {'loss': 3.1414, 'grad_norm': 0.9880234599113464, 'learning_rate': 0.0004998886095590134, 'epoch': 0.04}
  4%|▍         | 307/7812 [10:12<4:02:36,  1.94s/it]  4%|▍         | 308/7812 [10:14<3:59:16,  1.91s/it]                                                    {'loss': 3.1164, 'grad_norm': 0.9980438351631165, 'learning_rate': 0.0004998854941306064, 'epoch': 0.04}
  4%|▍         | 308/7812 [10:14<3:59:16,  1.91s/it]  4%|▍         | 309/7812 [10:16<4:25:10,  2.12s/it]                                                    {'loss': 3.0303, 'grad_norm': 0.8902761936187744, 'learning_rate': 0.000499882335743986, 'epoch': 0.04}
  4%|▍         | 309/7812 [10:16<4:25:10,  2.12s/it]  4%|▍         | 310/7812 [10:18<4:20:28,  2.08s/it]                                                    {'loss': 3.1069, 'grad_norm': 0.9304934144020081, 'learning_rate': 0.0004998791343996952, 'epoch': 0.04}
  4%|▍         | 310/7812 [10:18<4:20:28,  2.08s/it]  4%|▍         | 311/7812 [10:20<4:16:17,  2.05s/it]                                                    {'loss': 3.066, 'grad_norm': 0.9208337068557739, 'learning_rate': 0.0004998758900982845, 'epoch': 0.04}
  4%|▍         | 311/7812 [10:20<4:16:17,  2.05s/it]  4%|▍         | 312/7812 [10:22<4:20:36,  2.08s/it]                                                    {'loss': 3.1474, 'grad_norm': 0.8455056548118591, 'learning_rate': 0.0004998726028403114, 'epoch': 0.04}
  4%|▍         | 312/7812 [10:22<4:20:36,  2.08s/it]  4%|▍         | 313/7812 [10:24<4:13:52,  2.03s/it]                                                    {'loss': 3.1802, 'grad_norm': 0.850476861000061, 'learning_rate': 0.000499869272626341, 'epoch': 0.04}
  4%|▍         | 313/7812 [10:24<4:13:52,  2.03s/it]  4%|▍         | 314/7812 [10:26<4:13:20,  2.03s/it]                                                    {'loss': 3.1197, 'grad_norm': 0.8594541549682617, 'learning_rate': 0.0004998658994569459, 'epoch': 0.04}
  4%|▍         | 314/7812 [10:26<4:13:20,  2.03s/it]  4%|▍         | 315/7812 [10:28<4:10:15,  2.00s/it]                                                    {'loss': 3.1132, 'grad_norm': 1.0096418857574463, 'learning_rate': 0.0004998624833327061, 'epoch': 0.04}
  4%|▍         | 315/7812 [10:28<4:10:15,  2.00s/it]  4%|▍         | 316/7812 [10:30<4:00:10,  1.92s/it]                                                    {'loss': 3.1855, 'grad_norm': 0.9100611805915833, 'learning_rate': 0.0004998590242542087, 'epoch': 0.04}
  4%|▍         | 316/7812 [10:30<4:00:10,  1.92s/it]  4%|▍         | 317/7812 [10:32<4:00:39,  1.93s/it]                                                    {'loss': 3.1442, 'grad_norm': 0.9197818040847778, 'learning_rate': 0.0004998555222220485, 'epoch': 0.04}
  4%|▍         | 317/7812 [10:32<4:00:39,  1.93s/it]  4%|▍         | 318/7812 [10:34<4:15:03,  2.04s/it]                                                    {'loss': 3.2029, 'grad_norm': 0.861576497554779, 'learning_rate': 0.0004998519772368273, 'epoch': 0.04}
  4%|▍         | 318/7812 [10:34<4:15:03,  2.04s/it]  4%|▍         | 319/7812 [10:36<4:15:21,  2.04s/it]                                                    {'loss': 3.1598, 'grad_norm': 0.8664217591285706, 'learning_rate': 0.0004998483892991549, 'epoch': 0.04}
  4%|▍         | 319/7812 [10:36<4:15:21,  2.04s/it]  4%|▍         | 320/7812 [10:39<4:32:41,  2.18s/it]                                                    {'loss': 3.0919, 'grad_norm': 0.9398431777954102, 'learning_rate': 0.0004998447584096477, 'epoch': 0.04}
  4%|▍         | 320/7812 [10:39<4:32:41,  2.18s/it]  4%|▍         | 321/7812 [10:41<4:17:40,  2.06s/it]                                                    {'loss': 3.0619, 'grad_norm': 0.9446377754211426, 'learning_rate': 0.0004998410845689301, 'epoch': 0.04}
  4%|▍         | 321/7812 [10:41<4:17:40,  2.06s/it]  4%|▍         | 322/7812 [10:43<4:15:09,  2.04s/it]                                                    {'loss': 3.1427, 'grad_norm': 0.8987549543380737, 'learning_rate': 0.0004998373677776337, 'epoch': 0.04}
  4%|▍         | 322/7812 [10:43<4:15:09,  2.04s/it]  4%|▍         | 323/7812 [10:44<4:10:26,  2.01s/it]                                                    {'loss': 3.1467, 'grad_norm': 0.9283555150032043, 'learning_rate': 0.0004998336080363975, 'epoch': 0.04}
  4%|▍         | 323/7812 [10:44<4:10:26,  2.01s/it]  4%|▍         | 324/7812 [10:46<4:01:50,  1.94s/it]                                                    {'loss': 3.1928, 'grad_norm': 1.015196681022644, 'learning_rate': 0.0004998298053458676, 'epoch': 0.04}
  4%|▍         | 324/7812 [10:46<4:01:50,  1.94s/it]  4%|▍         | 325/7812 [10:48<4:02:00,  1.94s/it]                                                    {'loss': 3.1429, 'grad_norm': 0.75288987159729, 'learning_rate': 0.000499825959706698, 'epoch': 0.04}
  4%|▍         | 325/7812 [10:48<4:02:00,  1.94s/it]  4%|▍         | 326/7812 [10:50<4:06:12,  1.97s/it]                                                    {'loss': 2.996, 'grad_norm': 0.8119940161705017, 'learning_rate': 0.0004998220711195496, 'epoch': 0.04}
  4%|▍         | 326/7812 [10:50<4:06:12,  1.97s/it]  4%|▍         | 327/7812 [10:52<4:00:54,  1.93s/it]                                                    {'loss': 3.1594, 'grad_norm': 0.900418758392334, 'learning_rate': 0.0004998181395850911, 'epoch': 0.04}
  4%|▍         | 327/7812 [10:52<4:00:54,  1.93s/it]  4%|▍         | 328/7812 [10:54<4:00:32,  1.93s/it]                                                    {'loss': 3.0806, 'grad_norm': 0.868576169013977, 'learning_rate': 0.0004998141651039982, 'epoch': 0.04}
  4%|▍         | 328/7812 [10:54<4:00:32,  1.93s/it]  4%|▍         | 329/7812 [10:56<3:59:20,  1.92s/it]                                                    {'loss': 3.1914, 'grad_norm': 0.8168501257896423, 'learning_rate': 0.0004998101476769542, 'epoch': 0.04}
  4%|▍         | 329/7812 [10:56<3:59:20,  1.92s/it]  4%|▍         | 330/7812 [10:58<3:58:15,  1.91s/it]                                                    {'loss': 3.1597, 'grad_norm': 0.9231798052787781, 'learning_rate': 0.0004998060873046498, 'epoch': 0.04}
  4%|▍         | 330/7812 [10:58<3:58:15,  1.91s/it]  4%|▍         | 331/7812 [11:00<4:00:25,  1.93s/it]                                                    {'loss': 3.1006, 'grad_norm': 0.9514910578727722, 'learning_rate': 0.000499801983987783, 'epoch': 0.04}
  4%|▍         | 331/7812 [11:00<4:00:25,  1.93s/it]  4%|▍         | 332/7812 [11:02<4:01:04,  1.93s/it]                                                    {'loss': 3.1208, 'grad_norm': 0.7541351318359375, 'learning_rate': 0.0004997978377270591, 'epoch': 0.04}
  4%|▍         | 332/7812 [11:02<4:01:04,  1.93s/it]  4%|▍         | 333/7812 [11:03<3:53:45,  1.88s/it]                                                    {'loss': 3.0637, 'grad_norm': 0.8203004598617554, 'learning_rate': 0.0004997936485231911, 'epoch': 0.04}
  4%|▍         | 333/7812 [11:03<3:53:45,  1.88s/it]  4%|▍         | 334/7812 [11:05<3:51:04,  1.85s/it]                                                    {'loss': 3.1076, 'grad_norm': 0.8562555313110352, 'learning_rate': 0.0004997894163768992, 'epoch': 0.04}
  4%|▍         | 334/7812 [11:05<3:51:04,  1.85s/it]  4%|▍         | 335/7812 [11:07<3:56:31,  1.90s/it]                                                    {'loss': 3.1326, 'grad_norm': 0.8058054447174072, 'learning_rate': 0.0004997851412889106, 'epoch': 0.04}
  4%|▍         | 335/7812 [11:07<3:56:31,  1.90s/it]  4%|▍         | 336/7812 [11:09<3:53:52,  1.88s/it]                                                    {'loss': 3.1683, 'grad_norm': 0.8049592971801758, 'learning_rate': 0.0004997808232599604, 'epoch': 0.04}
  4%|▍         | 336/7812 [11:09<3:53:52,  1.88s/it]  4%|▍         | 337/7812 [11:11<3:55:39,  1.89s/it]                                                    {'loss': 3.0268, 'grad_norm': 0.7553438544273376, 'learning_rate': 0.0004997764622907911, 'epoch': 0.04}
  4%|▍         | 337/7812 [11:11<3:55:39,  1.89s/it]  4%|▍         | 338/7812 [11:13<3:54:08,  1.88s/it]                                                    {'loss': 3.1907, 'grad_norm': 0.7799020409584045, 'learning_rate': 0.0004997720583821523, 'epoch': 0.04}
  4%|▍         | 338/7812 [11:13<3:54:08,  1.88s/it]  4%|▍         | 339/7812 [11:15<3:52:29,  1.87s/it]                                                    {'loss': 3.0059, 'grad_norm': 0.7601535320281982, 'learning_rate': 0.000499767611534801, 'epoch': 0.04}
  4%|▍         | 339/7812 [11:15<3:52:29,  1.87s/it]  4%|▍         | 340/7812 [11:17<3:51:10,  1.86s/it]                                                    {'loss': 3.1147, 'grad_norm': 0.796134889125824, 'learning_rate': 0.0004997631217495018, 'epoch': 0.04}
  4%|▍         | 340/7812 [11:17<3:51:10,  1.86s/it]  4%|▍         | 341/7812 [11:19<4:00:16,  1.93s/it]                                                    {'loss': 3.1403, 'grad_norm': 0.7962773442268372, 'learning_rate': 0.0004997585890270265, 'epoch': 0.04}
  4%|▍         | 341/7812 [11:19<4:00:16,  1.93s/it]  4%|▍         | 342/7812 [11:21<4:09:24,  2.00s/it]                                                    {'loss': 3.0662, 'grad_norm': 0.773429274559021, 'learning_rate': 0.0004997540133681541, 'epoch': 0.04}
  4%|▍         | 342/7812 [11:21<4:09:24,  2.00s/it]  4%|▍         | 343/7812 [11:23<4:08:56,  2.00s/it]                                                    {'loss': 3.1047, 'grad_norm': 0.7849649786949158, 'learning_rate': 0.0004997493947736715, 'epoch': 0.04}
  4%|▍         | 343/7812 [11:23<4:08:56,  2.00s/it]  4%|▍         | 344/7812 [11:25<4:02:13,  1.95s/it]                                                    {'loss': 3.0768, 'grad_norm': 0.8890687227249146, 'learning_rate': 0.0004997447332443727, 'epoch': 0.04}
  4%|▍         | 344/7812 [11:25<4:02:13,  1.95s/it]  4%|▍         | 345/7812 [11:27<4:04:28,  1.96s/it]                                                    {'loss': 3.0465, 'grad_norm': 0.8126696348190308, 'learning_rate': 0.0004997400287810587, 'epoch': 0.04}
  4%|▍         | 345/7812 [11:27<4:04:28,  1.96s/it]  4%|▍         | 346/7812 [11:29<4:05:32,  1.97s/it]                                                    {'loss': 3.0404, 'grad_norm': 0.7184579372406006, 'learning_rate': 0.0004997352813845388, 'epoch': 0.04}
  4%|▍         | 346/7812 [11:29<4:05:32,  1.97s/it]  4%|▍         | 347/7812 [11:31<4:11:48,  2.02s/it]                                                    {'loss': 2.9998, 'grad_norm': 0.8350313901901245, 'learning_rate': 0.0004997304910556288, 'epoch': 0.04}
  4%|▍         | 347/7812 [11:31<4:11:48,  2.02s/it]  4%|▍         | 348/7812 [11:33<4:08:18,  2.00s/it]                                                    {'loss': 3.0847, 'grad_norm': 0.7794427871704102, 'learning_rate': 0.0004997256577951521, 'epoch': 0.04}
  4%|▍         | 348/7812 [11:33<4:08:18,  2.00s/it]  4%|▍         | 349/7812 [11:35<4:02:14,  1.95s/it]                                                    {'loss': 2.9853, 'grad_norm': 0.8106400370597839, 'learning_rate': 0.0004997207816039398, 'epoch': 0.04}
  4%|▍         | 349/7812 [11:35<4:02:14,  1.95s/it]  4%|▍         | 350/7812 [11:36<3:55:31,  1.89s/it]                                                    {'loss': 3.1605, 'grad_norm': 0.8324818015098572, 'learning_rate': 0.0004997158624828303, 'epoch': 0.04}
  4%|▍         | 350/7812 [11:36<3:55:31,  1.89s/it]  4%|▍         | 351/7812 [11:39<4:09:13,  2.00s/it]                                                    {'loss': 3.1994, 'grad_norm': 0.7455389499664307, 'learning_rate': 0.000499710900432669, 'epoch': 0.04}
  4%|▍         | 351/7812 [11:39<4:09:13,  2.00s/it]  5%|▍         | 352/7812 [11:41<4:20:59,  2.10s/it]                                                    {'loss': 3.0152, 'grad_norm': 0.7915196418762207, 'learning_rate': 0.0004997058954543089, 'epoch': 0.05}
  5%|▍         | 352/7812 [11:41<4:20:59,  2.10s/it]  5%|▍         | 353/7812 [11:43<4:11:19,  2.02s/it]                                                    {'loss': 3.1154, 'grad_norm': 0.7524362206459045, 'learning_rate': 0.0004997008475486107, 'epoch': 0.05}
  5%|▍         | 353/7812 [11:43<4:11:19,  2.02s/it]  5%|▍         | 354/7812 [11:45<4:11:05,  2.02s/it]                                                    {'loss': 3.0923, 'grad_norm': 0.7536296844482422, 'learning_rate': 0.000499695756716442, 'epoch': 0.05}
  5%|▍         | 354/7812 [11:45<4:11:05,  2.02s/it]  5%|▍         | 355/7812 [11:47<4:28:36,  2.16s/it]                                                    {'loss': 3.0645, 'grad_norm': 0.8200210928916931, 'learning_rate': 0.0004996906229586778, 'epoch': 0.05}
  5%|▍         | 355/7812 [11:47<4:28:36,  2.16s/it]  5%|▍         | 356/7812 [11:49<4:18:28,  2.08s/it]                                                    {'loss': 3.0477, 'grad_norm': 0.9129500985145569, 'learning_rate': 0.000499685446276201, 'epoch': 0.05}
  5%|▍         | 356/7812 [11:49<4:18:28,  2.08s/it]  5%|▍         | 357/7812 [11:51<4:23:28,  2.12s/it]                                                    {'loss': 3.0881, 'grad_norm': 0.8566024899482727, 'learning_rate': 0.0004996802266699014, 'epoch': 0.05}
  5%|▍         | 357/7812 [11:51<4:23:28,  2.12s/it]  5%|▍         | 358/7812 [11:53<4:17:39,  2.07s/it]                                                    {'loss': 3.1366, 'grad_norm': 0.8085132241249084, 'learning_rate': 0.0004996749641406763, 'epoch': 0.05}
  5%|▍         | 358/7812 [11:53<4:17:39,  2.07s/it]  5%|▍         | 359/7812 [11:55<4:08:18,  2.00s/it]                                                    {'loss': 3.0977, 'grad_norm': 0.831687331199646, 'learning_rate': 0.0004996696586894304, 'epoch': 0.05}
  5%|▍         | 359/7812 [11:55<4:08:18,  2.00s/it]  5%|▍         | 360/7812 [11:57<4:02:51,  1.96s/it]                                                    {'loss': 3.1109, 'grad_norm': 0.8317452669143677, 'learning_rate': 0.0004996643103170757, 'epoch': 0.05}
  5%|▍         | 360/7812 [11:57<4:02:51,  1.96s/it]  5%|▍         | 361/7812 [11:59<4:00:47,  1.94s/it]                                                    {'loss': 3.1291, 'grad_norm': 0.7560385465621948, 'learning_rate': 0.0004996589190245318, 'epoch': 0.05}
  5%|▍         | 361/7812 [11:59<4:00:47,  1.94s/it]  5%|▍         | 362/7812 [12:01<3:57:29,  1.91s/it]                                                    {'loss': 3.0867, 'grad_norm': 0.7824926972389221, 'learning_rate': 0.0004996534848127253, 'epoch': 0.05}
  5%|▍         | 362/7812 [12:01<3:57:29,  1.91s/it]  5%|▍         | 363/7812 [12:03<4:04:28,  1.97s/it]                                                    {'loss': 3.1659, 'grad_norm': 0.8042452931404114, 'learning_rate': 0.0004996480076825906, 'epoch': 0.05}
  5%|▍         | 363/7812 [12:03<4:04:28,  1.97s/it]  5%|▍         | 364/7812 [12:05<4:04:28,  1.97s/it]                                                    {'loss': 3.1729, 'grad_norm': 0.8667904734611511, 'learning_rate': 0.0004996424876350692, 'epoch': 0.05}
  5%|▍         | 364/7812 [12:05<4:04:28,  1.97s/it]  5%|▍         | 365/7812 [12:07<4:07:42,  2.00s/it]                                                    {'loss': 3.1324, 'grad_norm': 0.7145339846611023, 'learning_rate': 0.00049963692467111, 'epoch': 0.05}
  5%|▍         | 365/7812 [12:07<4:07:42,  2.00s/it]  5%|▍         | 366/7812 [12:09<4:01:05,  1.94s/it]                                                    {'loss': 3.1133, 'grad_norm': 0.8929138779640198, 'learning_rate': 0.0004996313187916694, 'epoch': 0.05}
  5%|▍         | 366/7812 [12:09<4:01:05,  1.94s/it]  5%|▍         | 367/7812 [12:11<4:01:08,  1.94s/it]                                                    {'loss': 3.0382, 'grad_norm': 0.7456083297729492, 'learning_rate': 0.0004996256699977112, 'epoch': 0.05}
  5%|▍         | 367/7812 [12:11<4:01:08,  1.94s/it]  5%|▍         | 368/7812 [12:12<3:58:23,  1.92s/it]                                                    {'loss': 3.0235, 'grad_norm': 0.7597713470458984, 'learning_rate': 0.0004996199782902064, 'epoch': 0.05}
  5%|▍         | 368/7812 [12:12<3:58:23,  1.92s/it]  5%|▍         | 369/7812 [12:15<4:07:22,  1.99s/it]                                                    {'loss': 3.1228, 'grad_norm': 0.7161765098571777, 'learning_rate': 0.0004996142436701336, 'epoch': 0.05}
  5%|▍         | 369/7812 [12:15<4:07:22,  1.99s/it]  5%|▍         | 370/7812 [12:17<4:10:42,  2.02s/it]                                                    {'loss': 2.9244, 'grad_norm': 0.6953368186950684, 'learning_rate': 0.0004996084661384783, 'epoch': 0.05}
  5%|▍         | 370/7812 [12:17<4:10:42,  2.02s/it]  5%|▍         | 371/7812 [12:19<4:02:21,  1.95s/it]                                                    {'loss': 3.1099, 'grad_norm': 0.7338658571243286, 'learning_rate': 0.0004996026456962341, 'epoch': 0.05}
  5%|▍         | 371/7812 [12:19<4:02:21,  1.95s/it]  5%|▍         | 372/7812 [12:20<4:03:22,  1.96s/it]                                                    {'loss': 3.0418, 'grad_norm': 0.6522376537322998, 'learning_rate': 0.0004995967823444015, 'epoch': 0.05}
  5%|▍         | 372/7812 [12:20<4:03:22,  1.96s/it]  5%|▍         | 373/7812 [12:22<4:04:23,  1.97s/it]                                                    {'loss': 2.9611, 'grad_norm': 0.7884069085121155, 'learning_rate': 0.0004995908760839884, 'epoch': 0.05}
  5%|▍         | 373/7812 [12:22<4:04:23,  1.97s/it]  5%|▍         | 374/7812 [12:24<4:03:53,  1.97s/it]                                                    {'loss': 3.0263, 'grad_norm': 0.7234141826629639, 'learning_rate': 0.0004995849269160102, 'epoch': 0.05}
  5%|▍         | 374/7812 [12:24<4:03:53,  1.97s/it]  5%|▍         | 375/7812 [12:26<4:00:43,  1.94s/it][2024-07-12 18:06:51,152] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 18:06:52,959] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-12 18:06:52,959] [INFO] [runner.py:568:main] cmd = /root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None bunny/train/train.py --deepspeed ./script/deepspeed/zero3.json --model_name_or_path /root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct --model_type phi-3 --version phi3 --data_path ./data/finetune/bunny_llava_allava_2m.json --image_folder ./data/finetune/images --vision_tower /root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384 --pretrain_mm_mlp_adapter ./checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin --mm_projector_type ldpv2 --image_aspect_ratio pad --group_by_modality_length False --bf16 True --output_dir ./checkpoints-phi-3/bunny-phi-3-siglip384 --num_train_epochs 1 --per_device_train_batch_size 8 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to none
[2024-07-12 18:06:54,277] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 18:06:56,164] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-12 18:06:56,164] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-12 18:06:56,164] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-12 18:06:56,164] [INFO] [launch.py:164:main] dist_world_size=8
[2024-07-12 18:06:56,165] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-12 18:06:56,165] [INFO] [launch.py:256:main] process 784639 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=0', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:06:56,165] [INFO] [launch.py:256:main] process 784640 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=1', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:06:56,166] [INFO] [launch.py:256:main] process 784641 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=2', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:06:56,166] [INFO] [launch.py:256:main] process 784642 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=3', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:06:56,166] [INFO] [launch.py:256:main] process 784643 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=4', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:06:56,167] [INFO] [launch.py:256:main] process 784644 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=5', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:06:56,167] [INFO] [launch.py:256:main] process 784645 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=6', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:06:56,167] [INFO] [launch.py:256:main] process 784646 spawned with command: ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=7', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none']
[2024-07-12 18:07:01,614] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 18:07:01,803] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 18:07:01,804] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 18:07:01,845] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 18:07:01,848] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 18:07:01,981] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 18:07:02,013] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 18:07:02,044] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 18:07:02,044] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 18:07:02,045] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-12 18:07:02,050] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-07-12 18:07:02,078] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible
[2024-07-12 18:07:02,217] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 18:07:02,250] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 18:07:02,250] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-12 18:07:02,280] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-12 18:07:02,286] [INFO] [comm.py:637:init_distributed] cdb=None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using a model of type phi3 to instantiate a model of type bunny-phi3. This is not supported for all configurations of models and can yield errors.
[2024-07-12 18:07:05,289] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 195, num_elems = 3.82B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.34it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.34it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.34it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.30it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.29it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.27it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.50s/it]
[2024-07-12 18:07:08,504] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 643, num_elems = 4.25B
[rank5]: Traceback (most recent call last):
[rank5]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank5]:     train()
[rank5]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank5]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank5]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank5]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank5]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank5]:     with _open_file_like(f, 'rb') as opened_file:
[rank5]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank5]:     return _open_file(name_or_buffer, mode)
[rank5]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank5]:     super().__init__(open(name, mode))
[rank5]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank6]:     train()
[rank6]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank6]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank6]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank6]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank6]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank6]:     with _open_file_like(f, 'rb') as opened_file:
[rank6]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank6]:     return _open_file(name_or_buffer, mode)
[rank6]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank6]:     super().__init__(open(name, mode))
[rank6]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank3]:     train()
[rank3]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank3]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank3]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank3]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank3]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank3]:     with _open_file_like(f, 'rb') as opened_file:
[rank3]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank3]:     return _open_file(name_or_buffer, mode)
[rank3]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank3]:     super().__init__(open(name, mode))
[rank3]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank7]:     train()
[rank7]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank7]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank7]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank7]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank7]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank7]:     with _open_file_like(f, 'rb') as opened_file:
[rank7]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank7]:     return _open_file(name_or_buffer, mode)
[rank7]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank7]:     super().__init__(open(name, mode))
[rank7]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank2]:     train()
[rank2]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank2]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank2]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank2]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank2]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank2]:     with _open_file_like(f, 'rb') as opened_file:
[rank2]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank2]:     return _open_file(name_or_buffer, mode)
[rank2]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank2]:     super().__init__(open(name, mode))
[rank2]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[rank4]: Traceback (most recent call last):
[rank4]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank4]:     train()
[rank4]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank4]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank4]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank4]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank4]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank4]:     with _open_file_like(f, 'rb') as opened_file:
[rank4]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank4]:     return _open_file(name_or_buffer, mode)
[rank4]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank4]:     super().__init__(open(name, mode))
[rank4]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank1]:     train()
[rank1]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank1]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank1]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank1]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank1]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank1]:     with _open_file_like(f, 'rb') as opened_file:
[rank1]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank1]:     return _open_file(name_or_buffer, mode)
[rank1]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank1]:     super().__init__(open(name, mode))
[rank1]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 403, in <module>
[rank0]:     train()
[rank0]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/train/train.py", line 324, in train
[rank0]:     model.get_model().initialize_vision_modules(model_args=model_args)
[rank0]:   File "/root/autodl-tmp/tzn/Projects/Bunny/bunny/model/bunny_arch.py", line 54, in initialize_vision_modules
[rank0]:     mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')
[rank0]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 997, in load
[rank0]:     with _open_file_like(f, 'rb') as opened_file:
[rank0]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 444, in _open_file_like
[rank0]:     return _open_file(name_or_buffer, mode)
[rank0]:   File "/root/autodl-tmp/tzn/anaconda3/envs/bunny/lib/python3.10/site-packages/torch/serialization.py", line 425, in __init__
[rank0]:     super().__init__(open(name, mode))
[rank0]: FileNotFoundError: [Errno 2] No such file or directory: './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin'
[2024-07-12 18:07:10,185] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784639
[2024-07-12 18:07:10,761] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784640
[2024-07-12 18:07:10,762] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784641
[2024-07-12 18:07:10,763] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784642
[2024-07-12 18:07:10,816] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784643
[2024-07-12 18:07:10,817] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784644
[2024-07-12 18:07:10,817] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784645
[2024-07-12 18:07:10,818] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 784646
[2024-07-12 18:07:10,818] [ERROR] [launch.py:325:sigkill_handler] ['/root/autodl-tmp/tzn/anaconda3/envs/bunny/bin/python', '-u', 'bunny/train/train.py', '--local_rank=7', '--deepspeed', './script/deepspeed/zero3.json', '--model_name_or_path', '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct', '--model_type', 'phi-3', '--version', 'phi3', '--data_path', './data/finetune/bunny_llava_allava_2m.json', '--image_folder', './data/finetune/images', '--vision_tower', '/root/autodl-tmp/tzn/Projects/pretrained/siglip/siglip-so400m-patch14-384', '--pretrain_mm_mlp_adapter', './checkpoints-pretrain/bunny-phi-3-pretrain-siglip384-ldpv2/mm_projector.bin', '--mm_projector_type', 'ldpv2', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'False', '--bf16', 'True', '--output_dir', './checkpoints-phi-3/bunny-phi-3-siglip384', '--num_train_epochs', '1', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'none'] exits with return code = 1
